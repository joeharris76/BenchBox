# Copyright 2026 Joe Harris / BenchBox Project
# Licensed under the MIT License. See LICENSE file in the project root for details.

# CI-focused pytest profile invoked with "pytest -c pytest-ci.ini"
[pytest]
# Enhanced pytest configuration for optimized execution

# Minimum version requirement
minversion = 8.0

# Test discovery patterns and paths
testpaths = 
    tests
    tests/unit
    tests/integration
    tests/performance
python_files = test_*.py *_test.py
python_classes = Test* *Tests
python_functions = test_* benchmark_*

# Test execution configuration with comprehensive strategies
addopts = 
    --strict-markers
    --strict-config
    --verbose
    --tb=short
    --durations=10
    --maxfail=10
    --disable-warnings
    -ra
    --cov=benchbox
    --cov-branch
    --cov-report=term-missing:skip-covered
    --cov-report=html:htmlcov
    --cov-report=xml:coverage.xml
    --cov-config=.coveragerc_core
    --junit-xml=test-results.xml
    --benchmark-disable
    
# Test execution strategies (use with -c pytest-strategy.ini)
# Fast development cycle: pytest -m "fast and unit"
# Integration testing: pytest -m "integration and not slow"
# Full benchmark suite: pytest -m "performance or specialized"
# Database-specific: pytest -m "duckdb" or pytest -m "sqlite"
# Benchmark-specific: pytest -m "tpch" or pytest -m "tpcds"
# Feature-specific: pytest -m "olap or window_functions"

# Test markers for categorization and selective execution
markers =
    # Execution speed markers
    fast: marks tests as fast (< 1 second execution time)
    medium: marks tests as medium speed (1-10 seconds execution time)
    slow: marks tests as slow (> 10 seconds execution time)
    
    # Test category markers
    unit: marks tests as unit tests (isolated component testing)
    integration: marks tests as integration tests (component interaction testing)
    performance: marks tests as performance tests (benchmarking and timing)
    regression: marks tests for regression testing
    monitoring: marks tests for monitoring functionality
    
    # Database type markers
    duckdb: marks tests that use DuckDB backend
    sqlite: marks tests that use SQLite backend
    database: marks tests that require database connections
    
    # Benchmark type markers
    tpch: marks TPC-H related tests
    tpcds: marks TPC-DS related tests
    tpcdi: marks TPC-DI related tests
    tpchavoc: marks TPC-H AVOC related tests
    ssb: marks Star Schema Benchmark tests
    amplab: marks AMPLab Big Data Benchmark tests
    clickbench: marks ClickBench related tests
    h2odb: marks H2O Database benchmark tests
    
    # Feature category markers
    olap: marks tests for OLAP functionality
    advanced_sql: marks tests for advanced SQL features
    generator: marks tests for data generation functionality
    validation: marks tests validating external TPC binaries
    
    # Additional markers aligned with default pytest.ini
    platform_smoke: marks lightweight platform smoke coverage
    primitives: marks primitive operations tests
    write_primitives: marks write primitives benchmark tests
    merge: marks merge/join operation tests
    window_functions: marks tests for window function support
    data_generation: marks tests for data generation functionality
    query_execution: marks tests for query execution functionality
    c_compatible: marks tests for C tool compatibility
    tpch_c: marks TPC-H C qgen compatibility tests
    qgen: marks tests that use qgen C tool
    reference_comparison: marks tests that compare with reference implementations

# Pytest plugins
required_plugins = 
    pytest-xdist>=3.0.0
    pytest-cov>=4.1.0
    pytest-benchmark>=4.0.0

# Coverage configuration
[coverage:run]
source = benchbox/core
disable_warnings = module-not-imported
omit =
    */tests/*
    */__pycache__/*
    setup.py
    conftest.py


[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
    if 0:
    if __name__ == .__main__.:
    class .*\bProtocol\):
    @(abc\.)?abstractmethod

show_missing = True
precision = 2
skip_covered = False
skip_empty = False

[coverage:html]
directory = htmlcov

[coverage:xml]
output = coverage.xml

# Parallel execution and performance configuration
[pytest-xdist]
# Automatically detect number of CPUs for parallel execution
auto = true
# Load balancing mode - distribute tests across workers
dist = loadscope
# Enable crash recovery and file synchronization
rsyncdir = benchbox tests
# Maximum number of workers (can be overridden with -n flag)
max_worker_restart = 2
# Worker timeout in seconds
rsyncdirs = benchbox tests conftest.py

# Pytest-benchmark configuration
[pytest-benchmark]
# Only run benchmarks when explicitly requested
disable_gc = true
warmup = false
warmup_iterations = 100000
min_rounds = 5
max_time = 1.0
min_time = 0.000005
timer = perf_counter
calibration_precision = 10
disable = false

# Pytest timeout configuration
timeout = 300
timeout_method = thread
# Timeout per test type
timeout_func_only = true
# Timeout for different test categories (can be overridden per test)
# Fast tests: 30s, Medium tests: 120s, Slow tests: 600s

# Pytest cache configuration
cache_dir = .pytest_cache

# Pytest warning filters
filterwarnings =
    ignore::UserWarning
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::ImportWarning
    ignore::ResourceWarning
    # Specific ignores for known issues
    ignore:.*imp module is deprecated.*:DeprecationWarning
    ignore:.*distutils.*:DeprecationWarning

# Pytest logging configuration
log_cli = false
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = tests.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(filename)s:%(lineno)d: %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Pytest collection configuration
collect_ignore = 
    setup.py
    conftest.py
    build
    dist
    .eggs
    *.egg-info
    .tox
    .coverage
    .pytest_cache
    htmlcov
    node_modules
    .git
    __pycache__
    *.pyc
    *.pyo
    .DS_Store
    Thumbs.db

# Pytest doctest configuration
doctest_optionflags = NORMALIZE_WHITESPACE IGNORE_EXCEPTION_DETAIL
doctest_ignore_import_errors = true

# Pytest assertion configuration
enable_assertion_pass_hook = false
junit_suite_name = benchbox_tests
junit_logging = system-out
junit_log_passing_tests = true
junit_duration_report = total
junit_family = xunit2

# Environment configuration
env =
    PYTHONPATH = {toxinidir}
    COVERAGE_FILE = {toxinidir}/.coverage
    PYTEST_CURRENT_TEST = {envname}

# Test session configuration
console_output_style = progress
