{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BenchBox Result Analysis Patterns\n",
    "\n",
    "This notebook demonstrates **advanced analytical patterns** for interpreting benchmark results. Learn how to detect performance regressions, identify trends, and perform root cause analysis.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- **Time series analysis**: Track performance changes over time\n",
    "- **Regression detection**: Identify performance degradation\n",
    "- **Variance analysis**: Understand performance stability\n",
    "- **Baseline comparison**: Compare against reference results\n",
    "- **Statistical testing**: Determine if changes are significant\n",
    "- **Anomaly detection**: Find outliers and unusual patterns\n",
    "- **Trend analysis**: Predict future performance\n",
    "- **Root cause patterns**: Debug performance issues\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- **CI/CD integration**: Automated performance validation\n",
    "- **Performance monitoring**: Track trends over releases\n",
    "- **Capacity planning**: Predict when scaling is needed\n",
    "- **Issue investigation**: Identify root causes of slowdowns\n",
    "\n",
    "## Expected Runtime\n",
    "\n",
    "Analysis runs on existing results:\n",
    "- Data loading: **10-20 seconds**\n",
    "- Statistical analysis: **30-60 seconds**\n",
    "- Complete notebook: **2-3 minutes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    \"results_dir\": \"./benchmark_results\",\n",
    "    \"baseline_file\": \"./benchmark_results/baseline.json\",  # Reference results\n",
    "    \"output_dir\": \"./analysis_results\",\n",
    "    # Analysis thresholds\n",
    "    \"regression_threshold\": 0.10,  # 10% slowdown = regression\n",
    "    \"significance_level\": 0.05,  # p-value threshold\n",
    "    \"outlier_std\": 3.0,  # 3 std deviations = outlier\n",
    "}\n",
    "\n",
    "os.makedirs(config[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Analysis environment configured\")\n",
    "print(f\"üìÅ Results directory: {config['results_dir']}\")\n",
    "print(f\"‚öôÔ∏è  Regression threshold: {config['regression_threshold'] * 100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_results(results_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Load all benchmark results from a directory into a DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: timestamp, platform, benchmark, scale_factor,\n",
    "                               query, execution_time_s, success, file\n",
    "    \"\"\"\n",
    "    results_path = Path(results_dir)\n",
    "\n",
    "    if not results_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  Directory not found: {results_dir}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_records = []\n",
    "\n",
    "    # Recursively find all JSON files\n",
    "    for json_file in results_path.rglob(\"*.json\"):\n",
    "        try:\n",
    "            with open(json_file) as f:\n",
    "                result = json.load(f)\n",
    "\n",
    "            # Skip if not a benchmark result\n",
    "            if \"query_results\" not in result:\n",
    "                continue\n",
    "\n",
    "            # Extract metadata\n",
    "            platform = result.get(\"platform\", json_file.parent.name)\n",
    "            benchmark = result.get(\"benchmark_name\", \"unknown\")\n",
    "            scale_factor = result.get(\"scale_factor\", 0.0)\n",
    "            file_time = datetime.fromtimestamp(json_file.stat().st_mtime)\n",
    "\n",
    "            # Extract query results\n",
    "            for qr in result.get(\"query_results\", []):\n",
    "                all_records.append(\n",
    "                    {\n",
    "                        \"timestamp\": file_time,\n",
    "                        \"platform\": platform,\n",
    "                        \"benchmark\": benchmark,\n",
    "                        \"scale_factor\": scale_factor,\n",
    "                        \"query\": qr.get(\"query_name\", qr.get(\"query_id\", \"unknown\")),\n",
    "                        \"execution_time_ms\": qr.get(\"execution_time_ms\", None),\n",
    "                        \"success\": qr.get(\"success\", False),\n",
    "                        \"file\": json_file.name,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error loading {json_file.name}: {e}\")\n",
    "\n",
    "    if not all_records:\n",
    "        print(\"‚ö†Ô∏è  No benchmark results found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    df[\"execution_time_s\"] = df[\"execution_time_ms\"] / 1000.0\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load all results\n",
    "df_all = load_all_results(config[\"results_dir\"])\n",
    "\n",
    "if len(df_all) > 0:\n",
    "    print(f\"\\nüìä Loaded {len(df_all)} query executions\")\n",
    "    print(f\"   Platforms: {df_all['platform'].nunique()}\")\n",
    "    print(f\"   Benchmarks: {df_all['benchmark'].nunique()}\")\n",
    "    print(f\"   Time range: {df_all['timestamp'].min()} to {df_all['timestamp'].max()}\")\n",
    "    print(f\"   Success rate: {df_all['success'].mean() * 100:.1f}%\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data loaded. This notebook will use synthetic data for demonstration.\")\n",
    "\n",
    "    # Generate synthetic data for demonstration\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range(start=\"2024-01-01\", periods=10, freq=\"W\")\n",
    "    queries = [f\"Q{i}\" for i in range(1, 11)]\n",
    "\n",
    "    records = []\n",
    "    for i, date in enumerate(dates):\n",
    "        for query in queries:\n",
    "            # Add gradual performance degradation over time\n",
    "            base_time = 1.0\n",
    "            time_factor = 1 + (i * 0.05)  # 5% degradation per week\n",
    "            noise = np.random.normal(1, 0.1)\n",
    "            exec_time = base_time * time_factor * noise\n",
    "\n",
    "            records.append(\n",
    "                {\n",
    "                    \"timestamp\": date,\n",
    "                    \"platform\": \"DuckDB\",\n",
    "                    \"benchmark\": \"TPC-H\",\n",
    "                    \"scale_factor\": 0.1,\n",
    "                    \"query\": query,\n",
    "                    \"execution_time_s\": exec_time,\n",
    "                    \"success\": True,\n",
    "                    \"file\": f\"synthetic_{date.strftime('%Y%m%d')}.json\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df_all = pd.DataFrame(records)\n",
    "    print(f\"\\nüìä Generated synthetic data: {len(df_all)} executions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance trends over time\n",
    "def analyze_time_series(df: pd.DataFrame, platform: str, query: str) -> Dict:\n",
    "    \"\"\"Analyze performance trend for a specific query over time.\"\"\"\n",
    "\n",
    "    query_data = df[(df[\"platform\"] == platform) & (df[\"query\"] == query) & (df[\"success\"] == True)].sort_values(\n",
    "        \"timestamp\"\n",
    "    )\n",
    "\n",
    "    if len(query_data) < 2:\n",
    "        return None\n",
    "\n",
    "    # Calculate trend using linear regression\n",
    "    x = np.arange(len(query_data))\n",
    "    y = query_data[\"execution_time_s\"].values\n",
    "\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "\n",
    "    # Calculate performance change\n",
    "    first_time = query_data[\"execution_time_s\"].iloc[0]\n",
    "    last_time = query_data[\"execution_time_s\"].iloc[-1]\n",
    "    pct_change = ((last_time - first_time) / first_time) * 100\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"data_points\": len(query_data),\n",
    "        \"first_time\": first_time,\n",
    "        \"last_time\": last_time,\n",
    "        \"pct_change\": pct_change,\n",
    "        \"slope\": slope,\n",
    "        \"r_squared\": r_value**2,\n",
    "        \"p_value\": p_value,\n",
    "        \"trend\": \"increasing\" if slope > 0 else \"decreasing\",\n",
    "    }\n",
    "\n",
    "\n",
    "# Analyze trends for all queries\n",
    "if len(df_all) > 0:\n",
    "    platform = df_all[\"platform\"].iloc[0]\n",
    "    trends = []\n",
    "\n",
    "    for query in df_all[\"query\"].unique():\n",
    "        trend = analyze_time_series(df_all, platform, query)\n",
    "        if trend:\n",
    "            trends.append(trend)\n",
    "\n",
    "    df_trends = pd.DataFrame(trends).sort_values(\"pct_change\", ascending=False)\n",
    "\n",
    "    print(f\"üìà Time Series Analysis ({platform})\\n\")\n",
    "    print(\"Queries with biggest performance changes:\\n\")\n",
    "    print(df_trends.head(10).to_string(index=False))\n",
    "\n",
    "    # Flag concerning trends\n",
    "    degrading = df_trends[df_trends[\"pct_change\"] > 10]\n",
    "    if len(degrading) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  {len(degrading)} queries showing >10% performance degradation\")\n",
    "        for _, row in degrading.iterrows():\n",
    "            print(f\"   {row['query']}: {row['pct_change']:.1f}% slower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Time series plot\n",
    "if len(df_all) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    # Plot top 5 queries with most change\n",
    "    top_queries = df_trends.head(5)[\"query\"].values\n",
    "\n",
    "    for query in top_queries:\n",
    "        query_data = df_all[(df_all[\"query\"] == query) & (df_all[\"success\"] == True)].sort_values(\"timestamp\")\n",
    "\n",
    "        ax.plot(\n",
    "            query_data[\"timestamp\"], query_data[\"execution_time_s\"], marker=\"o\", label=query, linewidth=2, markersize=6\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Date\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Execution Time (seconds)\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_title(\"Query Performance Over Time\\nTop 5 Queries by Change\", fontsize=14, fontweight=\"bold\", pad=20)\n",
    "    ax.legend(title=\"Query\", title_fontsize=11, fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config['output_dir']}/time_series_analysis.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"üíæ Saved: time_series_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_regressions(df: pd.DataFrame, baseline_file: Optional[str] = None, threshold: float = 0.10) -> pd.DataFrame:\n",
    "    \"\"\"Detect performance regressions compared to baseline.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with current results\n",
    "        baseline_file: Path to baseline results (if None, uses oldest results)\n",
    "        threshold: Regression threshold (e.g., 0.10 = 10% slower)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with regression analysis\n",
    "    \"\"\"\n",
    "    # Determine baseline\n",
    "    if baseline_file and Path(baseline_file).exists():\n",
    "        with open(baseline_file) as f:\n",
    "            baseline_result = json.load(f)\n",
    "\n",
    "        baseline_records = []\n",
    "        for qr in baseline_result.get(\"query_results\", []):\n",
    "            baseline_records.append(\n",
    "                {\n",
    "                    \"query\": qr.get(\"query_name\", qr.get(\"query_id\")),\n",
    "                    \"baseline_time\": qr.get(\"execution_time_ms\", 0) / 1000.0,\n",
    "                }\n",
    "            )\n",
    "        df_baseline = pd.DataFrame(baseline_records)\n",
    "    else:\n",
    "        # Use earliest timestamp as baseline\n",
    "        earliest = df[\"timestamp\"].min()\n",
    "        df_baseline = df[df[\"timestamp\"] == earliest].groupby(\"query\").agg({\"execution_time_s\": \"mean\"}).reset_index()\n",
    "        df_baseline.columns = [\"query\", \"baseline_time\"]\n",
    "\n",
    "    # Get latest results\n",
    "    latest = df[\"timestamp\"].max()\n",
    "    df_latest = df[df[\"timestamp\"] == latest].groupby(\"query\").agg({\"execution_time_s\": \"mean\"}).reset_index()\n",
    "    df_latest.columns = [\"query\", \"current_time\"]\n",
    "\n",
    "    # Compare\n",
    "    df_comparison = df_baseline.merge(df_latest, on=\"query\", how=\"inner\")\n",
    "    df_comparison[\"time_diff\"] = df_comparison[\"current_time\"] - df_comparison[\"baseline_time\"]\n",
    "    df_comparison[\"pct_change\"] = (\n",
    "        (df_comparison[\"current_time\"] - df_comparison[\"baseline_time\"]) / df_comparison[\"baseline_time\"] * 100\n",
    "    )\n",
    "\n",
    "    # Flag regressions\n",
    "    df_comparison[\"regression\"] = df_comparison[\"pct_change\"] > (threshold * 100)\n",
    "    df_comparison[\"improvement\"] = df_comparison[\"pct_change\"] < -(threshold * 100)\n",
    "\n",
    "    return df_comparison.sort_values(\"pct_change\", ascending=False)\n",
    "\n",
    "\n",
    "# Detect regressions\n",
    "if len(df_all) > 0:\n",
    "    df_regressions = detect_regressions(\n",
    "        df_all, baseline_file=config.get(\"baseline_file\"), threshold=config[\"regression_threshold\"]\n",
    "    )\n",
    "\n",
    "    print(\"üîç Regression Detection Analysis\\n\")\n",
    "    print(f\"Threshold: {config['regression_threshold'] * 100:.0f}%\\n\")\n",
    "\n",
    "    regressions = df_regressions[df_regressions[\"regression\"]]\n",
    "    improvements = df_regressions[df_regressions[\"improvement\"]]\n",
    "\n",
    "    if len(regressions) > 0:\n",
    "        print(f\"‚ùå {len(regressions)} REGRESSIONS DETECTED:\\n\")\n",
    "        for _, row in regressions.iterrows():\n",
    "            print(\n",
    "                f\"   {row['query']}: {row['baseline_time']:.3f}s ‚Üí {row['current_time']:.3f}s ({row['pct_change']:+.1f}%)\"\n",
    "            )\n",
    "    else:\n",
    "        print(\"‚úÖ No regressions detected\")\n",
    "\n",
    "    if len(improvements) > 0:\n",
    "        print(f\"\\n‚ú® {len(improvements)} IMPROVEMENTS DETECTED:\\n\")\n",
    "        for _, row in improvements.iterrows():\n",
    "            print(\n",
    "                f\"   {row['query']}: {row['baseline_time']:.3f}s ‚Üí {row['current_time']:.3f}s ({row['pct_change']:+.1f}%)\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Regression waterfall chart\n",
    "if len(df_all) > 0 and \"df_regressions\" in locals():\n",
    "    fig, ax = plt.subplots(figsize=(12, max(6, len(df_regressions) * 0.3)))\n",
    "\n",
    "    # Sort by change\n",
    "    df_plot = df_regressions.sort_values(\"pct_change\")\n",
    "\n",
    "    # Color by status\n",
    "    colors = [\n",
    "        \"red\" if reg else \"green\" if imp else \"gray\" for reg, imp in zip(df_plot[\"regression\"], df_plot[\"improvement\"])\n",
    "    ]\n",
    "\n",
    "    bars = ax.barh(df_plot[\"query\"], df_plot[\"pct_change\"], color=colors, alpha=0.7)\n",
    "\n",
    "    # Add threshold lines\n",
    "    ax.axvline(\n",
    "        config[\"regression_threshold\"] * 100,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        alpha=0.5,\n",
    "        label=\"Regression threshold\",\n",
    "    )\n",
    "    ax.axvline(\n",
    "        -config[\"regression_threshold\"] * 100,\n",
    "        color=\"green\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        alpha=0.5,\n",
    "        label=\"Improvement threshold\",\n",
    "    )\n",
    "    ax.axvline(0, color=\"black\", linestyle=\"-\", linewidth=1, alpha=0.3)\n",
    "\n",
    "    ax.set_xlabel(\"Performance Change (%)\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Query\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_title(\n",
    "        \"Regression Detection: Baseline vs Current\\nRed=Regression, Green=Improvement, Gray=Unchanged\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        pad=20,\n",
    "    )\n",
    "    ax.legend()\n",
    "    ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config['output_dir']}/regression_detection.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"üíæ Saved: regression_detection.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Variance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze stability and variance\n",
    "def analyze_variance(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Analyze performance variance and stability for each query.\"\"\"\n",
    "\n",
    "    variance_stats = df.groupby(\"query\").agg({\"execution_time_s\": [\"mean\", \"std\", \"min\", \"max\", \"count\"]}).round(4)\n",
    "\n",
    "    variance_stats.columns = [\"mean\", \"std\", \"min\", \"max\", \"count\"]\n",
    "    variance_stats = variance_stats.reset_index()\n",
    "\n",
    "    # Calculate coefficient of variation (CV = std / mean)\n",
    "    variance_stats[\"cv\"] = (variance_stats[\"std\"] / variance_stats[\"mean\"]).round(3)\n",
    "\n",
    "    # Calculate range\n",
    "    variance_stats[\"range\"] = variance_stats[\"max\"] - variance_stats[\"min\"]\n",
    "\n",
    "    # Stability score (inverse of CV, higher is better)\n",
    "    variance_stats[\"stability_score\"] = (1 / (1 + variance_stats[\"cv\"])).round(3)\n",
    "\n",
    "    return variance_stats.sort_values(\"cv\", ascending=False)\n",
    "\n",
    "\n",
    "if len(df_all) > 0:\n",
    "    df_variance = analyze_variance(df_all[df_all[\"success\"] == True])\n",
    "\n",
    "    print(\"üìä Performance Variance Analysis\\n\")\n",
    "    print(\"Queries with highest variance (least stable):\\n\")\n",
    "    print(df_variance.head(10).to_string(index=False))\n",
    "\n",
    "    print(\"\\nüéØ Stability Insights:\")\n",
    "    print(f\"   Most stable query: {df_variance.iloc[-1]['query']} (CV={df_variance.iloc[-1]['cv']:.3f})\")\n",
    "    print(f\"   Least stable query: {df_variance.iloc[0]['query']} (CV={df_variance.iloc[0]['cv']:.3f})\")\n",
    "    print(f\"   Average CV: {df_variance['cv'].mean():.3f}\")\n",
    "\n",
    "    # Flag highly variable queries\n",
    "    unstable = df_variance[df_variance[\"cv\"] > 0.3]\n",
    "    if len(unstable) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  {len(unstable)} queries with high variance (CV > 0.3):\")\n",
    "        for _, row in unstable.iterrows():\n",
    "            print(f\"   {row['query']}: {row['min']:.3f}s - {row['max']:.3f}s (CV={row['cv']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(df: pd.DataFrame, std_threshold: float = 3.0) -> pd.DataFrame:\n",
    "    \"\"\"Detect anomalous query executions using z-score method.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with query results\n",
    "        std_threshold: Number of standard deviations to flag as anomaly\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with anomalies\n",
    "    \"\"\"\n",
    "    anomalies = []\n",
    "\n",
    "    for query in df[\"query\"].unique():\n",
    "        query_data = df[df[\"query\"] == query].copy()\n",
    "\n",
    "        if len(query_data) < 3:\n",
    "            continue\n",
    "\n",
    "        # Calculate z-scores\n",
    "        mean = query_data[\"execution_time_s\"].mean()\n",
    "        std = query_data[\"execution_time_s\"].std()\n",
    "\n",
    "        if std == 0:\n",
    "            continue\n",
    "\n",
    "        query_data[\"z_score\"] = np.abs((query_data[\"execution_time_s\"] - mean) / std)\n",
    "        query_data[\"is_anomaly\"] = query_data[\"z_score\"] > std_threshold\n",
    "\n",
    "        # Get anomalies\n",
    "        query_anomalies = query_data[query_data[\"is_anomaly\"]]\n",
    "\n",
    "        if len(query_anomalies) > 0:\n",
    "            anomalies.extend(query_anomalies.to_dict(\"records\"))\n",
    "\n",
    "    return pd.DataFrame(anomalies) if anomalies else pd.DataFrame()\n",
    "\n",
    "\n",
    "if len(df_all) > 0:\n",
    "    df_anomalies = detect_anomalies(df_all[df_all[\"success\"] == True], std_threshold=config[\"outlier_std\"])\n",
    "\n",
    "    print(f\"üîç Anomaly Detection (>{config['outlier_std']} std deviations)\\n\")\n",
    "\n",
    "    if len(df_anomalies) > 0:\n",
    "        print(f\"‚ö†Ô∏è  {len(df_anomalies)} anomalous executions detected:\\n\")\n",
    "\n",
    "        for _, row in df_anomalies.sort_values(\"z_score\", ascending=False).head(10).iterrows():\n",
    "            print(f\"   {row['query']} @ {row['timestamp']}: {row['execution_time_s']:.3f}s (z={row['z_score']:.2f})\")\n",
    "\n",
    "        print(\"\\nüìä Anomaly Statistics:\")\n",
    "        print(f\"   Total anomalies: {len(df_anomalies)}\")\n",
    "        print(f\"   Queries affected: {df_anomalies['query'].nunique()}\")\n",
    "        print(f\"   Anomaly rate: {len(df_anomalies) / len(df_all) * 100:.2f}%\")\n",
    "    else:\n",
    "        print(\"‚úÖ No anomalies detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_statistical_significance(df: pd.DataFrame, query: str, before_date: datetime, after_date: datetime) -> Dict:\n",
    "    \"\"\"Test if performance change is statistically significant.\n",
    "\n",
    "    Uses Mann-Whitney U test (non-parametric) to compare distributions.\n",
    "    \"\"\"\n",
    "    query_data = df[df[\"query\"] == query]\n",
    "\n",
    "    before = query_data[query_data[\"timestamp\"] <= before_date][\"execution_time_s\"].values\n",
    "    after = query_data[query_data[\"timestamp\"] > after_date][\"execution_time_s\"].values\n",
    "\n",
    "    if len(before) < 2 or len(after) < 2:\n",
    "        return None\n",
    "\n",
    "    # Mann-Whitney U test\n",
    "    statistic, p_value = stats.mannwhitneyu(before, after, alternative=\"two-sided\")\n",
    "\n",
    "    # Effect size (Cohen's d)\n",
    "    mean_before, mean_after = np.mean(before), np.mean(after)\n",
    "    std_pooled = np.sqrt((np.var(before) + np.var(after)) / 2)\n",
    "    cohens_d = (mean_after - mean_before) / std_pooled if std_pooled > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"n_before\": len(before),\n",
    "        \"n_after\": len(after),\n",
    "        \"mean_before\": mean_before,\n",
    "        \"mean_after\": mean_after,\n",
    "        \"pct_change\": ((mean_after - mean_before) / mean_before * 100) if mean_before > 0 else 0,\n",
    "        \"p_value\": p_value,\n",
    "        \"significant\": p_value < config[\"significance_level\"],\n",
    "        \"cohens_d\": cohens_d,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test significance for queries with changes\n",
    "if len(df_all) > 0 and len(df_all[\"timestamp\"].unique()) >= 2:\n",
    "    # Split at midpoint\n",
    "    timestamps = sorted(df_all[\"timestamp\"].unique())\n",
    "    midpoint = timestamps[len(timestamps) // 2]\n",
    "\n",
    "    sig_tests = []\n",
    "    for query in df_all[\"query\"].unique():\n",
    "        result = test_statistical_significance(\n",
    "            df_all[df_all[\"success\"] == True], query, before_date=midpoint, after_date=midpoint\n",
    "        )\n",
    "        if result:\n",
    "            sig_tests.append(result)\n",
    "\n",
    "    df_sig = pd.DataFrame(sig_tests).sort_values(\"p_value\")\n",
    "\n",
    "    print(\"üìä Statistical Significance Testing\\n\")\n",
    "    print(f\"Comparing before/after {midpoint.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Significance level: Œ± = {config['significance_level']}\\n\")\n",
    "\n",
    "    significant = df_sig[df_sig[\"significant\"]]\n",
    "\n",
    "    if len(significant) > 0:\n",
    "        print(f\"‚úÖ {len(significant)} queries with statistically significant changes:\\n\")\n",
    "        for _, row in significant.head(5).iterrows():\n",
    "            direction = \"slower\" if row[\"pct_change\"] > 0 else \"faster\"\n",
    "            print(\n",
    "                f\"   {row['query']}: {abs(row['pct_change']):.1f}% {direction} (p={row['p_value']:.4f}, d={row['cohens_d']:.2f})\"\n",
    "            )\n",
    "    else:\n",
    "        print(\"No statistically significant changes detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Root Cause Analysis Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_cause_analysis(df: pd.DataFrame, problematic_queries: List[str]) -> Dict:\n",
    "    \"\"\"Analyze patterns in problematic queries to identify root causes.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with all results\n",
    "        problematic_queries: List of query names showing issues\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with potential root cause indicators\n",
    "    \"\"\"\n",
    "    analysis = {\n",
    "        \"affected_queries\": problematic_queries,\n",
    "        \"patterns\": [],\n",
    "    }\n",
    "\n",
    "    problem_data = df[df[\"query\"].isin(problematic_queries)]\n",
    "    normal_data = df[~df[\"query\"].isin(problematic_queries)]\n",
    "\n",
    "    # Pattern 1: Timing patterns\n",
    "    if \"timestamp\" in df.columns:\n",
    "        problem_times = problem_data[\"timestamp\"].dt.hour.value_counts()\n",
    "        if len(problem_times) > 0:\n",
    "            peak_hour = problem_times.idxmax()\n",
    "            analysis[\"patterns\"].append(f\"Most issues occur at hour {peak_hour}:00 (possible resource contention)\")\n",
    "\n",
    "    # Pattern 2: Scale factor correlation\n",
    "    if \"scale_factor\" in df.columns:\n",
    "        problem_scales = problem_data[\"scale_factor\"].unique()\n",
    "        if len(problem_scales) > 0:\n",
    "            analysis[\"patterns\"].append(f\"Issues appear at scale factors: {', '.join(map(str, problem_scales))}\")\n",
    "\n",
    "    # Pattern 3: Query characteristics\n",
    "    # Check if query names have common patterns\n",
    "    query_numbers = []\n",
    "    for q in problematic_queries:\n",
    "        try:\n",
    "            num = int(\"\".join(filter(str.isdigit, q)))\n",
    "            query_numbers.append(num)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if query_numbers:\n",
    "        avg_num = np.mean(query_numbers)\n",
    "        if avg_num > 15:\n",
    "            analysis[\"patterns\"].append(\"Issues concentrated in higher-numbered queries (complex queries)\")\n",
    "\n",
    "    # Pattern 4: Temporal clustering\n",
    "    if len(problem_data) > 1 and \"timestamp\" in df.columns:\n",
    "        time_diffs = problem_data.sort_values(\"timestamp\")[\"timestamp\"].diff().dt.total_seconds()\n",
    "        if time_diffs.median() < 3600:  # Within 1 hour\n",
    "            analysis[\"patterns\"].append(\"Issues clustered in time (possible infrastructure event)\")\n",
    "\n",
    "    return analysis\n",
    "\n",
    "\n",
    "# Perform root cause analysis on regressions\n",
    "if len(df_all) > 0 and \"df_regressions\" in locals():\n",
    "    problematic = df_regressions[df_regressions[\"regression\"]][\"query\"].tolist()\n",
    "\n",
    "    if problematic:\n",
    "        rca = root_cause_analysis(df_all, problematic)\n",
    "\n",
    "        print(\"üîç Root Cause Analysis\\n\")\n",
    "        print(f\"Analyzing {len(rca['affected_queries'])} problematic queries:\\n\")\n",
    "\n",
    "        if rca[\"patterns\"]:\n",
    "            print(\"Potential root cause indicators:\")\n",
    "            for i, pattern in enumerate(rca[\"patterns\"], 1):\n",
    "                print(f\"  {i}. {pattern}\")\n",
    "        else:\n",
    "            print(\"No clear patterns identified. Consider:\")\n",
    "            print(\"  ‚Ä¢ Check for infrastructure changes\")\n",
    "            print(\"  ‚Ä¢ Review query plans for affected queries\")\n",
    "            print(\"  ‚Ä¢ Examine data volume changes\")\n",
    "            print(\"  ‚Ä¢ Look for schema modifications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Analysis Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive analysis report\n",
    "if len(df_all) > 0:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    report = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"analysis_date\": datetime.now().isoformat(),\n",
    "        \"data_range\": {\n",
    "            \"start\": df_all[\"timestamp\"].min().isoformat(),\n",
    "            \"end\": df_all[\"timestamp\"].max().isoformat(),\n",
    "            \"total_executions\": len(df_all),\n",
    "        },\n",
    "        \"regressions\": df_regressions.to_dict(\"records\") if \"df_regressions\" in locals() else [],\n",
    "        \"trends\": df_trends.to_dict(\"records\") if \"df_trends\" in locals() else [],\n",
    "        \"variance\": df_variance.to_dict(\"records\") if \"df_variance\" in locals() else [],\n",
    "        \"anomalies\": df_anomalies.to_dict(\"records\") if \"df_anomalies\" in locals() and len(df_anomalies) > 0 else [],\n",
    "        \"config\": config,\n",
    "    }\n",
    "\n",
    "    report_file = f\"{config['output_dir']}/analysis_report_{timestamp}.json\"\n",
    "    with open(report_file, \"w\") as f:\n",
    "        json.dump(report, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"‚úÖ Exported comprehensive analysis report: {report_file}\")\n",
    "\n",
    "    # Export summary CSV files\n",
    "    if \"df_regressions\" in locals():\n",
    "        df_regressions.to_csv(f\"{config['output_dir']}/regressions_{timestamp}.csv\", index=False)\n",
    "        print(\"‚úÖ Exported regressions CSV\")\n",
    "\n",
    "    if \"df_variance\" in locals():\n",
    "        df_variance.to_csv(f\"{config['output_dir']}/variance_{timestamp}.csv\", index=False)\n",
    "        print(\"‚úÖ Exported variance analysis CSV\")\n",
    "\n",
    "    print(f\"\\nüìÅ All reports saved to: {config['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_all) > 0:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìä ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"\\nüìà Data Overview:\")\n",
    "    print(f\"   Time range: {df_all['timestamp'].min()} to {df_all['timestamp'].max()}\")\n",
    "    print(f\"   Total executions: {len(df_all)}\")\n",
    "    print(f\"   Unique queries: {df_all['query'].nunique()}\")\n",
    "    print(f\"   Success rate: {df_all['success'].mean() * 100:.1f}%\")\n",
    "\n",
    "    if \"df_regressions\" in locals() and len(df_regressions) > 0:\n",
    "        regressions = df_regressions[df_regressions[\"regression\"]]\n",
    "        improvements = df_regressions[df_regressions[\"improvement\"]]\n",
    "        print(\"\\nüîç Regression Analysis:\")\n",
    "        print(f\"   Regressions: {len(regressions)}\")\n",
    "        print(f\"   Improvements: {len(improvements)}\")\n",
    "        print(f\"   Unchanged: {len(df_regressions) - len(regressions) - len(improvements)}\")\n",
    "\n",
    "    if \"df_anomalies\" in locals() and len(df_anomalies) > 0:\n",
    "        print(\"\\n‚ö†Ô∏è  Anomalies:\")\n",
    "        print(f\"   Anomalous executions: {len(df_anomalies)}\")\n",
    "        print(f\"   Anomaly rate: {len(df_anomalies) / len(df_all) * 100:.2f}%\")\n",
    "\n",
    "    if \"df_variance\" in locals():\n",
    "        print(\"\\nüìä Stability:\")\n",
    "        print(f\"   Most stable: {df_variance.iloc[-1]['query']} (CV={df_variance.iloc[-1]['cv']:.3f})\")\n",
    "        print(f\"   Least stable: {df_variance.iloc[0]['query']} (CV={df_variance.iloc[0]['cv']:.3f})\")\n",
    "\n",
    "    print(\"\\nüí° Recommendations:\")\n",
    "\n",
    "    if \"regressions\" in locals() and len(regressions) > 0:\n",
    "        print(f\"   1. Investigate {len(regressions)} queries with performance regressions\")\n",
    "        print(f\"      Priority: {regressions.iloc[0]['query']} ({regressions.iloc[0]['pct_change']:+.1f}%)\")\n",
    "    else:\n",
    "        print(\"   1. ‚úÖ No performance regressions detected\")\n",
    "\n",
    "    if \"df_anomalies\" in locals() and len(df_anomalies) > 0:\n",
    "        print(f\"   2. Review {len(df_anomalies)} anomalous executions for infrastructure issues\")\n",
    "\n",
    "    if \"df_variance\" in locals():\n",
    "        unstable = df_variance[df_variance[\"cv\"] > 0.3]\n",
    "        if len(unstable) > 0:\n",
    "            print(f\"   3. Stabilize {len(unstable)} queries with high variance (CV > 0.3)\")\n",
    "\n",
    "    print(\"   4. Establish baseline for ongoing regression testing\")\n",
    "    print(\"   5. Integrate analysis into CI/CD pipeline\")\n",
    "    print(\"   6. Set up automated alerts for regressions and anomalies\")\n",
    "\n",
    "    print(f\"\\nüìÅ Detailed reports: {config['output_dir']}\")\n",
    "    print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
