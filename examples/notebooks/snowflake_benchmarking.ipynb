{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# BenchBox Snowflake Benchmarking\n",
    "\n",
    "This notebook demonstrates how to use BenchBox to benchmark **Snowflake** data warehouse.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Authenticate with Snowflake using multiple methods (password, key-pair, SSO)\n",
    "- Run TPC-H, TPC-DS, and ClickBench benchmarks\n",
    "- Compare warehouse sizes (XS, S, M, L, XL)\n",
    "- Use clustering keys and micro-partitions for optimization\n",
    "- Monitor credit consumption and query performance\n",
    "- Leverage Time Travel, Zero-Copy Cloning, and result caching\n",
    "- Troubleshoot common Snowflake issues\n",
    "\n",
    "**Prerequisites:**\n",
    "- Snowflake account\n",
    "- User with appropriate privileges (SYSADMIN or ACCOUNTADMIN)\n",
    "- Virtual warehouse (COMPUTE_WH or custom)\n",
    "- Database for benchmark data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e091cc9",
   "metadata": {},
   "source": [
    "### 1.1 Install Required Libraries\n",
    "\n",
    "Install BenchBox and Snowflake connector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install benchbox snowflake-connector-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import_section",
   "metadata": {},
   "source": [
    "### 1.2 Import Libraries\n",
    "\n",
    "Import BenchBox components and visualization libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Snowflake imports\n",
    "import snowflake.connector\n",
    "\n",
    "# BenchBox imports\n",
    "from benchbox.core.config import BenchmarkConfig, DatabaseConfig\n",
    "from benchbox.core.runner import LifecyclePhases, run_benchmark_lifecycle\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auth_section",
   "metadata": {},
   "source": [
    "### 1.3 Authentication\n",
    "\n",
    "Snowflake supports multiple authentication methods:\n",
    "\n",
    "**Method 1: Username/Password** - Simple for development\n",
    "```bash\n",
    "export SNOWFLAKE_ACCOUNT='xy12345'\n",
    "export SNOWFLAKE_USER='username'\n",
    "export SNOWFLAKE_PASSWORD='password'\n",
    "```\n",
    "\n",
    "**Method 2: Key-Pair Authentication** - Recommended for production\n",
    "```bash\n",
    "# Generate key pair\n",
    "openssl genrsa -out rsa_key.pem 2048\n",
    "openssl rsa -in rsa_key.pem -pubout -out rsa_key.pub\n",
    "\n",
    "# Configure in Snowflake\n",
    "ALTER USER username SET RSA_PUBLIC_KEY='MII...';\n",
    "\n",
    "export SNOWFLAKE_PRIVATE_KEY_PATH='/path/to/rsa_key.pem'\n",
    "```\n",
    "\n",
    "**Method 3: SSO/SAML** - Enterprise authentication\n",
    "```python\n",
    "authenticator='externalbrowser'  # Opens browser for SSO\n",
    "```\n",
    "\n",
    "This notebook will try environment variables with password fallback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Snowflake connection\n",
    "try:\n",
    "    # Try environment variables first\n",
    "    SNOWFLAKE_ACCOUNT = os.environ.get(\"SNOWFLAKE_ACCOUNT\")\n",
    "    SNOWFLAKE_USER = os.environ.get(\"SNOWFLAKE_USER\")\n",
    "    SNOWFLAKE_PASSWORD = os.environ.get(\"SNOWFLAKE_PASSWORD\")\n",
    "    SNOWFLAKE_WAREHOUSE = os.environ.get(\"SNOWFLAKE_WAREHOUSE\", \"COMPUTE_WH\")\n",
    "    SNOWFLAKE_DATABASE = os.environ.get(\"SNOWFLAKE_DATABASE\", \"BENCHBOX\")\n",
    "    SNOWFLAKE_SCHEMA = os.environ.get(\"SNOWFLAKE_SCHEMA\", \"PUBLIC\")\n",
    "\n",
    "    if not SNOWFLAKE_ACCOUNT or not SNOWFLAKE_USER:\n",
    "        print(\"‚ö†Ô∏è  Required environment variables not set\")\n",
    "        print(\"\\nüí° Set up authentication:\")\n",
    "        print(\"  Option 1 (Password):\")\n",
    "        print(\"    export SNOWFLAKE_ACCOUNT='xy12345'\")\n",
    "        print(\"    export SNOWFLAKE_USER='username'\")\n",
    "        print(\"    export SNOWFLAKE_PASSWORD='password'\")\n",
    "        print(\"  Option 2 (Key-Pair):\")\n",
    "        print(\"    export SNOWFLAKE_PRIVATE_KEY_PATH='/path/to/rsa_key.pem'\")\n",
    "        print(\"  Option 3 (SSO):\")\n",
    "        print(\"    Set authenticator='externalbrowser' in platform_cfg\")\n",
    "        raise ValueError(\"Missing required environment variables\")\n",
    "\n",
    "    if not SNOWFLAKE_PASSWORD and not os.environ.get(\"SNOWFLAKE_PRIVATE_KEY_PATH\"):\n",
    "        print(\"‚ö†Ô∏è  No password or private key configured\")\n",
    "        print(\"üí° Set SNOWFLAKE_PASSWORD or SNOWFLAKE_PRIVATE_KEY_PATH\")\n",
    "        raise ValueError(\"No authentication method configured\")\n",
    "\n",
    "    print(f\"‚úÖ Using account: {SNOWFLAKE_ACCOUNT}\")\n",
    "    print(f\"‚úÖ User: {SNOWFLAKE_USER}\")\n",
    "    print(f\"‚úÖ Warehouse: {SNOWFLAKE_WAREHOUSE}\")\n",
    "    print(f\"‚úÖ Database: {SNOWFLAKE_DATABASE}\")\n",
    "    print(f\"‚úÖ Schema: {SNOWFLAKE_SCHEMA}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Authentication error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connection_test_section",
   "metadata": {},
   "source": [
    "### 1.4 Test Connection\n",
    "\n",
    "Verify connectivity and permissions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connection_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize Snowflake connection\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=SNOWFLAKE_USER,\n",
    "        password=SNOWFLAKE_PASSWORD,\n",
    "        account=SNOWFLAKE_ACCOUNT,\n",
    "        warehouse=SNOWFLAKE_WAREHOUSE,\n",
    "        database=SNOWFLAKE_DATABASE,\n",
    "        schema=SNOWFLAKE_SCHEMA,\n",
    "    )\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Test 1: Check connection\n",
    "    print(\"1Ô∏è‚É£ Testing connection...\")\n",
    "    cursor.execute(\"SELECT CURRENT_VERSION()\")\n",
    "    version = cursor.fetchone()[0]\n",
    "    print(f\"   ‚úÖ Connected to Snowflake version: {version}\")\n",
    "\n",
    "    # Test 2: Check warehouse status\n",
    "    print(\"\\n2Ô∏è‚É£ Checking warehouse status...\")\n",
    "    cursor.execute(f\"SHOW WAREHOUSES LIKE '{SNOWFLAKE_WAREHOUSE}'\")\n",
    "    warehouse_info = cursor.fetchone()\n",
    "    if warehouse_info:\n",
    "        print(f\"   ‚úÖ Warehouse: {warehouse_info[0]}\")\n",
    "        print(f\"   Size: {warehouse_info[3]}\")\n",
    "        print(f\"   State: {warehouse_info[1]}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Warehouse '{SNOWFLAKE_WAREHOUSE}' not found\")\n",
    "\n",
    "    # Test 3: Check database exists\n",
    "    print(\"\\n3Ô∏è‚É£ Checking database...\")\n",
    "    cursor.execute(f\"SHOW DATABASES LIKE '{SNOWFLAKE_DATABASE}'\")\n",
    "    db_exists = cursor.fetchone()\n",
    "    if db_exists:\n",
    "        print(f\"   ‚úÖ Database exists: {SNOWFLAKE_DATABASE}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Database '{SNOWFLAKE_DATABASE}' does not exist\")\n",
    "        print(\"   üí° Creating database...\")\n",
    "        cursor.execute(f\"CREATE DATABASE IF NOT EXISTS {SNOWFLAKE_DATABASE}\")\n",
    "        print(f\"   ‚úÖ Created database: {SNOWFLAKE_DATABASE}\")\n",
    "\n",
    "    # Test 4: Run simple query\n",
    "    print(\"\\n4Ô∏è‚É£ Testing query execution...\")\n",
    "    cursor.execute(\"SELECT 1 as test\")\n",
    "    result = cursor.fetchone()\n",
    "    print(\"   ‚úÖ Query executed successfully\")\n",
    "\n",
    "    # Test 5: Check current role\n",
    "    print(\"\\n5Ô∏è‚É£ Checking user role...\")\n",
    "    cursor.execute(\"SELECT CURRENT_ROLE()\")\n",
    "    role = cursor.fetchone()[0]\n",
    "    print(f\"   ‚úÖ Current role: {role}\")\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    print(\"\\n‚úÖ All connection tests passed!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection test failed: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"  1. Verify account identifier (format: xy12345.region.cloud)\")\n",
    "    print(\"  2. Check username and password\")\n",
    "    print(\"  3. Verify warehouse is running\")\n",
    "    print(\"  4. Check network policies and IP allowlists\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warehouse_sizing_section",
   "metadata": {},
   "source": [
    "### 1.5 Warehouse Sizing Guide\n",
    "\n",
    "Understanding Snowflake warehouse sizes and pricing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warehouse_sizing",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Snowflake Warehouse Sizing Guide\\n\")\n",
    "print(\"Size    Credits/Hour  Servers  Use Case\")\n",
    "print(\"=\" * 60)\n",
    "print(\"X-Small      1         1      Development, small datasets\")\n",
    "print(\"Small        2         2      Small workloads, testing\")\n",
    "print(\"Medium       4         4      Medium workloads, production\")\n",
    "print(\"Large        8         8      Large workloads, analytics\")\n",
    "print(\"X-Large     16        16      Very large datasets\")\n",
    "print(\"2X-Large    32        32      Massive concurrent queries\")\n",
    "print(\"3X-Large    64        64      Extreme workloads\")\n",
    "print(\"4X-Large   128       128      Maximum performance\\n\")\n",
    "\n",
    "print(\"üí∞ Pricing (On-Demand):\")\n",
    "print(\"  - Standard Edition: $2-4 per credit\")\n",
    "print(\"  - Enterprise Edition: $3-5 per credit\")\n",
    "print(\"  - Business Critical: $4-6 per credit\\n\")\n",
    "\n",
    "print(\"üí° Sizing Guidelines:\")\n",
    "print(\"  - Start with X-Small for benchmarking (SF 0.01-0.1)\")\n",
    "print(\"  - Use Small-Medium for production workloads (SF 1-10)\")\n",
    "print(\"  - Scale up for larger datasets or faster execution\")\n",
    "print(\"  - Use multi-cluster warehouses for concurrency\\n\")\n",
    "\n",
    "print(\"‚ö° Auto-Scaling:\")\n",
    "print(\"  - Auto-suspend: Suspend after N minutes idle\")\n",
    "print(\"  - Auto-resume: Resume automatically on query\")\n",
    "print(\"  - Multi-cluster: Scale out for concurrency (1-10 clusters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_overview_section",
   "metadata": {},
   "source": [
    "### 1.6 Configuration Overview\n",
    "\n",
    "Summary of your Snowflake configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Snowflake Configuration Summary\\n\")\n",
    "print(f\"Account:   {SNOWFLAKE_ACCOUNT}\")\n",
    "print(f\"User:      {SNOWFLAKE_USER}\")\n",
    "print(f\"Warehouse: {SNOWFLAKE_WAREHOUSE}\")\n",
    "print(f\"Database:  {SNOWFLAKE_DATABASE}\")\n",
    "print(f\"Schema:    {SNOWFLAKE_SCHEMA}\")\n",
    "print(\"\\nüí° Tip: Use separate warehouses for ETL and analytics workloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## 2. Quick Start Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick_start_intro",
   "metadata": {},
   "source": [
    "### 2.1 Run TPC-H Power Test\n",
    "\n",
    "Run a TPC-H power test at scale factor 0.01 (~10MB data).\n",
    "\n",
    "**What happens:**\n",
    "1. Generate TPC-H data (8 tables: customer, orders, lineitem, etc.)\n",
    "2. Load data into Snowflake tables\n",
    "3. Execute 22 TPC-H queries sequentially\n",
    "4. Collect execution times and warehouse metrics\n",
    "\n",
    "**Expected time:** 2-3 minutes (X-Small warehouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure database connection\n",
    "db_cfg = DatabaseConfig(type=\"snowflake\", name=\"snowflake_benchbox\")\n",
    "\n",
    "# Snowflake platform configuration\n",
    "platform_cfg = {\n",
    "    \"account\": SNOWFLAKE_ACCOUNT,\n",
    "    \"user\": SNOWFLAKE_USER,\n",
    "    \"password\": SNOWFLAKE_PASSWORD,\n",
    "    \"warehouse\": SNOWFLAKE_WAREHOUSE,\n",
    "    \"database\": SNOWFLAKE_DATABASE,\n",
    "    \"schema\": SNOWFLAKE_SCHEMA,\n",
    "    # Optional: Use key-pair authentication\n",
    "    # \"private_key_path\": \"/path/to/rsa_key.pem\"\n",
    "}\n",
    "\n",
    "# Configure TPC-H benchmark\n",
    "bench_cfg = BenchmarkConfig(\n",
    "    name=\"tpch\",\n",
    "    display_name=\"TPC-H\",\n",
    "    scale_factor=0.01,  # 10MB dataset\n",
    "    test_execution_type=\"power\",\n",
    ")\n",
    "\n",
    "# Run full lifecycle: generate ‚Üí load ‚Üí execute\n",
    "print(\"üöÄ Starting TPC-H power test on Snowflake...\\n\")\n",
    "\n",
    "results = run_benchmark_lifecycle(\n",
    "    benchmark_config=bench_cfg,\n",
    "    database_config=db_cfg,\n",
    "    system_profile=None,\n",
    "    platform_config=platform_cfg,\n",
    "    phases=LifecyclePhases(\n",
    "        generate=True,  # Generate TPC-H data\n",
    "        load=True,  # Load into Snowflake\n",
    "        execute=True,  # Execute 22 queries\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Power test completed on Snowflake\")\n",
    "print(f\"Total queries executed: {len(results.query_results)}\")\n",
    "print(f\"Results saved to: {results.results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz_section",
   "metadata": {},
   "source": [
    "### 2.2 Visualize Results\n",
    "\n",
    "Create a bar chart of query execution times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results.query_results:\n",
    "    # Extract query names and execution times\n",
    "    query_names = [qr.query_name for qr in results.query_results]\n",
    "    execution_times = [qr.execution_time for qr in results.query_results]\n",
    "\n",
    "    # Create bar chart\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    bars = ax.bar(query_names, execution_times, color=\"#29B5E8\", alpha=0.8)  # Snowflake Blue\n",
    "\n",
    "    # Highlight slowest queries (top 30%)\n",
    "    max_time = max(execution_times)\n",
    "    for i, (bar, time) in enumerate(zip(bars, execution_times)):\n",
    "        if time > max_time * 0.7:\n",
    "            bar.set_color(\"#F26B1D\")  # Snowflake Orange for slow queries\n",
    "\n",
    "    ax.set_xlabel(\"Query\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Execution Time (seconds)\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_title(\"TPC-H Query Performance on Snowflake\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"\\nüìä Performance Summary:\")\n",
    "    print(f\"  Total time: {sum(execution_times):.2f}s\")\n",
    "    print(f\"  Average: {sum(execution_times) / len(execution_times):.2f}s\")\n",
    "    print(f\"  Fastest: {min(execution_times):.2f}s ({query_names[execution_times.index(min(execution_times))]})\")\n",
    "    print(f\"  Slowest: {max(execution_times):.2f}s ({query_names[execution_times.index(max(execution_times))]})\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No query results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warehouse_monitoring_section",
   "metadata": {},
   "source": [
    "### 2.3 Monitor Warehouse Usage\n",
    "\n",
    "Query ACCOUNT_USAGE.QUERY_HISTORY to analyze resource consumption:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warehouse_monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Connect to Snowflake\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=SNOWFLAKE_USER,\n",
    "        password=SNOWFLAKE_PASSWORD,\n",
    "        account=SNOWFLAKE_ACCOUNT,\n",
    "        warehouse=SNOWFLAKE_WAREHOUSE,\n",
    "        database=SNOWFLAKE_DATABASE,\n",
    "        schema=SNOWFLAKE_SCHEMA,\n",
    "    )\n",
    "\n",
    "    # Query recent query history\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        query_id,\n",
    "        query_text,\n",
    "        warehouse_name,\n",
    "        warehouse_size,\n",
    "        execution_time / 1000.0 as execution_seconds,\n",
    "        credits_used_cloud_services,\n",
    "        bytes_scanned,\n",
    "        rows_produced\n",
    "    FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY\n",
    "    WHERE start_time > DATEADD(hour, -1, CURRENT_TIMESTAMP())\n",
    "        AND warehouse_name = '{SNOWFLAKE_WAREHOUSE}'\n",
    "        AND query_type = 'SELECT'\n",
    "    ORDER BY start_time DESC\n",
    "    LIMIT 50\n",
    "    \"\"\"\n",
    "\n",
    "    df_history = pd.read_sql(query, conn)\n",
    "\n",
    "    if not df_history.empty:\n",
    "        print(\"üìä Recent Query History:\\n\")\n",
    "        print(f\"Total queries: {len(df_history)}\")\n",
    "        print(f\"Total execution time: {df_history['EXECUTION_SECONDS'].sum():.2f}s\")\n",
    "        print(f\"Total credits used: {df_history['CREDITS_USED_CLOUD_SERVICES'].sum():.6f}\")\n",
    "        print(f\"Total bytes scanned: {df_history['BYTES_SCANNED'].sum() / (1024**3):.2f} GB\")\n",
    "        print(f\"Total rows produced: {df_history['ROWS_PRODUCED'].sum():,}\")\n",
    "\n",
    "        print(\"\\nüí° Note: ACCOUNT_USAGE views have latency (up to 45 minutes)\")\n",
    "        print(\"   For real-time data, use INFORMATION_SCHEMA.QUERY_HISTORY instead\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No recent query history found\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not query ACCOUNT_USAGE: {e}\")\n",
    "    print(\"This requires ACCOUNTADMIN or appropriate grants on ACCOUNT_USAGE schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_overview_section",
   "metadata": {},
   "source": [
    "### 2.4 Results Overview\n",
    "\n",
    "View comprehensive results summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results_overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã Benchmark Results Summary\\n\")\n",
    "print(f\"Benchmark: {results.benchmark_name}\")\n",
    "print(f\"Platform: {results.database_config.type}\")\n",
    "print(f\"Scale Factor: {results.benchmark_config.scale_factor}\")\n",
    "print(f\"Test Type: {results.benchmark_config.test_execution_type}\")\n",
    "print(\"\\nExecution:\")\n",
    "print(f\"  Start: {results.execution_metadata.start_time}\")\n",
    "print(f\"  End: {results.execution_metadata.end_time}\")\n",
    "print(f\"  Duration: {results.execution_metadata.total_duration:.2f}s\")\n",
    "print(\"\\nQueries:\")\n",
    "print(f\"  Total: {len(results.query_results)}\")\n",
    "print(f\"  Successful: {sum(1 for qr in results.query_results if qr.success)}\")\n",
    "print(f\"  Failed: {sum(1 for qr in results.query_results if not qr.success)}\")\n",
    "print(\"\\nResults Location:\")\n",
    "print(f\"  {results.results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## 3. Advanced Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tpcds_section",
   "metadata": {},
   "source": [
    "### 3.1 TPC-DS Benchmark\n",
    "\n",
    "Run TPC-DS (99 queries, more complex than TPC-H):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tpcds_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPC-DS configuration\n",
    "tpcds_cfg = BenchmarkConfig(\n",
    "    name=\"tpcds\",\n",
    "    display_name=\"TPC-DS\",\n",
    "    scale_factor=0.01,  # 10MB dataset\n",
    "    test_execution_type=\"power\",\n",
    ")\n",
    "\n",
    "print(\"üöÄ Running TPC-DS benchmark...\")\n",
    "print(\"‚ö†Ô∏è  Note: TPC-DS has 99 queries and will take longer\\n\")\n",
    "\n",
    "tpcds_results = run_benchmark_lifecycle(\n",
    "    benchmark_config=tpcds_cfg,\n",
    "    database_config=db_cfg,\n",
    "    system_profile=None,\n",
    "    platform_config=platform_cfg,\n",
    "    phases=LifecyclePhases(generate=True, load=True, execute=True),\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ TPC-DS completed: {len(tpcds_results.query_results)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warehouse_scaling_section",
   "metadata": {},
   "source": [
    "### 3.2 Warehouse Size Comparison\n",
    "\n",
    "Compare performance across different warehouse sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warehouse_scaling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple warehouse sizes\n",
    "warehouse_sizes = [\"X-Small\", \"Small\", \"Medium\"]\n",
    "size_results = {}\n",
    "\n",
    "for size in warehouse_sizes:\n",
    "    print(f\"\\nüìä Testing warehouse size: {size}...\")\n",
    "\n",
    "    # Create/resize warehouse\n",
    "    test_warehouse = f\"BENCHBOX_{size.upper().replace('-', '_')}\"\n",
    "\n",
    "    size_cfg = {\n",
    "        \"account\": SNOWFLAKE_ACCOUNT,\n",
    "        \"user\": SNOWFLAKE_USER,\n",
    "        \"password\": SNOWFLAKE_PASSWORD,\n",
    "        \"warehouse\": test_warehouse,\n",
    "        \"warehouse_size\": size,  # Specify size\n",
    "        \"database\": SNOWFLAKE_DATABASE,\n",
    "        \"schema\": SNOWFLAKE_SCHEMA,\n",
    "    }\n",
    "\n",
    "    size_bench_cfg = BenchmarkConfig(\n",
    "        name=\"tpch\",\n",
    "        display_name=f\"TPC-H {size}\",\n",
    "        scale_factor=0.1,  # Larger dataset to see differences\n",
    "        test_execution_type=\"power\",\n",
    "    )\n",
    "\n",
    "    size_res = run_benchmark_lifecycle(\n",
    "        benchmark_config=size_bench_cfg,\n",
    "        database_config=db_cfg,\n",
    "        system_profile=None,\n",
    "        platform_config=size_cfg,\n",
    "        phases=LifecyclePhases(generate=False, load=False, execute=True),  # Reuse data\n",
    "    )\n",
    "\n",
    "    size_results[size] = size_res\n",
    "    avg_time = sum(qr.execution_time for qr in size_res.query_results) / len(size_res.query_results)\n",
    "    print(f\"  Average query time: {avg_time:.2f}s\")\n",
    "\n",
    "print(\"\\n‚úÖ Warehouse size comparison complete\")\n",
    "print(\"\\nüí° Findings:\")\n",
    "print(\"  - Larger warehouses execute faster but cost more credits\")\n",
    "print(\"  - Use right-sized warehouses for your workload\")\n",
    "print(\"  - Consider multi-cluster for concurrency, not speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clustering_section",
   "metadata": {},
   "source": [
    "### 3.3 Clustering Keys\n",
    "\n",
    "Use clustering keys to improve query performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clustering_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Testing clustering keys...\\n\")\n",
    "\n",
    "# Configure platform with clustering\n",
    "clustering_cfg = {\n",
    "    \"account\": SNOWFLAKE_ACCOUNT,\n",
    "    \"user\": SNOWFLAKE_USER,\n",
    "    \"password\": SNOWFLAKE_PASSWORD,\n",
    "    \"warehouse\": SNOWFLAKE_WAREHOUSE,\n",
    "    \"database\": SNOWFLAKE_DATABASE,\n",
    "    \"schema\": SNOWFLAKE_SCHEMA,\n",
    "    \"table_options\": {\n",
    "        \"lineitem\": {\"cluster_by\": \"(l_shipdate, l_returnflag)\"},\n",
    "        \"orders\": {\"cluster_by\": \"(o_orderdate)\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Clustering configuration:\")\n",
    "print(\"  - lineitem: cluster by (l_shipdate, l_returnflag)\")\n",
    "print(\"  - orders: cluster by (o_orderdate)\\n\")\n",
    "\n",
    "cluster_cfg = BenchmarkConfig(\n",
    "    name=\"tpch\",\n",
    "    display_name=\"TPC-H Clustered\",\n",
    "    scale_factor=1.0,  # Larger dataset to see benefits\n",
    "    test_execution_type=\"power\",\n",
    ")\n",
    "\n",
    "cluster_results = run_benchmark_lifecycle(\n",
    "    benchmark_config=cluster_cfg,\n",
    "    database_config=db_cfg,\n",
    "    system_profile=None,\n",
    "    platform_config=clustering_cfg,\n",
    "    phases=LifecyclePhases(generate=True, load=True, execute=True),\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Clustered benchmark complete\")\n",
    "print(\"\\nüí° Benefits of Clustering:\")\n",
    "print(\"  - Faster pruning (skip micro-partitions)\")\n",
    "print(\"  - Better for date range queries\")\n",
    "print(\"  - Automatic maintenance (but costs credits)\")\n",
    "print(\"  - Best for large tables (>1 TB)\")\n",
    "print(\"\\n‚ö†Ô∏è  Clustering Costs:\")\n",
    "print(\"  - Automatic reclustering consumes credits\")\n",
    "print(\"  - Monitor with AUTOMATIC_CLUSTERING_HISTORY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query_subset_section",
   "metadata": {},
   "source": [
    "### 3.4 Query Subset Selection\n",
    "\n",
    "Run only specific queries for faster iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_subset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only queries 1, 6, and 14 (fast queries for CI/CD)\n",
    "subset_cfg = BenchmarkConfig(\n",
    "    name=\"tpch\",\n",
    "    display_name=\"TPC-H Subset\",\n",
    "    scale_factor=0.01,\n",
    "    test_execution_type=\"power\",\n",
    "    query_filter=[1, 6, 14],  # Only these queries\n",
    ")\n",
    "\n",
    "print(\"üöÄ Running query subset (1, 6, 14)...\\n\")\n",
    "\n",
    "subset_results = run_benchmark_lifecycle(\n",
    "    benchmark_config=subset_cfg,\n",
    "    database_config=db_cfg,\n",
    "    system_profile=None,\n",
    "    platform_config=platform_cfg,\n",
    "    phases=LifecyclePhases(generate=False, load=False, execute=True),  # Reuse data\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Subset complete: {len(subset_results.query_results)} queries\")\n",
    "print(\"\\nüí° Use case: Fast regression testing in CI/CD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "result_cache_section",
   "metadata": {},
   "source": [
    "### 3.5 Result Caching\n",
    "\n",
    "Leverage Snowflake's automatic result caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "result_cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Result Caching Overview\\n\")\n",
    "print(\"What is Result Caching?\")\n",
    "print(\"  - Snowflake caches query results for 24 hours\")\n",
    "print(\"  - Identical queries return cached results instantly\")\n",
    "print(\"  - No compute charges for cached results\")\n",
    "print(\"  - Cache invalidated on data changes\\n\")\n",
    "\n",
    "print(\"Requirements for Cache Hit:\")\n",
    "print(\"  1. Exact same SQL text (byte-for-byte)\")\n",
    "print(\"  2. Same role and permissions\")\n",
    "print(\"  3. Table data hasn't changed\")\n",
    "print(\"  4. Within 24-hour window\\n\")\n",
    "\n",
    "print(\"üí∞ Cost Savings:\")\n",
    "print(\"  - No warehouse compute charges\")\n",
    "print(\"  - Small cloud services charge (typically $0)\")\n",
    "print(\"  - Instant response time\")\n",
    "print(\"  - Ideal for dashboards and repeated queries\\n\")\n",
    "\n",
    "print(\"üîç Check Cache Usage:\")\n",
    "print(\"  SELECT query_id, query_text, bytes_scanned\")\n",
    "print(\"  FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY\")\n",
    "print(\"  WHERE bytes_scanned = 0  -- Indicates cache hit\")\n",
    "print(\"    AND execution_time < 100;  -- Fast execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate_compute_section",
   "metadata": {},
   "source": [
    "### 3.6 Separate Compute for Load vs Query\n",
    "\n",
    "Use different warehouses for ETL and analytics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate_compute",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Best Practice: Separate Compute for ETL and Analytics\\n\")\n",
    "\n",
    "# Configure separate warehouses\n",
    "load_cfg = {\n",
    "    \"account\": SNOWFLAKE_ACCOUNT,\n",
    "    \"user\": SNOWFLAKE_USER,\n",
    "    \"password\": SNOWFLAKE_PASSWORD,\n",
    "    \"warehouse\": \"LOADING_WH\",  # Dedicated for data loading\n",
    "    \"database\": SNOWFLAKE_DATABASE,\n",
    "    \"schema\": SNOWFLAKE_SCHEMA,\n",
    "}\n",
    "\n",
    "query_cfg = {\n",
    "    \"account\": SNOWFLAKE_ACCOUNT,\n",
    "    \"user\": SNOWFLAKE_USER,\n",
    "    \"password\": SNOWFLAKE_PASSWORD,\n",
    "    \"warehouse\": \"ANALYTICS_WH\",  # Dedicated for analytics\n",
    "    \"database\": SNOWFLAKE_DATABASE,\n",
    "    \"schema\": SNOWFLAKE_SCHEMA,\n",
    "}\n",
    "\n",
    "print(\"Warehouse Strategy:\")\n",
    "print(\"  - LOADING_WH (Medium): For data ingestion and transformation\")\n",
    "print(\"  - ANALYTICS_WH (Large): For analytics and reporting\\n\")\n",
    "\n",
    "print(\"üí° Benefits:\")\n",
    "print(\"  - Isolate workloads (no resource contention)\")\n",
    "print(\"  - Independent scaling\")\n",
    "print(\"  - Better cost tracking (separate credit usage)\")\n",
    "print(\"  - Optimize warehouse size per workload type\\n\")\n",
    "\n",
    "print(\"üìä Example Configuration:\")\n",
    "print(\"  ETL Warehouse:\")\n",
    "print(\"    - Size: Medium (4 servers)\")\n",
    "print(\"    - Auto-suspend: 5 minutes\")\n",
    "print(\"    - Max clusters: 1 (serial ETL)\")\n",
    "print(\"  Analytics Warehouse:\")\n",
    "print(\"    - Size: Large (8 servers)\")\n",
    "print(\"    - Auto-suspend: 10 minutes\")\n",
    "print(\"    - Max clusters: 3 (concurrent users)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "throughput_section",
   "metadata": {},
   "source": [
    "### 3.7 Throughput Testing\n",
    "\n",
    "Run concurrent queries to test throughput:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "throughput_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throughput test configuration\n",
    "throughput_cfg = BenchmarkConfig(\n",
    "    name=\"tpch\",\n",
    "    display_name=\"TPC-H Throughput\",\n",
    "    scale_factor=0.1,\n",
    "    test_execution_type=\"throughput\",\n",
    "    throughput_streams=4,  # 4 concurrent streams\n",
    ")\n",
    "\n",
    "print(\"üöÄ Running throughput test (4 concurrent streams)...\\n\")\n",
    "\n",
    "throughput_results = run_benchmark_lifecycle(\n",
    "    benchmark_config=throughput_cfg,\n",
    "    database_config=db_cfg,\n",
    "    system_profile=None,\n",
    "    platform_config=platform_cfg,\n",
    "    phases=LifecyclePhases(generate=False, load=False, execute=True),\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Throughput test complete: {len(throughput_results.query_results)} queries\")\n",
    "print(\"\\nüí° Snowflake Concurrency:\")\n",
    "print(\"  - Single cluster: queues after ~8-16 concurrent queries\")\n",
    "print(\"  - Multi-cluster: scales out automatically (1-10 clusters)\")\n",
    "print(\"  - Query queuing: FIFO with priority support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "result_comparison_section",
   "metadata": {},
   "source": [
    "### 3.8 Result Comparison\n",
    "\n",
    "Compare results across different configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "result_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare results\n",
    "if \"results\" in locals() and \"tpcds_results\" in locals():\n",
    "    tpch_avg = sum(qr.execution_time for qr in results.query_results) / len(results.query_results)\n",
    "    tpcds_avg = sum(qr.execution_time for qr in tpcds_results.query_results) / len(tpcds_results.query_results)\n",
    "\n",
    "    # Create comparison visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    benchmarks = [\"TPC-H\\n(22 queries)\", \"TPC-DS\\n(99 queries)\"]\n",
    "    avg_times = [tpch_avg, tpcds_avg]\n",
    "    total_times = [\n",
    "        sum(qr.execution_time for qr in results.query_results),\n",
    "        sum(qr.execution_time for qr in tpcds_results.query_results),\n",
    "    ]\n",
    "\n",
    "    x = range(len(benchmarks))\n",
    "    width = 0.35\n",
    "\n",
    "    ax.bar([i - width / 2 for i in x], avg_times, width, label=\"Avg Time/Query\", color=\"#29B5E8\")\n",
    "    ax.bar([i + width / 2 for i in x], total_times, width, label=\"Total Time\", color=\"#F26B1D\")\n",
    "\n",
    "    ax.set_ylabel(\"Time (seconds)\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_title(\"Benchmark Comparison on Snowflake\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(benchmarks)\n",
    "    ax.legend()\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nüìä Comparison Summary:\")\n",
    "    print(f\"  TPC-H: {tpch_avg:.2f}s avg, {total_times[0]:.2f}s total\")\n",
    "    print(f\"  TPC-DS: {tpcds_avg:.2f}s avg, {total_times[1]:.2f}s total\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Run both TPC-H and TPC-DS benchmarks first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export_section",
   "metadata": {},
   "source": [
    "### 3.9 Export Results\n",
    "\n",
    "Export results in multiple formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchbox.core.results.exporter import ResultExporter\n",
    "\n",
    "# Export to JSON (default)\n",
    "exporter = ResultExporter(results)\n",
    "json_path = exporter.export(format=\"json\")\n",
    "print(f\"‚úÖ Exported to JSON: {json_path}\")\n",
    "\n",
    "# Export to CSV\n",
    "csv_path = exporter.export(format=\"csv\")\n",
    "print(f\"‚úÖ Exported to CSV: {csv_path}\")\n",
    "\n",
    "# Export to HTML\n",
    "html_path = exporter.export(format=\"html\")\n",
    "print(f\"‚úÖ Exported to HTML: {html_path}\")\n",
    "\n",
    "print(\"\\nüí° Use these exports for:\")\n",
    "print(\"  - JSON: API integration, programmatic analysis\")\n",
    "print(\"  - CSV: Excel, data science tools, Snowsight\")\n",
    "print(\"  - HTML: Shareable reports, documentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost_analysis_section",
   "metadata": {},
   "source": [
    "### 3.10 Credit Consumption Analysis\n",
    "\n",
    "Analyze Snowflake credit usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cost_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí∞ Snowflake Credit Consumption\\n\")\n",
    "print(\"Understanding Credits:\")\n",
    "print(\"  - 1 credit = 1 hour of X-Small warehouse\")\n",
    "print(\"  - Warehouse size determines credit rate\")\n",
    "print(\"  - Charged per second (minimum 60 seconds)\")\n",
    "print(\"  - Cloud services: up to 10% of compute (free)\\n\")\n",
    "\n",
    "print(\"Credit Rate by Warehouse Size:\")\n",
    "print(\"  X-Small:  1 credit/hour  = $2-4/hour\")\n",
    "print(\"  Small:    2 credits/hour = $4-8/hour\")\n",
    "print(\"  Medium:   4 credits/hour = $8-16/hour\")\n",
    "print(\"  Large:    8 credits/hour = $16-32/hour\\n\")\n",
    "\n",
    "print(\"üí° Cost Optimization:\")\n",
    "print(\"  1. Right-size warehouses (don't over-provision)\")\n",
    "print(\"  2. Set auto-suspend (5-10 minutes idle)\")\n",
    "print(\"  3. Use resource monitors to set spend limits\")\n",
    "print(\"  4. Enable query result caching\")\n",
    "print(\"  5. Use clustering sparingly (reclustering costs credits)\")\n",
    "print(\"  6. Monitor with WAREHOUSE_METERING_HISTORY view\\n\")\n",
    "\n",
    "print(\"üîç Check Credit Usage:\")\n",
    "print(\"  SELECT warehouse_name, SUM(credits_used) as total_credits\")\n",
    "print(\"  FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY\")\n",
    "print(\"  WHERE start_time > DATEADD(day, -7, CURRENT_TIMESTAMP())\")\n",
    "print(\"  GROUP BY warehouse_name\")\n",
    "print(\"  ORDER BY total_credits DESC;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "source": [
    "## 4. Platform-Specific Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auto_scaling_section",
   "metadata": {},
   "source": [
    "### 4.1 Auto-Scaling and Auto-Suspend\n",
    "\n",
    "Configure warehouse auto-scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auto_scaling",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö° Warehouse Auto-Scaling\\n\")\n",
    "print(\"Auto-Suspend:\")\n",
    "print(\"  - Automatically suspend warehouse after idle period\")\n",
    "print(\"  - Recommended: 5-10 minutes for production\")\n",
    "print(\"  - Recommended: 1 minute for development\")\n",
    "print(\"  - SQL: ALTER WAREHOUSE name SET AUTO_SUSPEND = 300;\\n\")\n",
    "\n",
    "print(\"Auto-Resume:\")\n",
    "print(\"  - Automatically resume on query submission\")\n",
    "print(\"  - First query waits for warehouse startup (~5-10 seconds)\")\n",
    "print(\"  - SQL: ALTER WAREHOUSE name SET AUTO_RESUME = TRUE;\\n\")\n",
    "\n",
    "print(\"Multi-Cluster Auto-Scaling:\")\n",
    "print(\"  - Scale out for high concurrency\")\n",
    "print(\"  - Min/Max clusters: 1-10\")\n",
    "print(\"  - Scaling policy: Standard or Economy\")\n",
    "print(\"  - SQL: ALTER WAREHOUSE name SET\")\n",
    "print(\"         MIN_CLUSTER_COUNT = 1\")\n",
    "print(\"         MAX_CLUSTER_COUNT = 3\")\n",
    "print(\"         SCALING_POLICY = 'STANDARD';\\n\")\n",
    "\n",
    "print(\"Scaling Policies:\")\n",
    "print(\"  Standard: Prevent queuing, start cluster immediately\")\n",
    "print(\"  Economy: Minimize credits, allow some queuing\\n\")\n",
    "\n",
    "print(\"üí° Best Practices:\")\n",
    "print(\"  - Use auto-suspend for all warehouses\")\n",
    "print(\"  - Multi-cluster for concurrent users, not speed\")\n",
    "print(\"  - Monitor with WAREHOUSE_LOAD_HISTORY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clustering_keys_section",
   "metadata": {},
   "source": [
    "### 4.2 Clustering Keys and Micro-Partitions\n",
    "\n",
    "Understanding Snowflake's automatic micro-partitioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clustering_keys",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Micro-Partitions and Clustering\\n\")\n",
    "print(\"Micro-Partitions (Automatic):\")\n",
    "print(\"  - Snowflake automatically divides data into 50-500 MB chunks\")\n",
    "print(\"  - Stores metadata (min/max values, null counts)\")\n",
    "print(\"  - Enables partition pruning (skip irrelevant micro-partitions)\")\n",
    "print(\"  - Compressed and encrypted automatically\\n\")\n",
    "\n",
    "print(\"Clustering Keys (Manual):\")\n",
    "print(\"  - Define clustering order for large tables (>1 TB)\")\n",
    "print(\"  - Improves pruning efficiency\")\n",
    "print(\"  - Up to 4 columns in clustering key\")\n",
    "print(\"  - Automatic reclustering (consumes credits)\\n\")\n",
    "\n",
    "print(\"When to Use Clustering:\")\n",
    "print(\"  ‚úÖ Large tables (>1 TB)\")\n",
    "print(\"  ‚úÖ Queries with range filters (WHERE date BETWEEN...)\")\n",
    "print(\"  ‚úÖ Queries on specific columns repeatedly\")\n",
    "print(\"  ‚ùå Small tables (<1 GB) - not worth it\")\n",
    "print(\"  ‚ùå Frequently updated tables - high reclustering cost\\n\")\n",
    "\n",
    "print(\"Example: Add Clustering Key\")\n",
    "print(\"  ALTER TABLE orders CLUSTER BY (o_orderdate);\\n\")\n",
    "\n",
    "print(\"Monitor Clustering:\")\n",
    "print(\"  SELECT system$clustering_information('orders', '(o_orderdate)');\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "time_travel_section",
   "metadata": {},
   "source": [
    "### 4.3 Time Travel and Zero-Copy Cloning\n",
    "\n",
    "Leverage Snowflake's data protection features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "time_travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚è∞ Time Travel\\n\")\n",
    "print(\"What is Time Travel?\")\n",
    "print(\"  - Query historical data within retention period\")\n",
    "print(\"  - Standard: 1 day retention\")\n",
    "print(\"  - Enterprise: Up to 90 days retention\")\n",
    "print(\"  - No additional storage cost (included)\\n\")\n",
    "\n",
    "print(\"Use Cases:\")\n",
    "print(\"  - Undo accidental deletes/updates\")\n",
    "print(\"  - Audit historical changes\")\n",
    "print(\"  - Compare data at different points in time\\n\")\n",
    "\n",
    "print(\"Examples:\")\n",
    "print(\"  -- Query table as of 5 minutes ago\")\n",
    "print(\"  SELECT * FROM orders AT(OFFSET => -60*5);\")\n",
    "print(\"\")\n",
    "print(\"  -- Query before specific timestamp\")\n",
    "print(\"  SELECT * FROM orders BEFORE(TIMESTAMP => '2025-01-01 12:00:00'::timestamp);\")\n",
    "print(\"\")\n",
    "print(\"  -- Restore deleted table\")\n",
    "print(\"  UNDROP TABLE orders;\\n\")\n",
    "\n",
    "print(\"üìã Zero-Copy Cloning\\n\")\n",
    "print(\"What is Zero-Copy Cloning?\")\n",
    "print(\"  - Create instant copy without duplicating data\")\n",
    "print(\"  - No additional storage cost (initially)\")\n",
    "print(\"  - Metadata-only operation (seconds)\")\n",
    "print(\"  - Changes diverge (copy-on-write)\\n\")\n",
    "\n",
    "print(\"Use Cases:\")\n",
    "print(\"  - Create dev/test environments\")\n",
    "print(\"  - Snapshot before major changes\")\n",
    "print(\"  - A/B testing\")\n",
    "print(\"  - Backup before data migration\\n\")\n",
    "\n",
    "print(\"Examples:\")\n",
    "print(\"  -- Clone database\")\n",
    "print(\"  CREATE DATABASE dev_db CLONE prod_db;\")\n",
    "print(\"\")\n",
    "print(\"  -- Clone table\")\n",
    "print(\"  CREATE TABLE orders_backup CLONE orders;\")\n",
    "print(\"\")\n",
    "print(\"  -- Clone at specific time\")\n",
    "print(\"  CREATE TABLE orders_snapshot CLONE orders AT(OFFSET => -60*60*24);  -- 1 day ago\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external_stages_section",
   "metadata": {},
   "source": [
    "### 4.4 External Stages (S3, Azure, GCS)\n",
    "\n",
    "Load data from external cloud storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external_stages",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚òÅÔ∏è  External Stages Overview\\n\")\n",
    "print(\"What are External Stages?\")\n",
    "print(\"  - Named references to external storage locations\")\n",
    "print(\"  - Support: S3, Azure Blob Storage, Google Cloud Storage\")\n",
    "print(\"  - Load data directly from cloud storage\")\n",
    "print(\"  - No intermediate storage required\\n\")\n",
    "\n",
    "print(\"Benefits:\")\n",
    "print(\"  - Load data from existing data lakes\")\n",
    "print(\"  - No duplication of data\")\n",
    "print(\"  - Supports all file formats (CSV, JSON, Parquet, etc.)\")\n",
    "print(\"  - Integration with Snowpipe for continuous loading\\n\")\n",
    "\n",
    "print(\"Example: Create S3 Stage\")\n",
    "print(\"\"\"\n",
    "CREATE STAGE my_s3_stage\n",
    "  URL = 's3://my-bucket/path/'\n",
    "  CREDENTIALS = (AWS_KEY_ID = 'xxx' AWS_SECRET_KEY = 'xxx')\n",
    "  FILE_FORMAT = (TYPE = PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nExample: Load from Stage\")\n",
    "print(\"\"\"\n",
    "COPY INTO orders\n",
    "FROM @my_s3_stage/orders.parquet\n",
    "FILE_FORMAT = (TYPE = PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüí° BenchBox Integration:\")\n",
    "print(\"  platform_cfg = {\")\n",
    "print('      \"stage_location\": \"@my_s3_stage\",')\n",
    "print('      \"file_format\": \"PARQUET\"')\n",
    "print(\"  }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "snowpipe_section",
   "metadata": {},
   "source": [
    "### 4.5 Snowpipe and Streams\n",
    "\n",
    "Continuous data ingestion with Snowpipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "snowpipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Snowpipe (Continuous Data Loading)\\n\")\n",
    "print(\"What is Snowpipe?\")\n",
    "print(\"  - Serverless, continuous data ingestion\")\n",
    "print(\"  - Load data within minutes of availability\")\n",
    "print(\"  - Event-driven (S3 notifications, Azure Event Grid)\")\n",
    "print(\"  - Pay per file processed (separate from warehouse credits)\\n\")\n",
    "\n",
    "print(\"Example: Create Snowpipe\")\n",
    "print(\"\"\"\n",
    "CREATE PIPE my_pipe\n",
    "  AUTO_INGEST = TRUE\n",
    "  AS\n",
    "  COPY INTO orders\n",
    "  FROM @my_s3_stage\n",
    "  FILE_FORMAT = (TYPE = PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüìä Streams (Change Data Capture)\\n\")\n",
    "print(\"What are Streams?\")\n",
    "print(\"  - Track changes to table (INSERT, UPDATE, DELETE)\")\n",
    "print(\"  - Enable CDC patterns\")\n",
    "print(\"  - No additional storage cost\")\n",
    "print(\"  - Consume stream with DML statements\\n\")\n",
    "\n",
    "print(\"Example: Create Stream\")\n",
    "print(\"\"\"\n",
    "CREATE STREAM orders_stream ON TABLE orders;\n",
    "\n",
    "-- Process changes\n",
    "INSERT INTO orders_history\n",
    "SELECT * FROM orders_stream WHERE metadata$action = 'INSERT';\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüí° Use Cases:\")\n",
    "print(\"  - Snowpipe: Real-time data ingestion from cloud storage\")\n",
    "print(\"  - Streams: Incremental processing, CDC, data pipeline triggers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "source": [
    "## 5. Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_results_section",
   "metadata": {},
   "source": [
    "### 5.1 Load and Prepare Results\n",
    "\n",
    "Load benchmark results for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from previous run\n",
    "if \"results\" in locals() and results.query_results:\n",
    "    # Convert to pandas DataFrame for analysis\n",
    "    df_results = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"query\": qr.query_name,\n",
    "                \"time\": qr.execution_time,\n",
    "                \"success\": qr.success,\n",
    "                \"rows_returned\": qr.row_count if hasattr(qr, \"row_count\") else None,\n",
    "            }\n",
    "            for qr in results.query_results\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Results loaded into DataFrame\")\n",
    "    print(f\"\\nShape: {df_results.shape[0]} queries, {df_results.shape[1]} columns\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df_results.head())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results available. Run a benchmark first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats_analysis_section",
   "metadata": {},
   "source": [
    "### 5.2 Statistical Analysis\n",
    "\n",
    "Compute detailed statistics and identify outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"df_results\" in locals():\n",
    "    # Compute statistics\n",
    "    stats = df_results[\"time\"].describe(percentiles=[0.25, 0.5, 0.75, 0.95, 0.99])\n",
    "\n",
    "    print(\"üìä Execution Time Statistics\\n\")\n",
    "    print(stats)\n",
    "\n",
    "    print(\"\\nüîç Key Percentiles:\")\n",
    "    print(f\"  P25 (25th percentile): {df_results['time'].quantile(0.25):.3f}s\")\n",
    "    print(f\"  P50 (median): {df_results['time'].median():.3f}s\")\n",
    "    print(f\"  P75 (75th percentile): {df_results['time'].quantile(0.75):.3f}s\")\n",
    "    print(f\"  P95 (95th percentile): {df_results['time'].quantile(0.95):.3f}s\")\n",
    "    print(f\"  P99 (99th percentile): {df_results['time'].quantile(0.99):.3f}s\")\n",
    "\n",
    "    # Identify outliers (>2 standard deviations)\n",
    "    mean_time = df_results[\"time\"].mean()\n",
    "    std_time = df_results[\"time\"].std()\n",
    "    outliers = df_results[df_results[\"time\"] > mean_time + 2 * std_time]\n",
    "\n",
    "    if not outliers.empty:\n",
    "        print(\"\\n‚ö†Ô∏è  Performance Outliers (>2œÉ):\")\n",
    "        for _, row in outliers.iterrows():\n",
    "            z_score = (row[\"time\"] - mean_time) / std_time\n",
    "            print(f\"  {row['query']}: {row['time']:.2f}s (z-score: {z_score:.2f})\")\n",
    "\n",
    "        print(\"\\nüí° Investigation steps:\")\n",
    "        print(\"  1. Check QUERY_PROFILE in Snowsight\")\n",
    "        print(\"  2. Review query execution plan\")\n",
    "        print(\"  3. Check for data skew\")\n",
    "        print(\"  4. Consider adding clustering keys\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No significant outliers detected\")\n",
    "\n",
    "    # Coefficient of variation (CV)\n",
    "    cv = (std_time / mean_time) * 100\n",
    "    print(\"\\nüìà Variability:\")\n",
    "    print(f\"  Standard deviation: {std_time:.3f}s\")\n",
    "    print(f\"  Coefficient of variation: {cv:.1f}%\")\n",
    "    if cv < 20:\n",
    "        print(\"  Assessment: Low variability (consistent performance)\")\n",
    "    elif cv < 50:\n",
    "        print(\"  Assessment: Moderate variability (typical for mixed workload)\")\n",
    "    else:\n",
    "        print(\"  Assessment: High variability (investigate slow queries)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Load results first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive_viz_section",
   "metadata": {},
   "source": [
    "### 5.3 Comprehensive Visualizations\n",
    "\n",
    "Multi-panel performance visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"df_results\" in locals():\n",
    "    # Create 2x2 subplot grid\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(\"Snowflake Performance Analysis\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    # 1. Distribution histogram\n",
    "    axes[0, 0].hist(df_results[\"time\"], bins=20, color=\"#29B5E8\", alpha=0.7, edgecolor=\"black\")\n",
    "    axes[0, 0].axvline(df_results[\"time\"].mean(), color=\"red\", linestyle=\"--\", linewidth=2, label=\"Mean\")\n",
    "    axes[0, 0].axvline(df_results[\"time\"].median(), color=\"green\", linestyle=\"--\", linewidth=2, label=\"Median\")\n",
    "    axes[0, 0].set_xlabel(\"Execution Time (seconds)\", fontweight=\"bold\")\n",
    "    axes[0, 0].set_ylabel(\"Frequency\", fontweight=\"bold\")\n",
    "    axes[0, 0].set_title(\"Execution Time Distribution\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # 2. Box plot\n",
    "    bp = axes[0, 1].boxplot(df_results[\"time\"], patch_artist=True, vert=True)\n",
    "    bp[\"boxes\"][0].set_facecolor(\"#29B5E8\")\n",
    "    bp[\"boxes\"][0].set_alpha(0.7)\n",
    "    axes[0, 1].set_ylabel(\"Execution Time (seconds)\", fontweight=\"bold\")\n",
    "    axes[0, 1].set_title(\"Box Plot (Outlier Detection)\")\n",
    "    axes[0, 1].set_xticklabels([\"All Queries\"])\n",
    "    axes[0, 1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # 3. Sorted horizontal bar chart (top 15)\n",
    "    df_sorted = df_results.sort_values(\"time\", ascending=True).tail(15)\n",
    "    colors = [\"#F26B1D\" if t > df_results[\"time\"].quantile(0.9) else \"#29B5E8\" for t in df_sorted[\"time\"]]\n",
    "    axes[1, 0].barh(df_sorted[\"query\"], df_sorted[\"time\"], color=colors, alpha=0.8)\n",
    "    axes[1, 0].set_xlabel(\"Execution Time (seconds)\", fontweight=\"bold\")\n",
    "    axes[1, 0].set_title(\"Slowest 15 Queries\")\n",
    "    axes[1, 0].grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "    # 4. Cumulative performance (Pareto analysis)\n",
    "    df_sorted_desc = df_results.sort_values(\"time\", ascending=False)\n",
    "    df_sorted_desc[\"cumulative_pct\"] = df_sorted_desc[\"time\"].cumsum() / df_sorted_desc[\"time\"].sum() * 100\n",
    "    axes[1, 1].plot(\n",
    "        range(len(df_sorted_desc)), df_sorted_desc[\"cumulative_pct\"], marker=\"o\", color=\"#29B5E8\", linewidth=2\n",
    "    )\n",
    "    axes[1, 1].axhline(80, color=\"red\", linestyle=\"--\", linewidth=2, label=\"80% threshold\")\n",
    "    axes[1, 1].set_xlabel(\"Number of Queries (sorted by time)\", fontweight=\"bold\")\n",
    "    axes[1, 1].set_ylabel(\"Cumulative % of Total Time\", fontweight=\"bold\")\n",
    "    axes[1, 1].set_title(\"Pareto Analysis (80/20 Rule)\")\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Pareto insight\n",
    "    queries_for_80pct = len(df_sorted_desc[df_sorted_desc[\"cumulative_pct\"] <= 80])\n",
    "    print(\"\\nüìä Pareto Insight:\")\n",
    "    print(\n",
    "        f\"  {queries_for_80pct} queries ({queries_for_80pct / len(df_results) * 100:.1f}%) account for 80% of total time\"\n",
    "    )\n",
    "    print(f\"  üí° Focus optimization efforts on these {queries_for_80pct} queries\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Load results first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query_history_section",
   "metadata": {},
   "source": [
    "### 5.4 Query History Analysis\n",
    "\n",
    "Deep dive into query performance using QUERY_HISTORY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_history",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=SNOWFLAKE_USER, password=SNOWFLAKE_PASSWORD, account=SNOWFLAKE_ACCOUNT, warehouse=SNOWFLAKE_WAREHOUSE\n",
    "    )\n",
    "\n",
    "    # Query detailed performance metrics\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        query_id,\n",
    "        query_text,\n",
    "        warehouse_name,\n",
    "        warehouse_size,\n",
    "        execution_time / 1000.0 as execution_seconds,\n",
    "        queued_provisioning_time / 1000.0 as queue_seconds,\n",
    "        compilation_time / 1000.0 as compile_seconds,\n",
    "        bytes_scanned,\n",
    "        bytes_written,\n",
    "        rows_produced,\n",
    "        partitions_scanned,\n",
    "        partitions_total\n",
    "    FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY\n",
    "    WHERE start_time > DATEADD(hour, -1, CURRENT_TIMESTAMP())\n",
    "        AND warehouse_name = '{SNOWFLAKE_WAREHOUSE}'\n",
    "        AND query_type = 'SELECT'\n",
    "    ORDER BY execution_time DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "\n",
    "    df_detailed = pd.read_sql(query, conn)\n",
    "\n",
    "    if not df_detailed.empty:\n",
    "        print(\"üìä Top 10 Slowest Queries:\\n\")\n",
    "        for idx, row in df_detailed.iterrows():\n",
    "            print(f\"{idx + 1}. Query ID: {row['QUERY_ID'][:16]}...\")\n",
    "            print(f\"   Execution: {row['EXECUTION_SECONDS']:.2f}s\")\n",
    "            print(f\"   Queue time: {row['QUEUE_SECONDS']:.2f}s\")\n",
    "            print(f\"   Compile time: {row['COMPILE_SECONDS']:.2f}s\")\n",
    "            print(f\"   Bytes scanned: {row['BYTES_SCANNED'] / (1024**3):.2f} GB\")\n",
    "            print(f\"   Rows produced: {row['ROWS_PRODUCED']:,}\")\n",
    "\n",
    "            if row[\"PARTITIONS_TOTAL\"] > 0:\n",
    "                prune_pct = (1 - row[\"PARTITIONS_SCANNED\"] / row[\"PARTITIONS_TOTAL\"]) * 100\n",
    "                print(f\"   Pruning efficiency: {prune_pct:.1f}%\")\n",
    "            print()\n",
    "\n",
    "        print(\"üí° Optimization Tips:\")\n",
    "        print(\"  - High queue time: Increase warehouse size or use multi-cluster\")\n",
    "        print(\"  - High compile time: Use query result caching\")\n",
    "        print(\"  - Low pruning: Add clustering keys on filter columns\")\n",
    "        print(\"  - High bytes scanned: Use SELECT columns instead of SELECT *\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No query history available\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not analyze query history: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "credit_consumption_section",
   "metadata": {},
   "source": [
    "### 5.5 Credit Consumption Analysis\n",
    "\n",
    "Analyze warehouse credit usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "credit_consumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=SNOWFLAKE_USER, password=SNOWFLAKE_PASSWORD, account=SNOWFLAKE_ACCOUNT, warehouse=SNOWFLAKE_WAREHOUSE\n",
    "    )\n",
    "\n",
    "    # Query warehouse metering\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        warehouse_name,\n",
    "        DATE_TRUNC('hour', start_time) as hour,\n",
    "        SUM(credits_used) as credits_used\n",
    "    FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY\n",
    "    WHERE start_time > DATEADD(day, -1, CURRENT_TIMESTAMP())\n",
    "        AND warehouse_name = '{SNOWFLAKE_WAREHOUSE}'\n",
    "    GROUP BY warehouse_name, hour\n",
    "    ORDER BY hour DESC\n",
    "    \"\"\"\n",
    "\n",
    "    df_credits = pd.read_sql(query, conn)\n",
    "\n",
    "    if not df_credits.empty:\n",
    "        total_credits = df_credits[\"CREDITS_USED\"].sum()\n",
    "\n",
    "        print(\"üí∞ Credit Consumption (Last 24 Hours)\\n\")\n",
    "        print(f\"Total credits used: {total_credits:.4f}\")\n",
    "        print(f\"Estimated cost: ${total_credits * 3:.2f} (assuming $3/credit)\")\n",
    "        print(\"\\nHourly breakdown:\")\n",
    "        for _, row in df_credits.head(10).iterrows():\n",
    "            print(f\"  {row['HOUR']}: {row['CREDITS_USED']:.4f} credits\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No credit usage data available\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not analyze credit consumption: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regression_section",
   "metadata": {},
   "source": [
    "### 5.6 Regression Detection\n",
    "\n",
    "Compare against baseline to detect performance regressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regression_detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"df_results\" in locals():\n",
    "    # Compare against baseline (you can load from a saved baseline file)\n",
    "    # For demonstration, we'll use a mock baseline\n",
    "    baseline_avg = 1.8  # seconds (mock baseline)\n",
    "    current_avg = df_results[\"time\"].mean()\n",
    "\n",
    "    # Calculate change\n",
    "    change_pct = ((current_avg - baseline_avg) / baseline_avg) * 100\n",
    "\n",
    "    print(\"üîç Performance Regression Analysis\\n\")\n",
    "    print(f\"Baseline average: {baseline_avg:.2f}s\")\n",
    "    print(f\"Current average: {current_avg:.2f}s\")\n",
    "    print(f\"Change: {change_pct:+.1f}%\\n\")\n",
    "\n",
    "    # Threshold: 10% change\n",
    "    if abs(change_pct) > 10:\n",
    "        if change_pct > 0:\n",
    "            status = \"‚ùå REGRESSION DETECTED\"\n",
    "            print(status)\n",
    "            print(f\"Performance degraded by {change_pct:.1f}%\\n\")\n",
    "            print(\"üí° Investigation Steps:\")\n",
    "            print(\"  1. Check warehouse size and availability\")\n",
    "            print(\"  2. Review clustering health (CLUSTERING_INFORMATION)\")\n",
    "            print(\"  3. Check for data growth\")\n",
    "            print(\"  4. Verify result cache hit rate\")\n",
    "            print(\"  5. Review query execution plans in Snowsight\")\n",
    "        else:\n",
    "            status = \"‚úÖ PERFORMANCE IMPROVEMENT\"\n",
    "            print(status)\n",
    "            print(f\"Performance improved by {abs(change_pct):.1f}%\\n\")\n",
    "            print(\"üí° Possible Reasons:\")\n",
    "            print(\"  - Clustering optimization\")\n",
    "            print(\"  - Result caching\")\n",
    "            print(\"  - Warehouse size increase\")\n",
    "            print(\"  - Query optimization\")\n",
    "    else:\n",
    "        print(\"‚úÖ Performance stable (within 10% threshold)\\n\")\n",
    "\n",
    "    print(\"\\nüí° Save current run as new baseline:\")\n",
    "    print(\"   df_results.to_csv('baseline_snowflake_tpch.csv', index=False)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Load results first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "source": [
    "## 6. Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connection_diagnostics_section",
   "metadata": {},
   "source": [
    "### 6.1 Connection Diagnostics\n",
    "\n",
    "Comprehensive connection troubleshooting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connection_diagnostics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_snowflake_connection():\n",
    "    \"\"\"Diagnose Snowflake connection issues\"\"\"\n",
    "    print(\"üîç Snowflake Connection Diagnostic\\n\")\n",
    "\n",
    "    # Check 1: Environment variables\n",
    "    print(\"1Ô∏è‚É£ Checking environment variables...\")\n",
    "    if SNOWFLAKE_ACCOUNT:\n",
    "        print(f\"   ‚úÖ SNOWFLAKE_ACCOUNT = {SNOWFLAKE_ACCOUNT}\")\n",
    "    else:\n",
    "        print(\"   ‚ùå SNOWFLAKE_ACCOUNT not set\")\n",
    "\n",
    "    if SNOWFLAKE_USER:\n",
    "        print(f\"   ‚úÖ SNOWFLAKE_USER = {SNOWFLAKE_USER}\")\n",
    "    else:\n",
    "        print(\"   ‚ùå SNOWFLAKE_USER not set\")\n",
    "\n",
    "    if SNOWFLAKE_PASSWORD:\n",
    "        print(\"   ‚úÖ SNOWFLAKE_PASSWORD is set\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  SNOWFLAKE_PASSWORD not set\")\n",
    "\n",
    "    # Check 2: Account format\n",
    "    print(\"\\n2Ô∏è‚É£ Validating account identifier...\")\n",
    "    if SNOWFLAKE_ACCOUNT and \".\" in SNOWFLAKE_ACCOUNT:\n",
    "        print(f\"   ‚úÖ Account includes region/cloud: {SNOWFLAKE_ACCOUNT}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Account may need full identifier (e.g., xy12345.us-east-1.aws)\")\n",
    "\n",
    "    # Check 3: Connection test\n",
    "    print(\"\\n3Ô∏è‚É£ Testing connection...\")\n",
    "    try:\n",
    "        test_conn = snowflake.connector.connect(\n",
    "            user=SNOWFLAKE_USER, password=SNOWFLAKE_PASSWORD, account=SNOWFLAKE_ACCOUNT\n",
    "        )\n",
    "        print(\"   ‚úÖ Connection successful\")\n",
    "        test_conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Connection failed: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìö Troubleshooting Guide:\\n\")\n",
    "    print(\"If connection fails:\")\n",
    "    print(\"  1. Verify account identifier (xy12345.region.cloud)\")\n",
    "    print(\"  2. Check username and password\")\n",
    "    print(\"  3. Verify network policies (IP allowlists)\")\n",
    "    print(\"  4. Check user is not locked or expired\\n\")\n",
    "\n",
    "    print(\"If warehouse issues:\")\n",
    "    print(\"  1. Verify warehouse exists: SHOW WAREHOUSES;\")\n",
    "    print(\"  2. Check warehouse state (suspended/running)\")\n",
    "    print(\"  3. Verify user has USAGE privilege on warehouse\")\n",
    "\n",
    "\n",
    "# Run diagnostics\n",
    "diagnose_snowflake_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permission_validation_section",
   "metadata": {},
   "source": [
    "### 6.2 Permission Validation\n",
    "\n",
    "Verify required permissions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permission_validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_snowflake_permissions():\n",
    "    \"\"\"Validate Snowflake permissions\"\"\"\n",
    "    print(\"üîí Snowflake Permission Validation\\n\")\n",
    "\n",
    "    try:\n",
    "        conn = snowflake.connector.connect(user=SNOWFLAKE_USER, password=SNOWFLAKE_PASSWORD, account=SNOWFLAKE_ACCOUNT)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Test 1: Current role\n",
    "        print(\"1Ô∏è‚É£ Checking current role...\")\n",
    "        cursor.execute(\"SELECT CURRENT_ROLE()\")\n",
    "        role = cursor.fetchone()[0]\n",
    "        print(f\"   ‚úÖ Current role: {role}\")\n",
    "\n",
    "        # Test 2: Warehouse access\n",
    "        print(\"\\n2Ô∏è‚É£ Testing warehouse access...\")\n",
    "        try:\n",
    "            cursor.execute(f\"USE WAREHOUSE {SNOWFLAKE_WAREHOUSE}\")\n",
    "            print(f\"   ‚úÖ Can use warehouse: {SNOWFLAKE_WAREHOUSE}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Cannot use warehouse: {e}\")\n",
    "\n",
    "        # Test 3: Database access\n",
    "        print(\"\\n3Ô∏è‚É£ Testing database access...\")\n",
    "        try:\n",
    "            cursor.execute(f\"USE DATABASE {SNOWFLAKE_DATABASE}\")\n",
    "            print(f\"   ‚úÖ Can use database: {SNOWFLAKE_DATABASE}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Cannot use database: {e}\")\n",
    "\n",
    "        # Test 4: Create table\n",
    "        print(\"\\n4Ô∏è‚É£ Testing table creation...\")\n",
    "        try:\n",
    "            cursor.execute(\"CREATE TEMPORARY TABLE test_table (id INT)\")\n",
    "            cursor.execute(\"DROP TABLE test_table\")\n",
    "            print(\"   ‚úÖ Can create and drop tables\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Cannot create tables: {e}\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üìã Required Privileges:\\n\")\n",
    "        print(\"Minimum (for benchmarking):\")\n",
    "        print(\"  - USAGE on warehouse\")\n",
    "        print(\"  - USAGE on database and schema\")\n",
    "        print(\"  - CREATE TABLE in schema\")\n",
    "        print(\"  - SELECT, INSERT on tables\\n\")\n",
    "        print(\"Recommended (for full features):\")\n",
    "        print(\"  - SYSADMIN role\")\n",
    "        print(\"  - CREATE WAREHOUSE privilege\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Permission validation failed: {e}\")\n",
    "\n",
    "\n",
    "# Run validation\n",
    "try:\n",
    "    validate_snowflake_permissions()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warehouse_diagnostics_section",
   "metadata": {},
   "source": [
    "### 6.3 Warehouse Diagnostics\n",
    "\n",
    "Check warehouse status and configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warehouse_diagnostics",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = snowflake.connector.connect(user=SNOWFLAKE_USER, password=SNOWFLAKE_PASSWORD, account=SNOWFLAKE_ACCOUNT)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    print(\"üîß Warehouse Diagnostics\\n\")\n",
    "\n",
    "    # Get warehouse details\n",
    "    cursor.execute(f\"SHOW WAREHOUSES LIKE '{SNOWFLAKE_WAREHOUSE}'\")\n",
    "    wh_info = cursor.fetchone()\n",
    "\n",
    "    if wh_info:\n",
    "        print(f\"Warehouse: {wh_info[0]}\")\n",
    "        print(f\"State: {wh_info[1]}\")\n",
    "        print(f\"Type: {wh_info[2]}\")\n",
    "        print(f\"Size: {wh_info[3]}\")\n",
    "        print(f\"Min Clusters: {wh_info[4]}\")\n",
    "        print(f\"Max Clusters: {wh_info[5]}\")\n",
    "        print(f\"Auto Suspend: {wh_info[9]} seconds\")\n",
    "        print(f\"Auto Resume: {wh_info[10]}\")\n",
    "\n",
    "        # Recommendations\n",
    "        print(\"\\nüí° Recommendations:\")\n",
    "        if wh_info[1] == \"SUSPENDED\":\n",
    "            print(\"  ‚ö†Ô∏è  Warehouse is suspended. It will resume on first query.\")\n",
    "        if wh_info[9] is None or wh_info[9] > 600:\n",
    "            print(\"  ‚ö†Ô∏è  Consider setting auto-suspend to 5-10 minutes\")\n",
    "        if wh_info[10] != \"true\":\n",
    "            print(\"  ‚ö†Ô∏è  Enable auto-resume for automatic startup\")\n",
    "    else:\n",
    "        print(f\"‚ùå Warehouse '{SNOWFLAKE_WAREHOUSE}' not found\")\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Warehouse diagnostic failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common_issues_section",
   "metadata": {},
   "source": [
    "### 6.4 Common Issues and Solutions\n",
    "\n",
    "Quick reference for common Snowflake benchmarking issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common_issues",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Common Snowflake Benchmarking Issues\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚ùå Issue: 'Account not found' or connection timeout\")\n",
    "print(\"‚úÖ Solution:\")\n",
    "print(\"   1. Use full account identifier: xy12345.us-east-1.aws\")\n",
    "print(\"   2. Check network policies (IP allowlists)\")\n",
    "print(\"   3. Verify account is not suspended\")\n",
    "print(\"   4. Test from Snowsight web UI first\\n\")\n",
    "\n",
    "print(\"‚ùå Issue: 'Warehouse not started' errors\")\n",
    "print(\"‚úÖ Solution:\")\n",
    "print(\"   1. Enable auto-resume: ALTER WAREHOUSE name SET AUTO_RESUME = TRUE;\")\n",
    "print(\"   2. Manually start: ALTER WAREHOUSE name RESUME;\")\n",
    "print(\"   3. Check warehouse privileges (USAGE)\\n\")\n",
    "\n",
    "print(\"‚ùå Issue: Slow query performance\")\n",
    "print(\"‚úÖ Solution:\")\n",
    "print(\"   1. Review query profile in Snowsight\")\n",
    "print(\"   2. Check clustering keys on large tables\")\n",
    "print(\"   3. Increase warehouse size (XS ‚Üí S ‚Üí M)\")\n",
    "print(\"   4. Enable result caching (automatic)\")\n",
    "print(\"   5. Use EXPLAIN to analyze execution plan\\n\")\n",
    "\n",
    "print(\"‚ùå Issue: High credit consumption\")\n",
    "print(\"‚úÖ Solution:\")\n",
    "print(\"   1. Set aggressive auto-suspend (5-10 minutes)\")\n",
    "print(\"   2. Right-size warehouses (don't over-provision)\")\n",
    "print(\"   3. Use separate warehouses for ETL vs analytics\")\n",
    "print(\"   4. Monitor with WAREHOUSE_METERING_HISTORY\")\n",
    "print(\"   5. Set resource monitors to limit spend\\n\")\n",
    "\n",
    "print(\"‚ùå Issue: Query queuing\")\n",
    "print(\"‚úÖ Solution:\")\n",
    "print(\"   1. Increase warehouse size (more compute)\")\n",
    "print(\"   2. Enable multi-cluster (scale out for concurrency)\")\n",
    "print(\"   3. Use dedicated warehouses per workload\")\n",
    "print(\"   4. Optimize queries to reduce runtime\\n\")\n",
    "\n",
    "print(\"‚ùå Issue: Data loading failures\")\n",
    "print(\"‚úÖ Solution:\")\n",
    "print(\"   1. Verify stage configuration (URL, credentials)\")\n",
    "print(\"   2. Check file format matches data\")\n",
    "print(\"   3. Use VALIDATION_MODE to test before loading\")\n",
    "print(\"   4. Enable ON_ERROR = 'CONTINUE' to skip bad rows\")\n",
    "print(\"   5. Review COPY_HISTORY for error details\\n\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüí° More Help:\")\n",
    "print(\"  - Snowflake docs: https://docs.snowflake.com\")\n",
    "print(\"  - Community: https://community.snowflake.com\")\n",
    "print(\"  - BenchBox docs: https://github.com/joeharris76/benchbox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps_section",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**Continue Learning:**\n",
    "- Explore other cloud platforms: BigQuery, Databricks, Redshift\n",
    "- Try different benchmarks: TPC-DS, ClickBench, SSB\n",
    "- Compare platforms with multi-platform notebooks\n",
    "- Set up CI/CD regression testing\n",
    "\n",
    "**Platform-Specific Features to Explore:**\n",
    "- Multi-cluster warehouses (scale out for concurrency)\n",
    "- Snowpipe (continuous data loading)\n",
    "- Streams and Tasks (CDC and ETL pipelines)\n",
    "- Data Sharing (share data across accounts)\n",
    "- Search Optimization Service\n",
    "\n",
    "**Resources:**\n",
    "- [BenchBox Documentation](https://github.com/joeharris76/benchbox)\n",
    "- [Snowflake Documentation](https://docs.snowflake.com)\n",
    "- [Snowflake Best Practices](https://docs.snowflake.com/en/user-guide/performance)\n",
    "- [Snowflake Pricing](https://www.snowflake.com/pricing/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
