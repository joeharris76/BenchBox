{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Amazon Redshift Benchmarking with BenchBox\n",
    "\n",
    "This notebook demonstrates comprehensive benchmarking of Amazon Redshift data warehouses using BenchBox.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Running TPC-H and TPC-DS benchmarks on Redshift clusters\n",
    "- Optimizing tables with distribution keys and sort keys\n",
    "- Loading data efficiently using COPY from S3\n",
    "- Monitoring cluster performance with STL/SVL system tables\n",
    "- Configuring Workload Management (WLM) for query prioritization\n",
    "- Analyzing costs and performance trade-offs\n",
    "\n",
    "**Prerequisites:**\n",
    "- Active Amazon Redshift cluster\n",
    "- AWS credentials with Redshift and S3 access\n",
    "- Database user with CREATE TABLE and COPY privileges\n",
    "- Security groups configured to allow connections\n",
    "\n",
    "**Estimated time:** 15-30 minutes (scale factor 0.01-1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be53684",
   "metadata": {},
   "source": [
    "### Install Required Packages\n",
    "\n",
    "Install BenchBox and the Redshift connector library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q benchbox redshift_connector boto3 pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-cell",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "Import BenchBox components and visualization libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# BenchBox imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualization imports\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from benchbox.core.results.comparison import BenchmarkComparator\n",
    "\n",
    "from benchbox.core.config import BenchmarkConfig, DatabaseConfig\n",
    "from benchbox.core.results.exporter import ResultExporter\n",
    "from benchbox.core.results.loader import ResultLoader\n",
    "from benchbox.core.runner import LifecyclePhases, run_benchmark_lifecycle\n",
    "\n",
    "# Redshift connector\n",
    "try:\n",
    "    import redshift_connector\n",
    "\n",
    "    print(\"‚úÖ redshift_connector imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing redshift_connector: {e}\")\n",
    "    print(\"   Install with: pip install redshift_connector\")\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"üì¶ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auth-cell",
   "metadata": {},
   "source": [
    "### Configure Authentication\n",
    "\n",
    "Set up credentials for your Redshift cluster. Three methods are supported:\n",
    "\n",
    "**Method 1: Environment Variables (Recommended)**\n",
    "```bash\n",
    "export REDSHIFT_HOST=\"mycluster.abc123.us-east-1.redshift.amazonaws.com\"\n",
    "export REDSHIFT_PORT=\"5439\"\n",
    "export REDSHIFT_DB=\"dev\"\n",
    "export REDSHIFT_USER=\"awsuser\"\n",
    "export REDSHIFT_PASSWORD=\"your-password\"\n",
    "export AWS_REGION=\"us-east-1\"\n",
    "export AWS_S3_BUCKET=\"my-benchmark-bucket\"  # Optional, for COPY operations\n",
    "export AWS_IAM_ROLE=\"arn:aws:iam::123456789012:role/RedshiftCopyRole\"  # Optional\n",
    "```\n",
    "\n",
    "**Method 2: IAM Authentication (Federated)**\n",
    "```python\n",
    "# Use get_cluster_credentials API for temporary credentials\n",
    "conn = redshift_connector.connect(\n",
    "    host='mycluster.abc123.us-east-1.redshift.amazonaws.com',\n",
    "    database='dev',\n",
    "    cluster_identifier='mycluster',\n",
    "    iam=True,\n",
    "    db_user='iamuser'\n",
    ")\n",
    "```\n",
    "\n",
    "**Method 3: Secrets Manager**\n",
    "```python\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "client = boto3.client('secretsmanager', region_name='us-east-1')\n",
    "secret = json.loads(client.get_secret_value(SecretId='redshift-credentials')['SecretString'])\n",
    "REDSHIFT_USER = secret['username']\n",
    "REDSHIFT_PASSWORD = secret['password']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auth-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try environment variables first\n",
    "try:\n",
    "    REDSHIFT_HOST = os.environ.get(\"REDSHIFT_HOST\")\n",
    "    REDSHIFT_PORT = int(os.environ.get(\"REDSHIFT_PORT\", \"5439\"))\n",
    "    REDSHIFT_DB = os.environ.get(\"REDSHIFT_DB\")\n",
    "    REDSHIFT_USER = os.environ.get(\"REDSHIFT_USER\")\n",
    "    REDSHIFT_PASSWORD = os.environ.get(\"REDSHIFT_PASSWORD\")\n",
    "    AWS_REGION = os.environ.get(\"AWS_REGION\", \"us-east-1\")\n",
    "    AWS_S3_BUCKET = os.environ.get(\"AWS_S3_BUCKET\", None)\n",
    "    AWS_IAM_ROLE = os.environ.get(\"AWS_IAM_ROLE\", None)\n",
    "\n",
    "    # Validate required variables\n",
    "    if not all([REDSHIFT_HOST, REDSHIFT_DB, REDSHIFT_USER, REDSHIFT_PASSWORD]):\n",
    "        print(\"‚ö†Ô∏è  Missing required environment variables. Please set:\")\n",
    "        print(\"   - REDSHIFT_HOST: Cluster endpoint\")\n",
    "        print(\"   - REDSHIFT_DB: Database name\")\n",
    "        print(\"   - REDSHIFT_USER: Database user\")\n",
    "        print(\"   - REDSHIFT_PASSWORD: User password\")\n",
    "        raise ValueError(\"Missing required Redshift configuration\")\n",
    "\n",
    "    print(\"‚úÖ Redshift Configuration:\")\n",
    "    print(f\"   Host: {REDSHIFT_HOST}\")\n",
    "    print(f\"   Port: {REDSHIFT_PORT}\")\n",
    "    print(f\"   Database: {REDSHIFT_DB}\")\n",
    "    print(f\"   User: {REDSHIFT_USER}\")\n",
    "    print(f\"   Region: {AWS_REGION}\")\n",
    "    if AWS_S3_BUCKET:\n",
    "        print(f\"   S3 Bucket: {AWS_S3_BUCKET}\")\n",
    "    if AWS_IAM_ROLE:\n",
    "        print(f\"   IAM Role: {AWS_IAM_ROLE[:50]}...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Configuration error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-conn-cell",
   "metadata": {},
   "source": [
    "### Test Connection\n",
    "\n",
    "Verify connectivity to your Redshift cluster and check cluster configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-conn-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Connect to Redshift\n",
    "    conn = redshift_connector.connect(\n",
    "        host=REDSHIFT_HOST, port=REDSHIFT_PORT, database=REDSHIFT_DB, user=REDSHIFT_USER, password=REDSHIFT_PASSWORD\n",
    "    )\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Check cluster version\n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    version = cursor.fetchone()[0]\n",
    "    print(\"‚úÖ Connected to Redshift\")\n",
    "    print(f\"   Version: {version[:80]}...\")\n",
    "\n",
    "    # Get cluster configuration\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT node_type, node_count\n",
    "        FROM stv_slices\n",
    "        WHERE slice = 0\n",
    "        LIMIT 1;\n",
    "    \"\"\")\n",
    "    result = cursor.fetchone()\n",
    "    if result:\n",
    "        node_type, node_count = result\n",
    "        print(\"\\nüìä Cluster Configuration:\")\n",
    "        print(f\"   Node Type: {node_type if node_type else 'Unknown'}\")\n",
    "        print(f\"   Node Count: {node_count if node_count else 'Unknown'}\")\n",
    "\n",
    "    # Check WLM configuration\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*) as queue_count\n",
    "        FROM stv_wlm_service_class_config\n",
    "        WHERE service_class >= 6;\n",
    "    \"\"\")\n",
    "    queue_count = cursor.fetchone()[0]\n",
    "    print(f\"   WLM Queues: {queue_count}\")\n",
    "\n",
    "    # Check disk space\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            SUM(capacity)/1024 as total_gb,\n",
    "            SUM(used)/1024 as used_gb,\n",
    "            ROUND(SUM(used)::float/SUM(capacity)::float*100, 1) as pct_used\n",
    "        FROM stv_partitions;\n",
    "    \"\"\")\n",
    "    total_gb, used_gb, pct_used = cursor.fetchone()\n",
    "    print(\"\\nüíæ Disk Usage:\")\n",
    "    print(f\"   Total: {total_gb:.1f} GB\")\n",
    "    print(f\"   Used: {used_gb:.1f} GB ({pct_used}%)\")\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    print(\"\\n‚úÖ Connection test successful!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    print(\"\\nüîç Troubleshooting steps:\")\n",
    "    print(\"   1. Verify security group allows inbound connections on port 5439\")\n",
    "    print(\"   2. Check VPC routing and network ACLs\")\n",
    "    print(\"   3. Ensure cluster is publicly accessible (if connecting from outside VPC)\")\n",
    "    print(\"   4. Verify database user credentials\")\n",
    "    print(\"   5. Check cluster is in 'available' state\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-cell",
   "metadata": {},
   "source": [
    "### Redshift Node Type Guide\n",
    "\n",
    "**RA3 Nodes (Recommended for most workloads)**\n",
    "- **RA3.XLPLUS**: 4 vCPUs, 32 GB RAM, 32 TB managed storage\n",
    "  - Best for: General analytics, growing datasets\n",
    "  - Cost: ~$1.086/hour per node\n",
    "- **RA3.4XLARGE**: 12 vCPUs, 96 GB RAM, 128 TB managed storage\n",
    "  - Best for: Large workloads, complex queries\n",
    "  - Cost: ~$3.26/hour per node\n",
    "- **RA3.16XLARGE**: 48 vCPUs, 384 GB RAM, 128 TB managed storage\n",
    "  - Best for: Massive datasets, highest performance\n",
    "  - Cost: ~$13.04/hour per node\n",
    "\n",
    "**DC2 Nodes (Compute-optimized, SSD)**\n",
    "- **DC2.LARGE**: 2 vCPUs, 15 GB RAM, 160 GB SSD\n",
    "  - Best for: Small datasets (<1 TB), development\n",
    "  - Cost: ~$0.25/hour per node\n",
    "- **DC2.8XLARGE**: 32 vCPUs, 244 GB RAM, 2.56 TB SSD\n",
    "  - Best for: High-performance, fixed storage needs\n",
    "  - Cost: ~$4.80/hour per node\n",
    "\n",
    "**Serverless**\n",
    "- Pay-per-use with Redshift Processing Units (RPUs)\n",
    "- No cluster management required\n",
    "- Best for: Variable workloads, getting started\n",
    "- Cost: ~$0.36/RPU-hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure benchmark settings\n",
    "config = {\n",
    "    \"project\": \"benchbox-redshift\",\n",
    "    \"cluster_endpoint\": REDSHIFT_HOST,\n",
    "    \"database\": REDSHIFT_DB,\n",
    "    \"user\": REDSHIFT_USER,\n",
    "    \"region\": AWS_REGION,\n",
    "    \"s3_bucket\": AWS_S3_BUCKET,\n",
    "    \"iam_role\": AWS_IAM_ROLE,\n",
    "    # Scale factors to test\n",
    "    \"scale_factors\": [0.01, 0.1, 1.0],  # 10MB, 100MB, 1GB\n",
    "    # Output directory\n",
    "    \"output_dir\": \"./benchmark_results\",\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "Path(config[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuration complete\")\n",
    "print(f\"   Output directory: {config['output_dir']}\")\n",
    "print(f\"   Scale factors: {config['scale_factors']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## 2. Quick Start Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quickstart-desc",
   "metadata": {},
   "source": [
    "### Run TPC-H Power Test\n",
    "\n",
    "Execute a TPC-H power test at scale factor 0.01 (10MB). This runs all 22 TPC-H queries sequentially.\n",
    "\n",
    "**What happens:**\n",
    "1. Generate TPC-H data (customer, orders, lineitem, etc.)\n",
    "2. Create tables in Redshift with default distribution\n",
    "3. Load data using COPY command\n",
    "4. Execute 22 queries and measure performance\n",
    "\n",
    "**Expected time:** ~3-5 minutes at SF 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quickstart-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure database connection\n",
    "db_cfg = DatabaseConfig(type=\"redshift\", name=\"redshift-tpch\")\n",
    "platform_cfg = {\n",
    "    \"host\": REDSHIFT_HOST,\n",
    "    \"port\": REDSHIFT_PORT,\n",
    "    \"database\": REDSHIFT_DB,\n",
    "    \"user\": REDSHIFT_USER,\n",
    "    \"password\": REDSHIFT_PASSWORD,\n",
    "}\n",
    "\n",
    "# Configure TPC-H benchmark\n",
    "bench_cfg = BenchmarkConfig(\n",
    "    name=\"tpch\", display_name=\"TPC-H Power Test\", scale_factor=0.01, test_execution_type=\"power\"\n",
    ")\n",
    "\n",
    "# Run complete lifecycle\n",
    "print(\"üöÄ Starting TPC-H power test on Redshift...\\n\")\n",
    "results = run_benchmark_lifecycle(\n",
    "    benchmark_config=bench_cfg,\n",
    "    database_config=db_cfg,\n",
    "    system_profile=None,\n",
    "    platform_config=platform_cfg,\n",
    "    phases=LifecyclePhases(generate=True, load=True, execute=True),\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ TPC-H power test completed!\")\n",
    "print(f\"   Benchmark: {results.benchmark_name}\")\n",
    "print(f\"   Total queries: {len(results.query_results)}\")\n",
    "print(f\"   Geometric mean: {results.geometric_mean:.3f}s\")\n",
    "print(f\"   Total execution time: {results.total_execution_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-cell",
   "metadata": {},
   "source": [
    "### Visualize Results\n",
    "\n",
    "Create a bar chart showing execution time for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results.query_results:\n",
    "    query_names = [qr.query_name for qr in results.query_results]\n",
    "    execution_times = [qr.execution_time for qr in results.query_results]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    bars = ax.bar(query_names, execution_times, color=\"#CC0000\", alpha=0.8, edgecolor=\"black\")\n",
    "\n",
    "    # Highlight slowest queries\n",
    "    max_time = max(execution_times)\n",
    "    for i, (bar, time) in enumerate(zip(bars, execution_times)):\n",
    "        if time > max_time * 0.7:  # Top 30% slowest\n",
    "            bar.set_color(\"#FF9900\")  # AWS orange\n",
    "            # Annotate with time\n",
    "            ax.text(i, time + 0.01, f\"{time:.2f}s\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "    ax.set_xlabel(\"Query\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Execution Time (seconds)\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_title(\"TPC-H Query Performance on Redshift (SF 0.01)\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nüìä Performance Summary:\")\n",
    "    print(f\"   Fastest query: {query_names[execution_times.index(min(execution_times))]} ({min(execution_times):.3f}s)\")\n",
    "    print(f\"   Slowest query: {query_names[execution_times.index(max(execution_times))]} ({max(execution_times):.3f}s)\")\n",
    "    print(f\"   Median time: {sorted(execution_times)[len(execution_times) // 2]:.3f}s\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No query results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cluster-monitor-cell",
   "metadata": {},
   "source": [
    "### Monitor Cluster Activity\n",
    "\n",
    "Check current cluster activity and query queue status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cluster-monitor-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = redshift_connector.connect(\n",
    "        host=REDSHIFT_HOST, port=REDSHIFT_PORT, database=REDSHIFT_DB, user=REDSHIFT_USER, password=REDSHIFT_PASSWORD\n",
    "    )\n",
    "\n",
    "    # Check active queries\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            pid,\n",
    "            user_name,\n",
    "            starttime,\n",
    "            query\n",
    "        FROM stv_inflight\n",
    "        WHERE user_name != 'rdsdb'\n",
    "        ORDER BY starttime DESC\n",
    "        LIMIT 5;\n",
    "    \"\"\"\n",
    "\n",
    "    df_active = pd.read_sql(query, conn)\n",
    "\n",
    "    print(\"üîÑ Active Queries:\")\n",
    "    if not df_active.empty:\n",
    "        for _, row in df_active.iterrows():\n",
    "            print(f\"   PID {row['pid']}: {row['query'][:80]}...\")\n",
    "    else:\n",
    "        print(\"   No active queries\")\n",
    "\n",
    "    # Check recent queries\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            query,\n",
    "            ROUND(total_exec_time/1000000.0, 2) as exec_time_sec,\n",
    "            rows,\n",
    "            queue_time/1000000 as queue_sec\n",
    "        FROM svl_query_summary\n",
    "        WHERE userid > 1\n",
    "        ORDER BY endtime DESC\n",
    "        LIMIT 5;\n",
    "    \"\"\"\n",
    "\n",
    "    df_recent = pd.read_sql(query, conn)\n",
    "\n",
    "    print(\"\\nüìú Recent Query Performance:\")\n",
    "    if not df_recent.empty:\n",
    "        for _, row in df_recent.iterrows():\n",
    "            print(f\"   Query {row['query']}: {row['exec_time_sec']}s, {row['rows']} rows\")\n",
    "    else:\n",
    "        print(\"   No recent queries\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not query cluster activity: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-overview-cell",
   "metadata": {},
   "source": [
    "### Results Overview\n",
    "\n",
    "Display detailed results including per-query breakdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-overview-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Detailed Results:\\n\")\n",
    "print(f\"Benchmark: {results.benchmark_name}\")\n",
    "print(f\"Platform: {results.platform}\")\n",
    "print(f\"Scale Factor: {results.scale_factor}\")\n",
    "print(f\"Test Type: {results.test_execution_type}\")\n",
    "print(f\"Timestamp: {results.start_time}\")\n",
    "print(\"\\nExecution Summary:\")\n",
    "print(f\"  Total queries: {len(results.query_results)}\")\n",
    "print(f\"  Successful: {sum(1 for qr in results.query_results if qr.success)}\")\n",
    "print(f\"  Failed: {sum(1 for qr in results.query_results if not qr.success)}\")\n",
    "print(f\"  Geometric mean: {results.geometric_mean:.3f}s\")\n",
    "print(f\"  Total time: {results.total_execution_time:.2f}s\")\n",
    "\n",
    "if results.data_generation_time:\n",
    "    print(f\"\\nData Generation: {results.data_generation_time:.2f}s\")\n",
    "if results.data_loading_time:\n",
    "    print(f\"Data Loading: {results.data_loading_time:.2f}s\")\n",
    "\n",
    "print(\"\\nüìã Query Breakdown:\")\n",
    "for qr in results.query_results[:5]:  # Show first 5\n",
    "    status = \"‚úÖ\" if qr.success else \"‚ùå\"\n",
    "    print(f\"  {status} {qr.query_name}: {qr.execution_time:.3f}s\")\n",
    "if len(results.query_results) > 5:\n",
    "    print(f\"  ... and {len(results.query_results) - 5} more queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## 3. Advanced Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tpcds-cell",
   "metadata": {},
   "source": [
    "### TPC-DS Benchmark\n",
    "\n",
    "Run the more complex TPC-DS benchmark (99 queries) with a smaller subset for faster iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tpcds-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TPC-DS with query subset\n",
    "tpcds_cfg = BenchmarkConfig(\n",
    "    name=\"tpcds\",\n",
    "    display_name=\"TPC-DS Sample\",\n",
    "    scale_factor=0.01,\n",
    "    test_execution_type=\"power\",\n",
    "    query_numbers=[1, 2, 3, 10, 25],  # Run subset for faster results\n",
    ")\n",
    "\n",
    "print(\"üöÄ Running TPC-DS subset on Redshift...\\n\")\n",
    "tpcds_results = run_benchmark_lifecycle(\n",
    "    benchmark_config=tpcds_cfg,\n",
    "    database_config=db_cfg,\n",
    "    system_profile=None,\n",
    "    platform_config=platform_cfg,\n",
    "    phases=LifecyclePhases(generate=True, load=True, execute=True),\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ TPC-DS completed: {tpcds_results.geometric_mean:.3f}s geometric mean\")\n",
    "print(f\"   Queries executed: {len(tpcds_results.query_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distkey-cell",
   "metadata": {},
   "source": [
    "### Distribution Keys and Sort Keys\n",
    "\n",
    "Optimize table performance using distribution and sort keys.\n",
    "\n",
    "**Distribution Styles:**\n",
    "- **KEY**: Distribute rows based on values in one column (best for joins)\n",
    "- **ALL**: Copy entire table to all nodes (best for small dimension tables)\n",
    "- **EVEN**: Distribute rows evenly across nodes (default, good for large fact tables)\n",
    "- **AUTO**: Let Redshift choose (recommended for most cases)\n",
    "\n",
    "**Sort Keys:**\n",
    "- **Compound**: Multiple columns, order matters (best for range queries)\n",
    "- **Interleaved**: Multiple columns, equal weight (best for multiple filter patterns)\n",
    "\n",
    "**Best Practices:**\n",
    "- Use KEY distribution on large tables' join columns\n",
    "- Use ALL distribution for small dimension tables (<1M rows)\n",
    "- Sort on date columns for time-series data\n",
    "- Sort on frequently filtered columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distkey-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure TPC-H with optimized distribution and sort keys\n",
    "optimized_cfg = {\n",
    "    \"host\": REDSHIFT_HOST,\n",
    "    \"port\": REDSHIFT_PORT,\n",
    "    \"database\": REDSHIFT_DB,\n",
    "    \"user\": REDSHIFT_USER,\n",
    "    \"password\": REDSHIFT_PASSWORD,\n",
    "    \"table_options\": {\n",
    "        \"customer\": {\n",
    "            \"distribution\": \"ALL\",  # Small dimension table\n",
    "            \"sort_keys\": [\"c_custkey\"],\n",
    "        },\n",
    "        \"orders\": {\n",
    "            \"distribution\": \"KEY\",\n",
    "            \"distribution_key\": \"o_custkey\",  # Join with customer\n",
    "            \"sort_keys\": [\"o_orderdate\", \"o_custkey\"],\n",
    "        },\n",
    "        \"lineitem\": {\n",
    "            \"distribution\": \"KEY\",\n",
    "            \"distribution_key\": \"l_orderkey\",  # Join with orders\n",
    "            \"sort_keys\": [\"l_shipdate\", \"l_orderkey\"],\n",
    "        },\n",
    "        \"part\": {\n",
    "            \"distribution\": \"ALL\",  # Small dimension table\n",
    "            \"sort_keys\": [\"p_partkey\"],\n",
    "        },\n",
    "        \"supplier\": {\n",
    "            \"distribution\": \"ALL\",  # Small dimension table\n",
    "            \"sort_keys\": [\"s_suppkey\"],\n",
    "        },\n",
    "        \"partsupp\": {\n",
    "            \"distribution\": \"KEY\",\n",
    "            \"distribution_key\": \"ps_partkey\",\n",
    "            \"sort_keys\": [\"ps_partkey\", \"ps_suppkey\"],\n",
    "        },\n",
    "        \"nation\": {\n",
    "            \"distribution\": \"ALL\",  # Very small reference table\n",
    "            \"sort_keys\": [\"n_nationkey\"],\n",
    "        },\n",
    "        \"region\": {\n",
    "            \"distribution\": \"ALL\",  # Very small reference table\n",
    "            \"sort_keys\": [\"r_regionkey\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Run with optimized settings\n",
    "print(\"üöÄ Running TPC-H with optimized distribution and sort keys...\\n\")\n",
    "optimized_results = run_benchmark_lifecycle(\n",
    "    benchmark_config=bench_cfg,\n",
    "    database_config=db_cfg,\n",
    "    system_profile=None,\n",
    "    platform_config=optimized_cfg,\n",
    "    phases=LifecyclePhases(generate=True, load=True, execute=True),\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Optimized run completed: {optimized_results.geometric_mean:.3f}s geometric mean\")\n",
    "print(f\"   Baseline: {results.geometric_mean:.3f}s\")\n",
    "improvement = ((results.geometric_mean - optimized_results.geometric_mean) / results.geometric_mean) * 100\n",
    "print(f\"   Improvement: {improvement:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scale-comp-cell",
   "metadata": {},
   "source": [
    "### Scale Factor Comparison\n",
    "\n",
    "Compare performance across different data sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scale-comp-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_results = {}\n",
    "\n",
    "for sf in [0.01, 0.1]:  # Test 10MB and 100MB\n",
    "    print(f\"\\nüöÄ Running TPC-H at scale factor {sf}...\")\n",
    "\n",
    "    sf_cfg = BenchmarkConfig(\n",
    "        name=\"tpch\",\n",
    "        display_name=f\"TPC-H SF {sf}\",\n",
    "        scale_factor=sf,\n",
    "        test_execution_type=\"power\",\n",
    "        query_numbers=list(range(1, 11)),  # First 10 queries only\n",
    "    )\n",
    "\n",
    "    sf_results = run_benchmark_lifecycle(\n",
    "        benchmark_config=sf_cfg,\n",
    "        database_config=db_cfg,\n",
    "        system_profile=None,\n",
    "        platform_config=platform_cfg,\n",
    "        phases=LifecyclePhases(generate=True, load=True, execute=True),\n",
    "    )\n",
    "\n",
    "    scale_results[sf] = sf_results.geometric_mean\n",
    "    print(f\"   Geometric mean: {sf_results.geometric_mean:.3f}s\")\n",
    "\n",
    "# Visualize scaling\n",
    "if len(scale_results) > 1:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sfs = list(scale_results.keys())\n",
    "    times = list(scale_results.values())\n",
    "\n",
    "    ax.plot(sfs, times, marker=\"o\", linewidth=2, markersize=10, color=\"#CC0000\")\n",
    "    ax.set_xlabel(\"Scale Factor\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Geometric Mean Time (seconds)\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_title(\"TPC-H Scaling on Redshift\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xscale(\"log\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nüìä Scaling Analysis:\")\n",
    "    for i in range(1, len(sfs)):\n",
    "        data_mult = sfs[i] / sfs[i - 1]\n",
    "        time_mult = times[i] / times[i - 1]\n",
    "        print(f\"   SF {sfs[i - 1]} ‚Üí {sfs[i]}: {data_mult}x data, {time_mult:.2f}x time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subset-cell",
   "metadata": {},
   "source": [
    "### Query Subset Selection\n",
    "\n",
    "Run specific queries for targeted testing or CI/CD pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subset-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast smoke test: Run 5 representative queries\n",
    "smoke_test_queries = [1, 3, 6, 10, 14]  # Mix of simple and complex\n",
    "\n",
    "subset_cfg = BenchmarkConfig(\n",
    "    name=\"tpch\",\n",
    "    display_name=\"TPC-H Smoke Test\",\n",
    "    scale_factor=0.01,\n",
    "    test_execution_type=\"power\",\n",
    "    query_numbers=smoke_test_queries,\n",
    ")\n",
    "\n",
    "print(f\"üöÄ Running smoke test with queries: {smoke_test_queries}\\n\")\n",
    "subset_results = run_benchmark_lifecycle(\n",
    "    benchmark_config=subset_cfg,\n",
    "    database_config=db_cfg,\n",
    "    system_profile=None,\n",
    "    platform_config=platform_cfg,\n",
    "    phases=LifecyclePhases(generate=False, load=False, execute=True),  # Reuse data\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Smoke test completed: {subset_results.geometric_mean:.3f}s geometric mean\")\n",
    "print(f\"   Queries: {len(subset_results.query_results)}\")\n",
    "print(f\"   Time saved vs full suite: ~{(1 - len(smoke_test_queries) / 22) * 100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wlm-cell",
   "metadata": {},
   "source": [
    "### Workload Management (WLM) Configuration\n",
    "\n",
    "Configure query queues for different workload priorities.\n",
    "\n",
    "**WLM Best Practices:**\n",
    "- Create separate queues for ETL, reporting, and ad-hoc queries\n",
    "- Allocate memory based on query complexity\n",
    "- Set concurrency limits to prevent resource contention\n",
    "- Use query monitoring rules to abort runaway queries\n",
    "\n",
    "**Example WLM Configuration:**\n",
    "```json\n",
    "[\n",
    "  {\"name\": \"etl\", \"concurrency\": 3, \"memory\": 40, \"priority\": \"highest\"},\n",
    "  {\"name\": \"reporting\", \"concurrency\": 5, \"memory\": 30, \"priority\": \"high\"},\n",
    "  {\"name\": \"adhoc\", \"concurrency\": 10, \"memory\": 20, \"priority\": \"normal\"},\n",
    "  {\"name\": \"default\", \"concurrency\": 5, \"memory\": 10, \"priority\": \"low\"}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wlm-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current WLM configuration\n",
    "try:\n",
    "    conn = redshift_connector.connect(\n",
    "        host=REDSHIFT_HOST, port=REDSHIFT_PORT, database=REDSHIFT_DB, user=REDSHIFT_USER, password=REDSHIFT_PASSWORD\n",
    "    )\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            service_class,\n",
    "            name,\n",
    "            num_query_tasks as concurrency,\n",
    "            query_working_mem as memory_mb\n",
    "        FROM stv_wlm_service_class_config\n",
    "        WHERE service_class >= 6\n",
    "        ORDER BY service_class;\n",
    "    \"\"\"\n",
    "\n",
    "    df_wlm = pd.read_sql(query, conn)\n",
    "\n",
    "    print(\"üéØ Current WLM Configuration:\\n\")\n",
    "    if not df_wlm.empty:\n",
    "        print(df_wlm.to_string(index=False))\n",
    "    else:\n",
    "        print(\"   Using default WLM configuration\")\n",
    "\n",
    "    # Check queue wait times\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            service_class,\n",
    "            COUNT(*) as query_count,\n",
    "            AVG(total_queue_time/1000000.0) as avg_queue_sec\n",
    "        FROM stl_wlm_query\n",
    "        WHERE service_class >= 6\n",
    "        GROUP BY service_class\n",
    "        ORDER BY service_class;\n",
    "    \"\"\"\n",
    "\n",
    "    df_queue = pd.read_sql(query, conn)\n",
    "\n",
    "    if not df_queue.empty:\n",
    "        print(\"\\n‚è±Ô∏è  Queue Statistics:\\n\")\n",
    "        print(df_queue.to_string(index=False))\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not query WLM configuration: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copy-s3-cell",
   "metadata": {},
   "source": [
    "### COPY from S3\n",
    "\n",
    "Load data efficiently from S3 using the COPY command.\n",
    "\n",
    "**Prerequisites:**\n",
    "- S3 bucket with data files\n",
    "- IAM role attached to cluster with S3 read permissions\n",
    "- Or AWS credentials with S3 access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copy-s3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if AWS_S3_BUCKET and AWS_IAM_ROLE:\n",
    "    print(\"üì¶ Example COPY command from S3:\\n\")\n",
    "\n",
    "    copy_cmd = f\"\"\"\n",
    "    COPY customer\n",
    "    FROM 's3://{AWS_S3_BUCKET}/tpch/customer/'\n",
    "    IAM_ROLE '{AWS_IAM_ROLE}'\n",
    "    DELIMITER '|'\n",
    "    REGION '{AWS_REGION}'\n",
    "    COMPUPDATE ON\n",
    "    STATUPDATE ON\n",
    "    MAXERROR 0;\n",
    "    \"\"\"\n",
    "\n",
    "    print(copy_cmd)\n",
    "\n",
    "    print(\"\\nüí° COPY Best Practices:\")\n",
    "    print(\"   - Split large files (100MB-1GB each)\")\n",
    "    print(\"   - Use GZIP compression to reduce I/O\")\n",
    "    print(\"   - Enable COMPUPDATE for automatic compression encoding\")\n",
    "    print(\"   - Enable STATUPDATE for automatic table statistics\")\n",
    "    print(\"   - Use manifest files for complex loads\")\n",
    "    print(\"   - Monitor STL_LOAD_ERRORS for failures\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  S3 bucket or IAM role not configured\")\n",
    "    print(\"   Set AWS_S3_BUCKET and AWS_IAM_ROLE environment variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "throughput-cell",
   "metadata": {},
   "source": [
    "### Throughput Test\n",
    "\n",
    "Run concurrent queries to test cluster throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "throughput-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run throughput test with 2 concurrent streams\n",
    "throughput_cfg = BenchmarkConfig(\n",
    "    name=\"tpch\",\n",
    "    display_name=\"TPC-H Throughput\",\n",
    "    scale_factor=0.01,\n",
    "    test_execution_type=\"throughput\",\n",
    "    query_numbers=list(range(1, 11)),  # First 10 queries\n",
    "    num_streams=2,  # Concurrent query streams\n",
    ")\n",
    "\n",
    "print(\"üöÄ Running throughput test with 2 concurrent streams...\\n\")\n",
    "throughput_results = run_benchmark_lifecycle(\n",
    "    benchmark_config=throughput_cfg,\n",
    "    database_config=db_cfg,\n",
    "    system_profile=None,\n",
    "    platform_config=platform_cfg,\n",
    "    phases=LifecyclePhases(generate=False, load=False, execute=True),\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Throughput test completed: {throughput_results.total_execution_time:.2f}s total time\")\n",
    "print(\n",
    "    f\"   Queries per hour: {(len(throughput_results.query_results) / throughput_results.total_execution_time) * 3600:.0f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-cell",
   "metadata": {},
   "source": [
    "### Compare Multiple Runs\n",
    "\n",
    "Compare different configurations or time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs optimized runs\n",
    "try:\n",
    "    comparator = BenchmarkComparator()\n",
    "\n",
    "    comparison = comparator.compare(\n",
    "        baseline=results,\n",
    "        comparison=optimized_results,\n",
    "        baseline_label=\"Default Distribution\",\n",
    "        comparison_label=\"Optimized Distribution\",\n",
    "    )\n",
    "\n",
    "    print(\"üìä Configuration Comparison:\\n\")\n",
    "    print(f\"Baseline (Default): {results.geometric_mean:.3f}s\")\n",
    "    print(f\"Optimized: {optimized_results.geometric_mean:.3f}s\")\n",
    "    print(f\"Improvement: {comparison['overall_improvement']:.1f}%\")\n",
    "\n",
    "    if comparison[\"regressions\"]:\n",
    "        print(f\"\\n‚ö†Ô∏è  Queries with regressions: {len(comparison['regressions'])}\")\n",
    "        for reg in comparison[\"regressions\"][:3]:\n",
    "            print(f\"   {reg['query']}: {reg['change']:+.1f}%\")\n",
    "\n",
    "    if comparison[\"improvements\"]:\n",
    "        print(f\"\\n‚úÖ Queries with improvements: {len(comparison['improvements'])}\")\n",
    "        for imp in comparison[\"improvements\"][:3]:\n",
    "            print(f\"   {imp['query']}: {imp['change']:+.1f}%\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not compare results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-cell",
   "metadata": {},
   "source": [
    "### Export Results\n",
    "\n",
    "Export benchmark results to various formats for reporting and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to multiple formats\n",
    "try:\n",
    "    exporter = ResultExporter(results)\n",
    "\n",
    "    output_dir = Path(config[\"output_dir\"])\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Export to JSON\n",
    "    json_path = output_dir / f\"redshift_tpch_{timestamp}.json\"\n",
    "    exporter.to_json(json_path)\n",
    "    print(f\"‚úÖ Exported JSON: {json_path}\")\n",
    "\n",
    "    # Export to CSV\n",
    "    csv_path = output_dir / f\"redshift_tpch_{timestamp}.csv\"\n",
    "    exporter.to_csv(csv_path)\n",
    "    print(f\"‚úÖ Exported CSV: {csv_path}\")\n",
    "\n",
    "    # Export to HTML report\n",
    "    html_path = output_dir / f\"redshift_tpch_{timestamp}.html\"\n",
    "    exporter.to_html(html_path)\n",
    "    print(f\"‚úÖ Exported HTML: {html_path}\")\n",
    "\n",
    "    print(f\"\\nüìÅ All results exported to: {output_dir}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Export failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost-cell",
   "metadata": {},
   "source": [
    "### Cost Analysis\n",
    "\n",
    "Estimate costs based on cluster configuration and runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cost-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost estimation (example with RA3.XLPLUS)\n",
    "try:\n",
    "    conn = redshift_connector.connect(\n",
    "        host=REDSHIFT_HOST, port=REDSHIFT_PORT, database=REDSHIFT_DB, user=REDSHIFT_USER, password=REDSHIFT_PASSWORD\n",
    "    )\n",
    "\n",
    "    # Get node count\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT COUNT(DISTINCT node) FROM stv_slices;\")\n",
    "    node_count = cursor.fetchone()[0]\n",
    "\n",
    "    # Cost assumptions (adjust for your region and node type)\n",
    "    cost_per_node_hour = 1.086  # RA3.XLPLUS in us-east-1\n",
    "\n",
    "    total_runtime_hours = results.total_execution_time / 3600\n",
    "    estimated_cost = node_count * cost_per_node_hour * total_runtime_hours\n",
    "\n",
    "    print(\"üí∞ Cost Estimation:\\n\")\n",
    "    print(f\"   Node count: {node_count}\")\n",
    "    print(f\"   Runtime: {total_runtime_hours:.4f} hours\")\n",
    "    print(f\"   Cost per node-hour: ${cost_per_node_hour}\")\n",
    "    print(f\"   Estimated cost: ${estimated_cost:.4f}\")\n",
    "\n",
    "    # Extrapolate to production scale\n",
    "    sf_multiplier = 1.0 / results.scale_factor  # Scale to SF 1.0\n",
    "    production_runtime = total_runtime_hours * sf_multiplier\n",
    "    production_cost = node_count * cost_per_node_hour * production_runtime\n",
    "\n",
    "    print(\"\\nüìä Extrapolated to SF 1.0:\")\n",
    "    print(f\"   Estimated runtime: {production_runtime:.2f} hours\")\n",
    "    print(f\"   Estimated cost: ${production_cost:.2f}\")\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not estimate costs: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "source": [
    "## 4. Platform-Specific Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concurrency-cell",
   "metadata": {},
   "source": [
    "### Concurrency Scaling\n",
    "\n",
    "Monitor and configure concurrency scaling to handle burst workloads.\n",
    "\n",
    "**How it works:**\n",
    "- Automatically adds cluster capacity when queues are long\n",
    "- Routes queries to transient clusters\n",
    "- Charged per-second of usage\n",
    "- Free tier: 1 hour per day per cluster\n",
    "\n",
    "**Best practices:**\n",
    "- Enable for read-heavy workloads\n",
    "- Monitor usage to control costs\n",
    "- Use for unpredictable query loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concurrency-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = redshift_connector.connect(\n",
    "        host=REDSHIFT_HOST, port=REDSHIFT_PORT, database=REDSHIFT_DB, user=REDSHIFT_USER, password=REDSHIFT_PASSWORD\n",
    "    )\n",
    "\n",
    "    # Check concurrency scaling usage\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            service_class,\n",
    "            COUNT(*) as query_count,\n",
    "            SUM(CASE WHEN concurrency_scaling_status = 1 THEN 1 ELSE 0 END) as cs_queries\n",
    "        FROM stl_query\n",
    "        WHERE userid > 1\n",
    "        GROUP BY service_class;\n",
    "    \"\"\"\n",
    "\n",
    "    df_cs = pd.read_sql(query, conn)\n",
    "\n",
    "    print(\"üöÄ Concurrency Scaling Usage:\\n\")\n",
    "    if not df_cs.empty:\n",
    "        print(df_cs.to_string(index=False))\n",
    "        total_cs = df_cs[\"cs_queries\"].sum()\n",
    "        total_queries = df_cs[\"query_count\"].sum()\n",
    "        if total_queries > 0:\n",
    "            cs_pct = (total_cs / total_queries) * 100\n",
    "            print(f\"\\n   Concurrency scaling usage: {cs_pct:.1f}%\")\n",
    "    else:\n",
    "        print(\"   No concurrency scaling usage\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    print(\"\\nüí° To enable concurrency scaling:\")\n",
    "    print(\"   ALTER WORKLOAD GROUP my_group SET concurrency_scaling = auto;\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not query concurrency scaling: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compression-cell",
   "metadata": {},
   "source": [
    "### Compression Encodings\n",
    "\n",
    "Redshift automatically chooses compression encodings, but you can optimize manually.\n",
    "\n",
    "**Available encodings:**\n",
    "- **RAW**: No compression\n",
    "- **AZ64**: Best for numeric data (default)\n",
    "- **LZO**: Fast compression for text\n",
    "- **ZSTD**: High compression ratio for mixed data\n",
    "- **DELTA**: Best for sorted numeric columns\n",
    "- **RUNLENGTH**: Best for low-cardinality columns\n",
    "\n",
    "**Check current encodings:**\n",
    "```sql\n",
    "SELECT \n",
    "    tablename,\n",
    "    column,\n",
    "    encoding,\n",
    "    distkey,\n",
    "    sortkey\n",
    "FROM pg_table_def\n",
    "WHERE schemaname = 'public'\n",
    "ORDER BY tablename, column;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compression-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = redshift_connector.connect(\n",
    "        host=REDSHIFT_HOST, port=REDSHIFT_PORT, database=REDSHIFT_DB, user=REDSHIFT_USER, password=REDSHIFT_PASSWORD\n",
    "    )\n",
    "\n",
    "    # Check compression for TPC-H tables\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            tablename,\n",
    "            \"column\",\n",
    "            encoding\n",
    "        FROM pg_table_def\n",
    "        WHERE schemaname = 'public'\n",
    "            AND tablename IN ('customer', 'orders', 'lineitem')\n",
    "        ORDER BY tablename, column;\n",
    "    \"\"\"\n",
    "\n",
    "    df_encoding = pd.read_sql(query, conn)\n",
    "\n",
    "    print(\"üóúÔ∏è  Compression Encodings:\\n\")\n",
    "    if not df_encoding.empty:\n",
    "        for table in df_encoding[\"tablename\"].unique():\n",
    "            table_df = df_encoding[df_encoding[\"tablename\"] == table]\n",
    "            print(f\"\\n{table}:\")\n",
    "            for _, row in table_df.head(5).iterrows():  # Show first 5 columns\n",
    "                print(f\"   {row['column']}: {row['encoding']}\")\n",
    "    else:\n",
    "        print(\"   No tables found\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    print(\"\\nüí° To analyze compression recommendations:\")\n",
    "    print(\"   ANALYZE COMPRESSION customer;\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not query compression encodings: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vacuum-cell",
   "metadata": {},
   "source": [
    "### VACUUM and ANALYZE\n",
    "\n",
    "Maintain table performance with VACUUM and ANALYZE operations.\n",
    "\n",
    "**VACUUM:**\n",
    "- Reclaims space from deleted rows\n",
    "- Re-sorts rows according to sort keys\n",
    "- Types: FULL, DELETE ONLY, SORT ONLY, REINDEX\n",
    "\n",
    "**ANALYZE:**\n",
    "- Updates table statistics for query planner\n",
    "- Run after loading data or major changes\n",
    "\n",
    "**Best practices:**\n",
    "- Run VACUUM during maintenance windows\n",
    "- Use VACUUM DELETE ONLY for frequent deletes\n",
    "- Run ANALYZE after COPY or INSERT operations\n",
    "- Automatic VACUUM runs in background (can be disabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vacuum-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = redshift_connector.connect(\n",
    "        host=REDSHIFT_HOST, port=REDSHIFT_PORT, database=REDSHIFT_DB, user=REDSHIFT_USER, password=REDSHIFT_PASSWORD\n",
    "    )\n",
    "\n",
    "    # Check table statistics\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            \"table\",\n",
    "            unsorted,\n",
    "            stats_off,\n",
    "            tbl_rows\n",
    "        FROM svv_table_info\n",
    "        WHERE \"schema\" = 'public'\n",
    "        ORDER BY unsorted DESC;\n",
    "    \"\"\"\n",
    "\n",
    "    df_stats = pd.read_sql(query, conn)\n",
    "\n",
    "    print(\"üîß Table Maintenance Status:\\n\")\n",
    "    if not df_stats.empty:\n",
    "        print(\"Table                Unsorted%  Stats Off%  Rows\")\n",
    "        print(\"=\" * 60)\n",
    "        for _, row in df_stats.head(10).iterrows():\n",
    "            print(f\"{row['table']:<20} {row['unsorted']:>8.1f}%  {row['stats_off']:>10.1f}%  {row['tbl_rows']:>10,}\")\n",
    "\n",
    "        # Recommendations\n",
    "        needs_vacuum = df_stats[df_stats[\"unsorted\"] > 20]\n",
    "        needs_analyze = df_stats[df_stats[\"stats_off\"] > 10]\n",
    "\n",
    "        if not needs_vacuum.empty:\n",
    "            print(f\"\\n‚ö†Ô∏è  {len(needs_vacuum)} tables need VACUUM (>20% unsorted)\")\n",
    "        if not needs_analyze.empty:\n",
    "            print(f\"‚ö†Ô∏è  {len(needs_analyze)} tables need ANALYZE (>10% stats off)\")\n",
    "    else:\n",
    "        print(\"   No tables found\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    print(\"\\nüí° Maintenance commands:\")\n",
    "    print(\"   VACUUM orders;\")\n",
    "    print(\"   ANALYZE lineitem;\")\n",
    "    print(\"   VACUUM FULL;  -- All tables\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not query table statistics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectrum-cell",
   "metadata": {},
   "source": [
    "### Redshift Spectrum\n",
    "\n",
    "Query external data in S3 without loading into Redshift.\n",
    "\n",
    "**Use cases:**\n",
    "- Query historical data stored in S3\n",
    "- Join S3 data with Redshift tables\n",
    "- Reduce storage costs\n",
    "- Process data lake files\n",
    "\n",
    "**Example:**\n",
    "```sql\n",
    "CREATE EXTERNAL SCHEMA spectrum\n",
    "FROM data catalog\n",
    "DATABASE 'mydb'\n",
    "IAM_ROLE 'arn:aws:iam::123456789012:role/SpectrumRole'\n",
    "CREATE EXTERNAL DATABASE IF NOT EXISTS;\n",
    "\n",
    "CREATE EXTERNAL TABLE spectrum.orders (\n",
    "    o_orderkey bigint,\n",
    "    o_custkey bigint,\n",
    "    o_orderstatus varchar(1),\n",
    "    o_totalprice decimal(15,2),\n",
    "    o_orderdate date\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION 's3://my-bucket/orders/';\n",
    "\n",
    "-- Query S3 data\n",
    "SELECT COUNT(*) FROM spectrum.orders;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectrum-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if AWS_S3_BUCKET and AWS_IAM_ROLE:\n",
    "    print(\"üåü Redshift Spectrum Example\\n\")\n",
    "    print(\"To create external schema:\")\n",
    "    print(f\"\"\"\n",
    "CREATE EXTERNAL SCHEMA spectrum\n",
    "FROM data catalog\n",
    "DATABASE 'benchbox'\n",
    "IAM_ROLE '{AWS_IAM_ROLE}'\n",
    "CREATE EXTERNAL DATABASE IF NOT EXISTS;\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"\\nTo create external table:\")\n",
    "    print(f\"\"\"\n",
    "CREATE EXTERNAL TABLE spectrum.lineitem (\n",
    "    l_orderkey bigint,\n",
    "    l_partkey bigint,\n",
    "    l_suppkey bigint,\n",
    "    l_linenumber int,\n",
    "    l_quantity decimal(15,2),\n",
    "    l_extendedprice decimal(15,2),\n",
    "    l_discount decimal(15,2),\n",
    "    l_tax decimal(15,2),\n",
    "    l_returnflag varchar(1),\n",
    "    l_linestatus varchar(1),\n",
    "    l_shipdate date,\n",
    "    l_commitdate date,\n",
    "    l_receiptdate date,\n",
    "    l_shipinstruct varchar(25),\n",
    "    l_shipmode varchar(10),\n",
    "    l_comment varchar(44)\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION 's3://{AWS_S3_BUCKET}/tpch/lineitem/';\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"\\nüí∞ Spectrum Pricing:\")\n",
    "    print(\"   $5 per TB of data scanned from S3\")\n",
    "    print(\"   Use partitioning to reduce data scanned\")\n",
    "    print(\"   Use columnar formats (Parquet, ORC) for efficiency\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  S3 bucket and IAM role required for Spectrum\")\n",
    "    print(\"   Set AWS_S3_BUCKET and AWS_IAM_ROLE environment variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "source": [
    "## 5. Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-results-cell",
   "metadata": {},
   "source": [
    "### Load and Analyze Previous Results\n",
    "\n",
    "Load saved benchmark results for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-results-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most recent result file\n",
    "try:\n",
    "    result_files = sorted(Path(config[\"output_dir\"]).glob(\"redshift_tpch_*.json\"))\n",
    "\n",
    "    if result_files:\n",
    "        latest_file = result_files[-1]\n",
    "        print(f\"üìÇ Loading results from: {latest_file.name}\\n\")\n",
    "\n",
    "        loader = ResultLoader()\n",
    "        loaded_results = loader.load_json(latest_file)\n",
    "\n",
    "        print(f\"‚úÖ Loaded {len(loaded_results.query_results)} query results\")\n",
    "        print(f\"   Benchmark: {loaded_results.benchmark_name}\")\n",
    "        print(f\"   Scale factor: {loaded_results.scale_factor}\")\n",
    "        print(f\"   Geometric mean: {loaded_results.geometric_mean:.3f}s\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No result files found. Run a benchmark first.\")\n",
    "        loaded_results = results  # Use current results\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load results: {e}\")\n",
    "    loaded_results = results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats-cell",
   "metadata": {},
   "source": [
    "### Statistical Analysis\n",
    "\n",
    "Calculate detailed statistics on query performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if loaded_results.query_results:\n",
    "    times = [qr.execution_time for qr in loaded_results.query_results if qr.success]\n",
    "\n",
    "    if times:\n",
    "        # Calculate statistics\n",
    "        import numpy as np\n",
    "\n",
    "        stats = {\n",
    "            \"count\": len(times),\n",
    "            \"mean\": np.mean(times),\n",
    "            \"median\": np.median(times),\n",
    "            \"std\": np.std(times),\n",
    "            \"min\": np.min(times),\n",
    "            \"max\": np.max(times),\n",
    "            \"p25\": np.percentile(times, 25),\n",
    "            \"p75\": np.percentile(times, 75),\n",
    "            \"p95\": np.percentile(times, 95),\n",
    "            \"p99\": np.percentile(times, 99),\n",
    "        }\n",
    "\n",
    "        print(\"üìä Statistical Summary:\\n\")\n",
    "        print(f\"Count:      {stats['count']} queries\")\n",
    "        print(f\"Mean:       {stats['mean']:.3f}s\")\n",
    "        print(f\"Median:     {stats['median']:.3f}s\")\n",
    "        print(f\"Std Dev:    {stats['std']:.3f}s\")\n",
    "        print(f\"Min:        {stats['min']:.3f}s\")\n",
    "        print(f\"Max:        {stats['max']:.3f}s\")\n",
    "        print(\"\\nPercentiles:\")\n",
    "        print(f\"P25:        {stats['p25']:.3f}s\")\n",
    "        print(f\"P75:        {stats['p75']:.3f}s\")\n",
    "        print(f\"P95:        {stats['p95']:.3f}s\")\n",
    "        print(f\"P99:        {stats['p99']:.3f}s\")\n",
    "\n",
    "        # Coefficient of variation\n",
    "        cv = stats[\"std\"] / stats[\"mean\"]\n",
    "        print(f\"\\nCoefficient of Variation: {cv:.2f}\")\n",
    "        if cv < 0.5:\n",
    "            print(\"   ‚úÖ Low variability - consistent performance\")\n",
    "        elif cv < 1.0:\n",
    "            print(\"   ‚ö†Ô∏è  Moderate variability\")\n",
    "        else:\n",
    "            print(\"   ‚ùå High variability - investigate outliers\")\n",
    "\n",
    "        # Identify outliers (>2 std devs from mean)\n",
    "        outliers = [\n",
    "            qr\n",
    "            for qr in loaded_results.query_results\n",
    "            if qr.success and abs(qr.execution_time - stats[\"mean\"]) > 2 * stats[\"std\"]\n",
    "        ]\n",
    "\n",
    "        if outliers:\n",
    "            print(f\"\\n‚ö†Ô∏è  {len(outliers)} outlier queries detected:\")\n",
    "            for qr in outliers[:5]:\n",
    "                deviation = (qr.execution_time - stats[\"mean\"]) / stats[\"std\"]\n",
    "                print(f\"   {qr.query_name}: {qr.execution_time:.3f}s ({deviation:+.1f}œÉ)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No successful queries to analyze\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No query results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-advanced-cell",
   "metadata": {},
   "source": [
    "### Advanced Visualizations\n",
    "\n",
    "Create comprehensive performance visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-advanced-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if loaded_results.query_results:\n",
    "    times = [qr.execution_time for qr in loaded_results.query_results if qr.success]\n",
    "    query_names = [qr.query_name for qr in loaded_results.query_results if qr.success]\n",
    "\n",
    "    if times:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "        # 1. Histogram with distribution\n",
    "        ax1 = axes[0, 0]\n",
    "        ax1.hist(times, bins=20, color=\"#CC0000\", alpha=0.7, edgecolor=\"black\")\n",
    "        ax1.axvline(np.mean(times), color=\"#FF9900\", linestyle=\"--\", linewidth=2, label=f\"Mean: {np.mean(times):.2f}s\")\n",
    "        ax1.axvline(\n",
    "            np.median(times), color=\"green\", linestyle=\"--\", linewidth=2, label=f\"Median: {np.median(times):.2f}s\"\n",
    "        )\n",
    "        ax1.set_xlabel(\"Execution Time (seconds)\", fontweight=\"bold\")\n",
    "        ax1.set_ylabel(\"Frequency\", fontweight=\"bold\")\n",
    "        ax1.set_title(\"Query Execution Time Distribution\", fontweight=\"bold\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        # 2. Box plot\n",
    "        ax2 = axes[0, 1]\n",
    "        box = ax2.boxplot([times], vert=True, patch_artist=True, widths=0.5)\n",
    "        box[\"boxes\"][0].set_facecolor(\"#CC0000\")\n",
    "        box[\"boxes\"][0].set_alpha(0.7)\n",
    "        ax2.set_ylabel(\"Execution Time (seconds)\", fontweight=\"bold\")\n",
    "        ax2.set_title(\"Query Performance Box Plot\", fontweight=\"bold\")\n",
    "        ax2.set_xticklabels([\"All Queries\"])\n",
    "        ax2.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        # 3. Sorted bar chart (top 10 slowest)\n",
    "        ax3 = axes[1, 0]\n",
    "        sorted_indices = np.argsort(times)[-10:]\n",
    "        sorted_times = [times[i] for i in sorted_indices]\n",
    "        sorted_names = [query_names[i] for i in sorted_indices]\n",
    "\n",
    "        bars = ax3.barh(sorted_names, sorted_times, color=\"#CC0000\", alpha=0.8, edgecolor=\"black\")\n",
    "        ax3.set_xlabel(\"Execution Time (seconds)\", fontweight=\"bold\")\n",
    "        ax3.set_title(\"Top 10 Slowest Queries\", fontweight=\"bold\")\n",
    "        ax3.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "        # 4. Cumulative performance (Pareto)\n",
    "        ax4 = axes[1, 1]\n",
    "        sorted_all_indices = np.argsort(times)[::-1]\n",
    "        sorted_all_times = [times[i] for i in sorted_all_indices]\n",
    "        cumulative = np.cumsum(sorted_all_times)\n",
    "        cumulative_pct = (cumulative / cumulative[-1]) * 100\n",
    "\n",
    "        ax4.plot(range(len(cumulative_pct)), cumulative_pct, marker=\"o\", color=\"#CC0000\", linewidth=2)\n",
    "        ax4.axhline(80, color=\"#FF9900\", linestyle=\"--\", linewidth=2, label=\"80% of total time\")\n",
    "        ax4.set_xlabel(\"Number of Queries (sorted by time)\", fontweight=\"bold\")\n",
    "        ax4.set_ylabel(\"Cumulative % of Total Time\", fontweight=\"bold\")\n",
    "        ax4.set_title(\"Cumulative Performance (Pareto Analysis)\", fontweight=\"bold\")\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "\n",
    "        # Find how many queries account for 80% of time\n",
    "        queries_80pct = np.argmax(cumulative_pct >= 80) + 1\n",
    "        ax4.axvline(\n",
    "            queries_80pct, color=\"green\", linestyle=\"--\", linewidth=2, label=f\"{queries_80pct} queries = 80% time\"\n",
    "        )\n",
    "        ax4.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\n",
    "            f\"\\nüí° Insight: {queries_80pct} queries ({queries_80pct / len(times) * 100:.0f}%) account for 80% of total execution time\"\n",
    "        )\n",
    "        print(\"   Focus optimization efforts on these queries for maximum impact\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No successful queries to visualize\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No query results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query-history-cell",
   "metadata": {},
   "source": [
    "### Query History Analysis\n",
    "\n",
    "Analyze historical query patterns using STL system tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-history-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = redshift_connector.connect(\n",
    "        host=REDSHIFT_HOST, port=REDSHIFT_PORT, database=REDSHIFT_DB, user=REDSHIFT_USER, password=REDSHIFT_PASSWORD\n",
    "    )\n",
    "\n",
    "    # Top 10 slowest queries by total execution time\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            query,\n",
    "            TRIM(database) as db,\n",
    "            ROUND(total_exec_time/1000000.0, 2) as exec_sec,\n",
    "            ROUND(compile_time/1000000.0, 2) as compile_sec,\n",
    "            rows,\n",
    "            aborted\n",
    "        FROM svl_query_summary\n",
    "        WHERE userid > 1\n",
    "            AND starttime >= DATEADD(hour, -24, GETDATE())\n",
    "        ORDER BY total_exec_time DESC\n",
    "        LIMIT 10;\n",
    "    \"\"\"\n",
    "\n",
    "    df_slow = pd.read_sql(query, conn)\n",
    "\n",
    "    print(\"üêå Slowest Queries (Last 24h):\\n\")\n",
    "    if not df_slow.empty:\n",
    "        print(\"Query ID  Exec(s)  Compile(s)  Rows        Aborted\")\n",
    "        print(\"=\" * 60)\n",
    "        for _, row in df_slow.iterrows():\n",
    "            print(\n",
    "                f\"{row['query']:<9} {row['exec_sec']:>7.2f}  {row['compile_sec']:>10.2f}  {row['rows']:>10,}  {row['aborted']}\"\n",
    "            )\n",
    "    else:\n",
    "        print(\"   No queries found in last 24 hours\")\n",
    "\n",
    "    # Query queue wait times\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            service_class,\n",
    "            COUNT(*) as query_count,\n",
    "            AVG(total_queue_time/1000000.0) as avg_queue_sec,\n",
    "            MAX(total_queue_time/1000000.0) as max_queue_sec\n",
    "        FROM stl_wlm_query\n",
    "        WHERE service_class >= 6\n",
    "            AND queue_start_time >= DATEADD(hour, -24, GETDATE())\n",
    "        GROUP BY service_class\n",
    "        ORDER BY avg_queue_sec DESC;\n",
    "    \"\"\"\n",
    "\n",
    "    df_queue = pd.read_sql(query, conn)\n",
    "\n",
    "    if not df_queue.empty:\n",
    "        print(\"\\n‚è±Ô∏è  Queue Wait Times:\\n\")\n",
    "        print(\"Queue  Queries  Avg Wait(s)  Max Wait(s)\")\n",
    "        print(\"=\" * 50)\n",
    "        for _, row in df_queue.iterrows():\n",
    "            print(\n",
    "                f\"{row['service_class']:<6} {row['query_count']:>7,}  {row['avg_queue_sec']:>11.2f}  {row['max_queue_sec']:>11.2f}\"\n",
    "            )\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not query history: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disk-analysis-cell",
   "metadata": {},
   "source": [
    "### Disk Space Analysis\n",
    "\n",
    "Monitor table sizes and disk usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disk-analysis-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = redshift_connector.connect(\n",
    "        host=REDSHIFT_HOST, port=REDSHIFT_PORT, database=REDSHIFT_DB, user=REDSHIFT_USER, password=REDSHIFT_PASSWORD\n",
    "    )\n",
    "\n",
    "    # Table sizes\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            \"table\",\n",
    "            ROUND(size/1024.0, 2) as size_gb,\n",
    "            tbl_rows as rows,\n",
    "            ROUND((size/1024.0) / NULLIF(tbl_rows, 0) * 1024 * 1024, 2) as bytes_per_row\n",
    "        FROM svv_table_info\n",
    "        WHERE \"schema\" = 'public'\n",
    "        ORDER BY size DESC\n",
    "        LIMIT 10;\n",
    "    \"\"\"\n",
    "\n",
    "    df_size = pd.read_sql(query, conn)\n",
    "\n",
    "    print(\"üíæ Table Storage Analysis:\\n\")\n",
    "    if not df_size.empty:\n",
    "        print(\"Table              Size (GB)      Rows  Bytes/Row\")\n",
    "        print(\"=\" * 60)\n",
    "        for _, row in df_size.iterrows():\n",
    "            print(f\"{row['table']:<18} {row['size_gb']:>9.2f}  {row['rows']:>10,}  {row['bytes_per_row']:>9.0f}\")\n",
    "\n",
    "        total_gb = df_size[\"size_gb\"].sum()\n",
    "        print(f\"\\nTotal: {total_gb:.2f} GB\")\n",
    "    else:\n",
    "        print(\"   No tables found\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not analyze disk usage: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regression-cell",
   "metadata": {},
   "source": [
    "### Regression Detection\n",
    "\n",
    "Compare current results against baseline to detect performance regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regression-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare current run against baseline (if available)\n",
    "baseline_path = Path(config[\"output_dir\"]) / \"baseline_redshift.json\"\n",
    "\n",
    "if baseline_path.exists():\n",
    "    try:\n",
    "        loader = ResultLoader()\n",
    "        baseline = loader.load_json(baseline_path)\n",
    "\n",
    "        comparator = BenchmarkComparator()\n",
    "        comparison = comparator.compare(\n",
    "            baseline=baseline, comparison=loaded_results, baseline_label=\"Baseline\", comparison_label=\"Current\"\n",
    "        )\n",
    "\n",
    "        print(\"üîç Regression Detection:\\n\")\n",
    "        print(f\"Baseline: {baseline.geometric_mean:.3f}s\")\n",
    "        print(f\"Current:  {loaded_results.geometric_mean:.3f}s\")\n",
    "        print(f\"Change:   {comparison['overall_improvement']:+.1f}%\")\n",
    "\n",
    "        # Flag regressions (>10% slower)\n",
    "        regressions = [r for r in comparison[\"regressions\"] if r[\"change\"] < -10]\n",
    "\n",
    "        if regressions:\n",
    "            print(f\"\\n‚ùå {len(regressions)} queries regressed >10%:\")\n",
    "            for reg in regressions[:5]:\n",
    "                print(f\"   {reg['query']}: {reg['baseline']:.3f}s ‚Üí {reg['comparison']:.3f}s ({reg['change']:+.1f}%)\")\n",
    "        else:\n",
    "            print(\"\\n‚úÖ No significant regressions detected\")\n",
    "\n",
    "        # Flag improvements (>10% faster)\n",
    "        improvements = [i for i in comparison[\"improvements\"] if i[\"change\"] > 10]\n",
    "\n",
    "        if improvements:\n",
    "            print(f\"\\n‚úÖ {len(improvements)} queries improved >10%:\")\n",
    "            for imp in improvements[:5]:\n",
    "                print(f\"   {imp['query']}: {imp['baseline']:.3f}s ‚Üí {imp['comparison']:.3f}s ({imp['change']:+.1f}%)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not compare results: {e}\")\n",
    "else:\n",
    "    print(\"üí° No baseline found. To enable regression detection:\")\n",
    "    print(\"   1. Run a baseline benchmark\")\n",
    "    print(f\"   2. Save results to: {baseline_path}\")\n",
    "    print(\"   3. Re-run this cell\")\n",
    "\n",
    "    # Optionally save current results as baseline\n",
    "    save_baseline = False  # Set to True to save\n",
    "    if save_baseline:\n",
    "        try:\n",
    "            exporter = ResultExporter(loaded_results)\n",
    "            exporter.to_json(baseline_path)\n",
    "            print(f\"\\n‚úÖ Saved current results as baseline: {baseline_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not save baseline: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "source": [
    "## 6. Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diag-cell",
   "metadata": {},
   "source": [
    "### Connection Diagnostics\n",
    "\n",
    "Comprehensive diagnostic tool for troubleshooting connection issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diag-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_redshift_connection():\n",
    "    \"\"\"Diagnose Redshift connection issues\"\"\"\n",
    "    print(\"üîç Redshift Connection Diagnostic\\n\")\n",
    "\n",
    "    # Check 1: Environment variables\n",
    "    print(\"1. Checking environment variables...\")\n",
    "    required_vars = {\n",
    "        \"REDSHIFT_HOST\": REDSHIFT_HOST,\n",
    "        \"REDSHIFT_DB\": REDSHIFT_DB,\n",
    "        \"REDSHIFT_USER\": REDSHIFT_USER,\n",
    "        \"REDSHIFT_PASSWORD\": \"***\" if REDSHIFT_PASSWORD else None,\n",
    "    }\n",
    "\n",
    "    all_set = True\n",
    "    for var, value in required_vars.items():\n",
    "        if value:\n",
    "            print(f\"   ‚úÖ {var} is set\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {var} is not set\")\n",
    "            all_set = False\n",
    "\n",
    "    if not all_set:\n",
    "        print(\"\\n   Action: Set missing environment variables\")\n",
    "        return False\n",
    "\n",
    "    # Check 2: Host format\n",
    "    print(\"\\n2. Validating host format...\")\n",
    "    if \".redshift.amazonaws.com\" in REDSHIFT_HOST or \".redshift-serverless.amazonaws.com\" in REDSHIFT_HOST:\n",
    "        print(f\"   ‚úÖ Host format looks correct: {REDSHIFT_HOST}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Host may be incorrect: {REDSHIFT_HOST}\")\n",
    "        print(\"   Expected format: <cluster>.<region>.redshift.amazonaws.com\")\n",
    "\n",
    "    # Check 3: Port\n",
    "    print(\"\\n3. Checking port...\")\n",
    "    if REDSHIFT_PORT == 5439:\n",
    "        print(f\"   ‚úÖ Using default Redshift port: {REDSHIFT_PORT}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Using non-standard port: {REDSHIFT_PORT}\")\n",
    "\n",
    "    # Check 4: Test connection\n",
    "    print(\"\\n4. Testing connection...\")\n",
    "    try:\n",
    "        conn = redshift_connector.connect(\n",
    "            host=REDSHIFT_HOST,\n",
    "            port=REDSHIFT_PORT,\n",
    "            database=REDSHIFT_DB,\n",
    "            user=REDSHIFT_USER,\n",
    "            password=REDSHIFT_PASSWORD,\n",
    "            timeout=10,\n",
    "        )\n",
    "        conn.close()\n",
    "        print(\"   ‚úÖ Connection successful!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Connection failed: {str(e)[:100]}\")\n",
    "\n",
    "        # Provide specific guidance\n",
    "        if \"timeout\" in str(e).lower():\n",
    "            print(\"\\n   Likely cause: Network connectivity\")\n",
    "            print(\"   - Check security group allows inbound on port 5439\")\n",
    "            print(\"   - Verify VPC routing and network ACLs\")\n",
    "            print(\"   - Ensure cluster is publicly accessible (if connecting from outside VPC)\")\n",
    "        elif \"authentication\" in str(e).lower() or \"password\" in str(e).lower():\n",
    "            print(\"\\n   Likely cause: Invalid credentials\")\n",
    "            print(\"   - Verify username and password\")\n",
    "            print(\"   - Check user has CONNECT privilege on database\")\n",
    "        elif \"database\" in str(e).lower():\n",
    "            print(\"\\n   Likely cause: Invalid database name\")\n",
    "            print(\"   - Verify database exists\")\n",
    "            print(\"   - Check spelling (case-sensitive)\")\n",
    "        else:\n",
    "            print(\"\\n   Check AWS Console for cluster status\")\n",
    "\n",
    "        return False\n",
    "\n",
    "    print(\"\\nüìö Additional Resources:\")\n",
    "    print(\"   - Amazon Redshift Documentation: https://docs.aws.amazon.com/redshift/\")\n",
    "    print(\"   - Security Groups: https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-security-groups.html\")\n",
    "    print(\"   - Connection Issues: https://docs.aws.amazon.com/redshift/latest/mgmt/connecting-troubleshooting.html\")\n",
    "\n",
    "\n",
    "# Run diagnostics\n",
    "diagnose_redshift_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permissions-cell",
   "metadata": {},
   "source": [
    "### Permission Validation\n",
    "\n",
    "Check user permissions for common operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permissions-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = redshift_connector.connect(\n",
    "        host=REDSHIFT_HOST, port=REDSHIFT_PORT, database=REDSHIFT_DB, user=REDSHIFT_USER, password=REDSHIFT_PASSWORD\n",
    "    )\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    print(\"üîê Permission Check\\n\")\n",
    "\n",
    "    # Check CREATE privilege\n",
    "    try:\n",
    "        cursor.execute(\"CREATE TEMP TABLE permission_test (id INT);\")\n",
    "        cursor.execute(\"DROP TABLE permission_test;\")\n",
    "        print(\"‚úÖ CREATE TABLE: Allowed\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå CREATE TABLE: Denied ({str(e)[:50]}...)\")\n",
    "\n",
    "    # Check SELECT privilege\n",
    "    try:\n",
    "        cursor.execute(\"SELECT 1;\")\n",
    "        print(\"‚úÖ SELECT: Allowed\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå SELECT: Denied ({str(e)[:50]}...)\")\n",
    "\n",
    "    # Check system table access\n",
    "    try:\n",
    "        cursor.execute(\"SELECT 1 FROM stv_slices LIMIT 1;\")\n",
    "        print(\"‚úÖ System tables: Allowed\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  System tables: Limited access ({str(e)[:50]}...)\")\n",
    "\n",
    "    # Check COPY privilege (requires S3)\n",
    "    if AWS_S3_BUCKET and AWS_IAM_ROLE:\n",
    "        try:\n",
    "            # This will fail on missing S3 file, but tells us if COPY is allowed\n",
    "            cursor.execute(\"\"\"\n",
    "                CREATE TEMP TABLE copy_test (id INT);\n",
    "            \"\"\")\n",
    "            print(\"‚úÖ COPY capability: Available (IAM role configured)\")\n",
    "            cursor.execute(\"DROP TABLE copy_test;\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  COPY: May have issues ({str(e)[:50]}...)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  COPY: S3 configuration not set\")\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    print(\"\\nüí° To grant permissions:\")\n",
    "    print(\"   GRANT CREATE ON SCHEMA public TO your_user;\")\n",
    "    print(\"   GRANT SELECT ON ALL TABLES IN SCHEMA public TO your_user;\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Permission check failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cluster-diag-cell",
   "metadata": {},
   "source": [
    "### Cluster Health Check\n",
    "\n",
    "Check cluster health and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cluster-diag-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = redshift_connector.connect(\n",
    "        host=REDSHIFT_HOST, port=REDSHIFT_PORT, database=REDSHIFT_DB, user=REDSHIFT_USER, password=REDSHIFT_PASSWORD\n",
    "    )\n",
    "\n",
    "    print(\"üè• Cluster Health Check\\n\")\n",
    "\n",
    "    # Check node health\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            node,\n",
    "            slice,\n",
    "            node_type\n",
    "        FROM stv_slices\n",
    "        WHERE slice = 0\n",
    "        ORDER BY node;\n",
    "    \"\"\"\n",
    "\n",
    "    df_nodes = pd.read_sql(query, conn)\n",
    "\n",
    "    if not df_nodes.empty:\n",
    "        print(f\"‚úÖ Cluster has {len(df_nodes)} healthy nodes\")\n",
    "        if len(df_nodes) > 0:\n",
    "            print(f\"   Node type: {df_nodes.iloc[0]['node_type']}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Could not determine node count\")\n",
    "\n",
    "    # Check disk space\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            ROUND(SUM(used)::float/SUM(capacity)::float*100, 1) as pct_used\n",
    "        FROM stv_partitions;\n",
    "    \"\"\"\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    pct_used = cursor.fetchone()[0]\n",
    "\n",
    "    if pct_used < 75:\n",
    "        print(f\"‚úÖ Disk usage: {pct_used}% (healthy)\")\n",
    "    elif pct_used < 90:\n",
    "        print(f\"‚ö†Ô∏è  Disk usage: {pct_used}% (approaching limit)\")\n",
    "    else:\n",
    "        print(f\"‚ùå Disk usage: {pct_used}% (critical - add nodes or clean up)\")\n",
    "\n",
    "    # Check for disk-based queries\n",
    "    query = \"\"\"\n",
    "        SELECT COUNT(*) as disk_queries\n",
    "        FROM svl_query_summary\n",
    "        WHERE is_diskbased = 't'\n",
    "            AND starttime >= DATEADD(hour, -24, GETDATE());\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.execute(query)\n",
    "    disk_queries = cursor.fetchone()[0]\n",
    "\n",
    "    if disk_queries == 0:\n",
    "        print(\"‚úÖ No disk-based queries (last 24h)\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {disk_queries} disk-based queries detected (last 24h)\")\n",
    "        print(\"   Consider increasing memory or optimizing queries\")\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Health check failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-issues-cell",
   "metadata": {},
   "source": [
    "### Common Issues and Solutions\n",
    "\n",
    "**1. Connection Timeout**\n",
    "```\n",
    "Error: timeout expired\n",
    "```\n",
    "**Solution:**\n",
    "- Check security group allows inbound TCP 5439\n",
    "- Verify VPC routing table has route to internet gateway (if public)\n",
    "- Check network ACLs aren't blocking traffic\n",
    "- Ensure cluster is in \"Available\" state\n",
    "\n",
    "**2. Authentication Failed**\n",
    "```\n",
    "Error: password authentication failed\n",
    "```\n",
    "**Solution:**\n",
    "- Verify username and password are correct\n",
    "- Check user has CONNECT privilege: `GRANT CONNECT ON DATABASE dbname TO username;`\n",
    "- Try resetting password in AWS Console\n",
    "\n",
    "**3. Insufficient Privileges**\n",
    "```\n",
    "Error: permission denied for schema public\n",
    "```\n",
    "**Solution:**\n",
    "```sql\n",
    "GRANT CREATE ON SCHEMA public TO username;\n",
    "GRANT ALL ON ALL TABLES IN SCHEMA public TO username;\n",
    "```\n",
    "\n",
    "**4. Slow Query Performance**\n",
    "**Solution:**\n",
    "- Run VACUUM to reclaim space and re-sort: `VACUUM FULL;`\n",
    "- Run ANALYZE to update statistics: `ANALYZE;`\n",
    "- Check for disk-based queries: `SELECT * FROM svl_query_summary WHERE is_diskbased='t';`\n",
    "- Review distribution and sort keys\n",
    "- Consider adding nodes or using concurrency scaling\n",
    "\n",
    "**5. Disk Space Full**\n",
    "```\n",
    "Error: disk full\n",
    "```\n",
    "**Solution:**\n",
    "- Drop unused tables\n",
    "- Run VACUUM to reclaim deleted row space\n",
    "- Add more nodes (RA3 recommended)\n",
    "- Move historical data to S3 and use Spectrum\n",
    "\n",
    "**6. COPY from S3 Failed**\n",
    "```\n",
    "Error: S3ServiceException\n",
    "```\n",
    "**Solution:**\n",
    "- Verify IAM role is attached to cluster\n",
    "- Check IAM role has s3:GetObject permission\n",
    "- Verify S3 bucket policy allows Redshift access\n",
    "- Check S3 path is correct (case-sensitive)\n",
    "- Review errors: `SELECT * FROM stl_load_errors ORDER BY starttime DESC LIMIT 10;`\n",
    "\n",
    "**7. High Queue Times**\n",
    "**Solution:**\n",
    "- Review WLM configuration\n",
    "- Enable concurrency scaling\n",
    "- Increase query queue concurrency\n",
    "- Add more nodes for additional capacity\n",
    "\n",
    "**8. Query Monitoring Rule Abort**\n",
    "```\n",
    "Error: Query was aborted by a query monitoring rule\n",
    "```\n",
    "**Solution:**\n",
    "- Review QMR settings in parameter group\n",
    "- Optimize query to use less resources\n",
    "- Adjust QMR thresholds if appropriate\n",
    "\n",
    "**Need More Help?**\n",
    "- Amazon Redshift Documentation: https://docs.aws.amazon.com/redshift/\n",
    "- AWS Support: https://console.aws.amazon.com/support/\n",
    "- BenchBox Documentation: https://github.com/joeharris76/benchbox\n",
    "- Query system tables for detailed diagnostics:\n",
    "  - `stl_query` - Query history\n",
    "  - `stl_wlm_query` - WLM queue history\n",
    "  - `svl_query_summary` - Query execution details\n",
    "  - `stv_inflight` - Currently running queries\n",
    "  - `stl_load_errors` - COPY command errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps-cell",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**Explore more features:**\n",
    "1. Test different node types and cluster sizes\n",
    "2. Compare RA3 vs DC2 performance and costs\n",
    "3. Implement automated VACUUM and ANALYZE schedules\n",
    "4. Set up query monitoring rules for runaway queries\n",
    "5. Configure workload management (WLM) for your use case\n",
    "6. Load data from S3 using COPY for production workflows\n",
    "7. Use Spectrum to query S3 data directly\n",
    "8. Monitor with CloudWatch metrics\n",
    "\n",
    "**Related notebooks:**\n",
    "- `snowflake_benchmarking.ipynb` - Compare with Snowflake\n",
    "- `bigquery_benchmarking.ipynb` - Compare with BigQuery  \n",
    "- `platform_comparison.ipynb` - Multi-cloud comparison\n",
    "- `cost_analysis.ipynb` - Cost optimization strategies\n",
    "\n",
    "**Resources:**\n",
    "- BenchBox Documentation: https://github.com/joeharris76/benchbox\n",
    "- Amazon Redshift Best Practices: https://docs.aws.amazon.com/redshift/latest/dg/best-practices.html\n",
    "- TPC Benchmarks: http://www.tpc.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}