{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": [
    "# SQLite Local Benchmarking with BenchBox\n",
    "\n",
    "This notebook demonstrates benchmarking SQLite, the world's most widely deployed database engine.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Running TPC-H benchmarks with SQLite\n",
    "- Optimizing SQLite for analytical workloads\n",
    "- Using indexes and query optimization\n",
    "- Comparing SQLite with DuckDB for analytics\n",
    "- Understanding SQLite's strengths and limitations\n",
    "\n",
    "**Why SQLite?**\n",
    "- **Ubiquitous**: Pre-installed on most systems, used by billions of devices\n",
    "- **Zero-config**: No server, no setup - just a single file\n",
    "- **Portable**: Database is a single file you can copy/move\n",
    "- **Reliable**: Extensively tested, ACID-compliant\n",
    "- **Free**: Public domain, no licensing costs\n",
    "\n",
    "**Important Note:**\n",
    "SQLite is optimized for **transactional workloads** (OLTP), not analytics (OLAP). For analytical workloads, DuckDB is typically 10-100x faster. This notebook helps you understand when to use each.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Python 3.8+ (SQLite included in Python standard library)\n",
    "- Sufficient disk space for test data (~100MB-10GB depending on scale)\n",
    "\n",
    "**Estimated time:** 5-30 minutes (scale factor 0.01-1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-desc",
   "metadata": {},
   "source": [
    "### Install Required Packages\n",
    "\n",
    "Install BenchBox and visualization libraries. SQLite is included with Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q benchbox pandas matplotlib seaborn psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-desc",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "Import BenchBox components and SQLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# BenchBox imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Visualization imports\n",
    "import seaborn as sns\n",
    "\n",
    "from benchbox.core.config import BenchmarkConfig, DatabaseConfig\n",
    "from benchbox.core.results.exporter import ResultExporter\n",
    "from benchbox.core.runner import LifecyclePhases, run_benchmark_lifecycle\n",
    "\n",
    "# Check SQLite version\n",
    "print(f\"‚úÖ SQLite {sqlite3.sqlite_version} (included with Python {sqlite3.version})\")\n",
    "\n",
    "# System monitoring\n",
    "try:\n",
    "    import psutil\n",
    "\n",
    "    print(\"‚úÖ psutil imported for system monitoring\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  psutil not available - install for system monitoring: pip install psutil\")\n",
    "    psutil = None\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"\\nüì¶ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-desc",
   "metadata": {},
   "source": [
    "### Configure SQLite\n",
    "\n",
    "SQLite performance depends heavily on configuration. Here are key settings:\n",
    "\n",
    "**PRAGMAs for Analytics:**\n",
    "```sql\n",
    "PRAGMA journal_mode = WAL;          -- Write-Ahead Logging for better concurrency\n",
    "PRAGMA synchronous = NORMAL;        -- Balance safety and speed\n",
    "PRAGMA cache_size = -64000;         -- 64MB cache (negative = KB)\n",
    "PRAGMA temp_store = MEMORY;         -- Keep temp data in memory\n",
    "PRAGMA mmap_size = 30000000000;     -- 30GB memory-mapped I/O\n",
    "```\n",
    "\n",
    "**Important Notes:**\n",
    "- SQLite is single-threaded (one write at a time)\n",
    "- Best for datasets <1GB; DuckDB recommended for larger\n",
    "- Analytical queries can be slow without proper indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure benchmark settings\n",
    "config = {\n",
    "    \"database_file\": \"./benchmark_runs/sqlite/benchbox.db\",\n",
    "    \"cache_size_mb\": 64,  # Cache size in MB\n",
    "    \"use_wal\": True,  # Write-Ahead Logging\n",
    "    # Scale factors to test (keep small for SQLite)\n",
    "    \"scale_factors\": [0.01, 0.1],  # 10MB, 100MB (1GB can be slow)\n",
    "    # Output directory\n",
    "    \"output_dir\": \"./benchmark_results\",\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "Path(config[\"database_file\"]).parent.mkdir(parents=True, exist_ok=True)\n",
    "Path(config[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get system information\n",
    "if psutil:\n",
    "    total_ram = psutil.virtual_memory().total / (1024**3)  # GB\n",
    "    available_ram = psutil.virtual_memory().available / (1024**3)  # GB\n",
    "    print(\"üíª System Information:\")\n",
    "    print(f\"   Total RAM: {total_ram:.1f} GB\")\n",
    "    print(f\"   Available RAM: {available_ram:.1f} GB\")\n",
    "    print(f\"   SQLite Cache: {config['cache_size_mb']} MB\")\n",
    "else:\n",
    "    print(\"üíª System Information:\")\n",
    "    print(f\"   SQLite Cache: {config['cache_size_mb']} MB\")\n",
    "\n",
    "print(\"\\n‚úÖ Configuration complete\")\n",
    "print(f\"   Database: {config['database_file']}\")\n",
    "print(f\"   WAL mode: {config['use_wal']}\")\n",
    "print(f\"   Output directory: {config['output_dir']}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  SQLite Performance Note:\")\n",
    "print(\"   SQLite is optimized for transactional workloads, not analytics.\")\n",
    "print(\"   Expect slower performance than DuckDB on analytical queries.\")\n",
    "print(\"   For large analytical datasets, consider DuckDB instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-conn-desc",
   "metadata": {},
   "source": [
    "### Test SQLite Connection\n",
    "\n",
    "Verify SQLite is working and apply performance settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-conn-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Connect to SQLite\n",
    "    conn = sqlite3.connect(config[\"database_file\"])\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Apply performance settings\n",
    "    if config[\"use_wal\"]:\n",
    "        cursor.execute(\"PRAGMA journal_mode = WAL;\")\n",
    "    cursor.execute(\"PRAGMA synchronous = NORMAL;\")\n",
    "    cursor.execute(f\"PRAGMA cache_size = -{config['cache_size_mb'] * 1024};\")\n",
    "    cursor.execute(\"PRAGMA temp_store = MEMORY;\")\n",
    "    cursor.execute(\"PRAGMA mmap_size = 30000000000;\")  # 30GB\n",
    "\n",
    "    # Check version\n",
    "    version = cursor.execute(\"SELECT sqlite_version();\").fetchone()[0]\n",
    "    print(\"‚úÖ Connected to SQLite\")\n",
    "    print(f\"   Version: {version}\")\n",
    "\n",
    "    # Check applied settings\n",
    "    journal_mode = cursor.execute(\"PRAGMA journal_mode;\").fetchone()[0]\n",
    "    cache_size = cursor.execute(\"PRAGMA cache_size;\").fetchone()[0]\n",
    "\n",
    "    print(\"\\n‚öôÔ∏è  Current Settings:\")\n",
    "    print(f\"   Journal mode: {journal_mode}\")\n",
    "    print(f\"   Cache size: {abs(cache_size) / 1024:.1f} MB\")\n",
    "\n",
    "    # Check compile options\n",
    "    print(\"\\nüîß Compile Options:\")\n",
    "    options = cursor.execute(\"PRAGMA compile_options;\").fetchall()\n",
    "    key_options = [\"THREADSAFE\", \"ENABLE_FTS\", \"ENABLE_JSON\", \"MAX_LENGTH\"]\n",
    "    for (option,) in options:\n",
    "        for key in key_options:\n",
    "            if key in option:\n",
    "                print(f\"   {option}\")\n",
    "\n",
    "    # Simple test query\n",
    "    result = cursor.execute(\"SELECT 42 as answer, 'SQLite' as database;\").fetchone()\n",
    "    print(f\"\\n‚úÖ Test query successful: {result}\")\n",
    "\n",
    "    conn.close()\n",
    "    print(\"\\n‚úÖ Connection test passed!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quickstart-header",
   "metadata": {},
   "source": [
    "## 2. Quick Start Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quickstart-desc",
   "metadata": {},
   "source": [
    "### Run TPC-H Power Test\n",
    "\n",
    "Execute a TPC-H power test at scale factor 0.01 (10MB). This runs all 22 TPC-H queries sequentially.\n",
    "\n",
    "**What happens:**\n",
    "1. Generate TPC-H data (customer, orders, lineitem, etc.)\n",
    "2. Create tables in SQLite\n",
    "3. Load data from generated files\n",
    "4. Execute 22 queries and measure performance\n",
    "\n",
    "**Expected time:** ~2-5 minutes at SF 0.01 (slower than DuckDB)\n",
    "\n",
    "**Note**: SQLite is single-threaded and not optimized for analytics. Queries will be slower than DuckDB or cloud warehouses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quickstart-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure database connection\n",
    "db_cfg = DatabaseConfig(type=\"sqlite\", name=\"sqlite-local\")\n",
    "platform_cfg = {\n",
    "    \"database\": config[\"database_file\"],\n",
    "    \"pragmas\": {\n",
    "        \"journal_mode\": \"WAL\" if config[\"use_wal\"] else \"DELETE\",\n",
    "        \"synchronous\": \"NORMAL\",\n",
    "        \"cache_size\": -(config[\"cache_size_mb\"] * 1024),\n",
    "        \"temp_store\": \"MEMORY\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Configure TPC-H benchmark\n",
    "bench_cfg = BenchmarkConfig(\n",
    "    name=\"tpch\", display_name=\"TPC-H Power Test\", scale_factor=0.01, test_execution_type=\"power\"\n",
    ")\n",
    "\n",
    "# Track start time\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Run complete lifecycle\n",
    "print(\"üöÄ Starting TPC-H power test on SQLite...\\n\")\n",
    "print(\"‚è±Ô∏è  Note: SQLite is single-threaded and optimized for OLTP.\")\n",
    "print(\"   Analytical queries may take longer than DuckDB.\\n\")\n",
    "\n",
    "results = run_benchmark_lifecycle(\n",
    "    benchmark_config=bench_cfg,\n",
    "    database_config=db_cfg,\n",
    "    system_profile=None,\n",
    "    platform_config=platform_cfg,\n",
    "    phases=LifecyclePhases(generate=True, load=True, execute=True),\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "total_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(\"\\n‚úÖ TPC-H power test completed!\")\n",
    "print(f\"   Benchmark: {results.benchmark_name}\")\n",
    "print(f\"   Total queries: {len(results.query_results)}\")\n",
    "print(f\"   Geometric mean: {results.geometric_mean:.3f}s\")\n",
    "print(f\"   Total execution time: {results.total_execution_time:.2f}s\")\n",
    "print(f\"   Wall clock time: {total_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-desc",
   "metadata": {},
   "source": [
    "### Visualize Results\n",
    "\n",
    "Create a bar chart showing execution time for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results.query_results:\n",
    "    query_names = [qr.query_name for qr in results.query_results]\n",
    "    execution_times = [qr.execution_time for qr in results.query_results]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    bars = ax.bar(query_names, execution_times, color=\"#003B57\", alpha=0.8, edgecolor=\"black\")\n",
    "\n",
    "    # Highlight slowest queries\n",
    "    max_time = max(execution_times)\n",
    "    for i, (bar, time) in enumerate(zip(bars, execution_times)):\n",
    "        if time > max_time * 0.7:  # Top 30% slowest\n",
    "            bar.set_color(\"#0F80AA\")  # SQLite blue accent\n",
    "            # Annotate with time\n",
    "            ax.text(i, time + max_time * 0.02, f\"{time:.2f}s\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "    ax.set_xlabel(\"Query\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Execution Time (seconds)\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_title(\"TPC-H Query Performance on SQLite (SF 0.01)\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nüìä Performance Summary:\")\n",
    "    print(f\"   Fastest query: {query_names[execution_times.index(min(execution_times))]} ({min(execution_times):.3f}s)\")\n",
    "    print(f\"   Slowest query: {query_names[execution_times.index(max(execution_times))]} ({max(execution_times):.3f}s)\")\n",
    "    print(f\"   Median time: {sorted(execution_times)[len(execution_times) // 2]:.3f}s\")\n",
    "\n",
    "    # Calculate queries per second\n",
    "    qps = len(execution_times) / results.total_execution_time\n",
    "    print(f\"   Throughput: {qps:.2f} queries/second\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No query results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resource-desc",
   "metadata": {},
   "source": [
    "### Monitor Resource Usage\n",
    "\n",
    "Check system resource consumption during the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resource-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if psutil:\n",
    "    # Get current resource usage\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    memory = psutil.virtual_memory()\n",
    "    disk = psutil.disk_usage(\".\")\n",
    "\n",
    "    print(\"üíª Resource Usage:\\n\")\n",
    "    print(f\"CPU Usage: {cpu_percent}%\")\n",
    "    print(f\"Memory: {memory.used / (1024**3):.1f} GB / {memory.total / (1024**3):.1f} GB ({memory.percent}%)\")\n",
    "    print(f\"Disk: {disk.used / (1024**3):.1f} GB / {disk.total / (1024**3):.1f} GB ({disk.percent}%)\")\n",
    "\n",
    "    print(\"\\nüí° SQLite Resource Notes:\")\n",
    "    print(\"   - SQLite is single-threaded (won't use all CPU cores)\")\n",
    "    print(\"   - Low memory footprint compared to other databases\")\n",
    "    print(\"   - I/O bound for analytical queries\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Resource monitoring not available (install psutil)\")\n",
    "\n",
    "# Check database file size\n",
    "if Path(config[\"database_file\"]).exists():\n",
    "    db_size = Path(config[\"database_file\"]).stat().st_size / (1024**2)\n",
    "\n",
    "    # Check for WAL files\n",
    "    wal_file = Path(str(config[\"database_file\"]) + \"-wal\")\n",
    "    shm_file = Path(str(config[\"database_file\"]) + \"-shm\")\n",
    "\n",
    "    print(\"\\nüíæ Database Files:\")\n",
    "    print(f\"   Main database: {db_size:.1f} MB\")\n",
    "    if wal_file.exists():\n",
    "        wal_size = wal_file.stat().st_size / (1024**2)\n",
    "        print(f\"   WAL file: {wal_size:.1f} MB\")\n",
    "    if shm_file.exists():\n",
    "        shm_size = shm_file.stat().st_size / (1024**2)\n",
    "        print(f\"   SHM file: {shm_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-desc",
   "metadata": {},
   "source": [
    "### Results Overview\n",
    "\n",
    "Display detailed results including per-query breakdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Detailed Results:\\n\")\n",
    "print(f\"Benchmark: {results.benchmark_name}\")\n",
    "print(f\"Platform: {results.platform}\")\n",
    "print(f\"Scale Factor: {results.scale_factor}\")\n",
    "print(f\"Test Type: {results.test_execution_type}\")\n",
    "print(f\"Timestamp: {results.start_time}\")\n",
    "print(\"\\nExecution Summary:\")\n",
    "print(f\"  Total queries: {len(results.query_results)}\")\n",
    "print(f\"  Successful: {sum(1 for qr in results.query_results if qr.success)}\")\n",
    "print(f\"  Failed: {sum(1 for qr in results.query_results if not qr.success)}\")\n",
    "print(f\"  Geometric mean: {results.geometric_mean:.3f}s\")\n",
    "print(f\"  Total time: {results.total_execution_time:.2f}s\")\n",
    "\n",
    "if results.data_generation_time:\n",
    "    print(f\"\\nData Generation: {results.data_generation_time:.2f}s\")\n",
    "if results.data_loading_time:\n",
    "    print(f\"Data Loading: {results.data_loading_time:.2f}s\")\n",
    "\n",
    "print(\"\\nüìã Query Breakdown:\")\n",
    "for qr in results.query_results[:5]:  # Show first 5\n",
    "    status = \"‚úÖ\" if qr.success else \"‚ùå\"\n",
    "    print(f\"  {status} {qr.query_name}: {qr.execution_time:.3f}s\")\n",
    "if len(results.query_results) > 5:\n",
    "    print(f\"  ... and {len(results.query_results) - 5} more queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-header",
   "metadata": {},
   "source": [
    "## 3. Advanced Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subset-desc",
   "metadata": {},
   "source": [
    "### Query Subset Selection\n",
    "\n",
    "Run specific queries for targeted testing. With SQLite's slower performance, subsets are especially useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subset-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast smoke test: Run 5 representative queries\n",
    "smoke_test_queries = [1, 3, 6, 10, 14]  # Mix of simple and complex\n",
    "\n",
    "subset_cfg = BenchmarkConfig(\n",
    "    name=\"tpch\",\n",
    "    display_name=\"TPC-H Smoke Test\",\n",
    "    scale_factor=0.01,\n",
    "    test_execution_type=\"power\",\n",
    "    query_numbers=smoke_test_queries,\n",
    ")\n",
    "\n",
    "print(f\"üöÄ Running smoke test with queries: {smoke_test_queries}\\n\")\n",
    "subset_results = run_benchmark_lifecycle(\n",
    "    benchmark_config=subset_cfg,\n",
    "    database_config=db_cfg,\n",
    "    system_profile=None,\n",
    "    platform_config=platform_cfg,\n",
    "    phases=LifecyclePhases(generate=False, load=False, execute=True),  # Reuse data\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Smoke test completed: {subset_results.geometric_mean:.3f}s geometric mean\")\n",
    "print(f\"   Queries: {len(subset_results.query_results)}\")\n",
    "print(f\"   Time saved vs full suite: ~{(1 - len(smoke_test_queries) / 22) * 100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indexes-desc",
   "metadata": {},
   "source": [
    "### Index Optimization\n",
    "\n",
    "SQLite benefits greatly from proper indexing for analytical queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indexes-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìö SQLite Index Strategy for TPC-H\\n\")\n",
    "print(\"Recommended indexes for better analytical performance:\\n\")\n",
    "\n",
    "print(\"1. Foreign Key Indexes (for joins):\")\n",
    "print(\"   CREATE INDEX idx_orders_custkey ON orders(o_custkey);\")\n",
    "print(\"   CREATE INDEX idx_lineitem_orderkey ON lineitem(l_orderkey);\")\n",
    "print(\"   CREATE INDEX idx_lineitem_partkey ON lineitem(l_partkey);\")\n",
    "print(\"   CREATE INDEX idx_lineitem_suppkey ON lineitem(l_suppkey);\")\n",
    "\n",
    "print(\"\\n2. Date Range Indexes (for filters):\")\n",
    "print(\"   CREATE INDEX idx_orders_orderdate ON orders(o_orderdate);\")\n",
    "print(\"   CREATE INDEX idx_lineitem_shipdate ON lineitem(l_shipdate);\")\n",
    "\n",
    "print(\"\\n3. Composite Indexes (for complex queries):\")\n",
    "print(\"   CREATE INDEX idx_lineitem_dates ON lineitem(l_shipdate, l_receiptdate);\")\n",
    "print(\"   CREATE INDEX idx_orders_date_status ON orders(o_orderdate, o_orderstatus);\")\n",
    "\n",
    "print(\"\\nüí° Index Trade-offs:\")\n",
    "print(\"   ‚úÖ Significantly speed up SELECT queries\")\n",
    "print(\"   ‚úÖ Essential for WHERE and JOIN clauses\")\n",
    "print(\"   ‚ùå Slow down INSERT operations\")\n",
    "print(\"   ‚ùå Increase database file size\")\n",
    "print(\"   ‚ùå Not useful for full table scans\")\n",
    "\n",
    "print(\"\\nüéØ Best Practices:\")\n",
    "print(\"   - Index columns used in WHERE clauses\")\n",
    "print(\"   - Index foreign keys for join performance\")\n",
    "print(\"   - Use EXPLAIN QUERY PLAN to verify index usage\")\n",
    "print(\"   - Run ANALYZE after creating indexes\")\n",
    "print(\"   - Avoid over-indexing (balance read vs write performance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-desc",
   "metadata": {},
   "source": [
    "### SQLite vs DuckDB Comparison\n",
    "\n",
    "Compare SQLite and DuckDB performance on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öñÔ∏è  SQLite vs DuckDB Comparison\\n\")\n",
    "print(\"Use Case Recommendations:\\n\")\n",
    "\n",
    "print(\"**Choose SQLite when:**\")\n",
    "print(\"‚úÖ Embedded application database\")\n",
    "print(\"‚úÖ Mobile or IoT devices\")\n",
    "print(\"‚úÖ Small datasets (<1GB)\")\n",
    "print(\"‚úÖ Transactional workloads (OLTP)\")\n",
    "print(\"‚úÖ Simple queries\")\n",
    "print(\"‚úÖ Maximum portability (single file)\")\n",
    "print(\"‚úÖ Mature, battle-tested stability\")\n",
    "\n",
    "print(\"\\n**Choose DuckDB when:**\")\n",
    "print(\"‚úÖ Analytical workloads (OLAP)\")\n",
    "print(\"‚úÖ Large datasets (>1GB)\")\n",
    "print(\"‚úÖ Complex aggregations and joins\")\n",
    "print(\"‚úÖ Data science / pandas integration\")\n",
    "print(\"‚úÖ Multi-threaded performance needed\")\n",
    "print(\"‚úÖ Parquet/CSV querying\")\n",
    "print(\"‚úÖ 10-100x faster for analytics\")\n",
    "\n",
    "print(\"\\nüìä Performance Expectations (TPC-H SF 0.01):\")\n",
    "print(f\"SQLite:  ~{results.geometric_mean:.2f}s geometric mean\")\n",
    "print(f\"DuckDB:  ~{results.geometric_mean / 10:.2f}s expected (10x faster)\")\n",
    "\n",
    "print(\"\\nüí° Hybrid Approach:\")\n",
    "print(\"   Use SQLite for application data (OLTP)\")\n",
    "print(\"   Export to Parquet for analysis with DuckDB (OLAP)\")\n",
    "print(\"   Best of both worlds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-desc",
   "metadata": {},
   "source": [
    "### Export Results\n",
    "\n",
    "Export benchmark results to various formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to multiple formats\n",
    "try:\n",
    "    exporter = ResultExporter(results)\n",
    "\n",
    "    output_dir = Path(config[\"output_dir\"])\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Export to JSON\n",
    "    json_path = output_dir / f\"sqlite_tpch_{timestamp}.json\"\n",
    "    exporter.to_json(json_path)\n",
    "    print(f\"‚úÖ Exported JSON: {json_path}\")\n",
    "\n",
    "    # Export to CSV\n",
    "    csv_path = output_dir / f\"sqlite_tpch_{timestamp}.csv\"\n",
    "    exporter.to_csv(csv_path)\n",
    "    print(f\"‚úÖ Exported CSV: {csv_path}\")\n",
    "\n",
    "    # Export to HTML report\n",
    "    html_path = output_dir / f\"sqlite_tpch_{timestamp}.html\"\n",
    "    exporter.to_html(html_path)\n",
    "    print(f\"‚úÖ Exported HTML: {html_path}\")\n",
    "\n",
    "    print(f\"\\nüìÅ All results exported to: {output_dir}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Export failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "platform-header",
   "metadata": {},
   "source": [
    "## 4. Platform-Specific Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pragma-desc",
   "metadata": {},
   "source": [
    "### PRAGMA Commands\n",
    "\n",
    "SQLite's PRAGMA commands control database behavior and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pragma-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(config[\"database_file\"])\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(\"‚öôÔ∏è  Important PRAGMA Commands\\n\")\n",
    "\n",
    "# Get current settings\n",
    "pragmas = {\n",
    "    \"journal_mode\": cursor.execute(\"PRAGMA journal_mode;\").fetchone()[0],\n",
    "    \"synchronous\": cursor.execute(\"PRAGMA synchronous;\").fetchone()[0],\n",
    "    \"cache_size\": cursor.execute(\"PRAGMA cache_size;\").fetchone()[0],\n",
    "    \"temp_store\": cursor.execute(\"PRAGMA temp_store;\").fetchone()[0],\n",
    "    \"mmap_size\": cursor.execute(\"PRAGMA mmap_size;\").fetchone()[0],\n",
    "}\n",
    "\n",
    "print(\"Current Settings:\")\n",
    "for key, value in pragmas.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nüí° Key PRAGMAs for Analytics:\\n\")\n",
    "\n",
    "print(\"1. journal_mode = WAL\")\n",
    "print(\"   Write-Ahead Logging for better concurrency\")\n",
    "print(\"   Allows concurrent readers during writes\")\n",
    "\n",
    "print(\"\\n2. synchronous = NORMAL (or OFF for benchmarks)\")\n",
    "print(\"   Balance between safety and speed\")\n",
    "print(\"   OFF is fastest but risks corruption on crash\")\n",
    "\n",
    "print(\"\\n3. cache_size = -64000 (64MB in KB)\")\n",
    "print(\"   Larger cache = better performance\")\n",
    "print(\"   Negative value = size in KB, positive = pages\")\n",
    "\n",
    "print(\"\\n4. temp_store = MEMORY\")\n",
    "print(\"   Keep temporary tables in RAM\")\n",
    "print(\"   Faster for complex queries with temp results\")\n",
    "\n",
    "print(\"\\n5. mmap_size = 30000000000 (30GB)\")\n",
    "print(\"   Memory-mapped I/O for faster reads\")\n",
    "print(\"   Lets OS cache database pages\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\nüéØ Optimization Strategy:\")\n",
    "print(\"   - Set PRAGMAs at connection time\")\n",
    "print(\"   - Run ANALYZE after loading data\")\n",
    "print(\"   - Use EXPLAIN QUERY PLAN to verify execution\")\n",
    "print(\"   - Monitor with PRAGMA stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explain-desc",
   "metadata": {},
   "source": [
    "### EXPLAIN QUERY PLAN\n",
    "\n",
    "Understand how SQLite executes queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explain-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(config[\"database_file\"])\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(\"üîç Query Execution Plans\\n\")\n",
    "\n",
    "# Example query\n",
    "query = \"\"\"\n",
    "SELECT c_name, SUM(o_totalprice) as total\n",
    "FROM customer\n",
    "JOIN orders ON c_custkey = o_custkey\n",
    "WHERE o_orderdate >= '1995-01-01'\n",
    "GROUP BY c_custkey, c_name\n",
    "ORDER BY total DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Example Query:\")\n",
    "print(query)\n",
    "\n",
    "try:\n",
    "    # Get query plan\n",
    "    plan = cursor.execute(f\"EXPLAIN QUERY PLAN {query}\").fetchall()\n",
    "\n",
    "    print(\"\\nQuery Plan:\")\n",
    "    for row in plan:\n",
    "        # Format: (id, parent, notused, detail)\n",
    "        indent = \"  \" * row[0]\n",
    "        print(f\"{indent}{row[3]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Could not get query plan: {e}\")\n",
    "    print(\"   (This is expected if tables don't exist yet)\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\nüí° Understanding the Plan:\\n\")\n",
    "print(\"- SCAN TABLE: Full table scan (slow, needs index)\")\n",
    "print(\"- SEARCH TABLE USING INDEX: Using an index (fast)\")\n",
    "print(\"- USE TEMP B-TREE FOR: Temporary sorting/grouping\")\n",
    "print(\"- COVERING INDEX: Index contains all needed columns (fastest)\")\n",
    "\n",
    "print(\"\\nüéØ Optimization Tips:\")\n",
    "print(\"   1. Avoid SCAN operations on large tables\")\n",
    "print(\"   2. Create indexes for frequently filtered columns\")\n",
    "print(\"   3. Consider covering indexes for common queries\")\n",
    "print(\"   4. Run ANALYZE to update query planner statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attach-desc",
   "metadata": {},
   "source": [
    "### ATTACH DATABASE\n",
    "\n",
    "SQLite can attach multiple database files for cross-database queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attach-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîó ATTACH DATABASE Feature\\n\")\n",
    "print(\"SQLite can work with multiple database files simultaneously:\\n\")\n",
    "\n",
    "print(\"1. Attach another database:\")\n",
    "print(\"   ATTACH DATABASE 'archive.db' AS archive;\")\n",
    "\n",
    "print(\"\\n2. Query across databases:\")\n",
    "print(\"   SELECT * FROM main.customers\")\n",
    "print(\"   JOIN archive.orders ON customers.id = orders.customer_id;\")\n",
    "\n",
    "print(\"\\n3. Copy data between databases:\")\n",
    "print(\"   INSERT INTO archive.old_orders SELECT * FROM main.orders\")\n",
    "print(\"   WHERE order_date < '2020-01-01';\")\n",
    "\n",
    "print(\"\\n4. Detach when done:\")\n",
    "print(\"   DETACH DATABASE archive;\")\n",
    "\n",
    "print(\"\\nüí° Use Cases:\")\n",
    "print(\"   - Separate hot/cold data (current vs archive)\")\n",
    "print(\"   - Data migration between databases\")\n",
    "print(\"   - Multi-tenant applications\")\n",
    "print(\"   - Backup and restore operations\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Limitations:\")\n",
    "print(\"   - All databases must use same SQLite version\")\n",
    "print(\"   - Transactions can span attached databases\")\n",
    "print(\"   - Performance: Keep total data size reasonable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-header",
   "metadata": {},
   "source": [
    "## 5. Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats-desc",
   "metadata": {},
   "source": [
    "### Statistical Analysis\n",
    "\n",
    "Calculate detailed statistics on query performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results.query_results:\n",
    "    times = [qr.execution_time for qr in results.query_results if qr.success]\n",
    "\n",
    "    if times:\n",
    "        stats = {\n",
    "            \"count\": len(times),\n",
    "            \"mean\": np.mean(times),\n",
    "            \"median\": np.median(times),\n",
    "            \"std\": np.std(times),\n",
    "            \"min\": np.min(times),\n",
    "            \"max\": np.max(times),\n",
    "            \"p95\": np.percentile(times, 95),\n",
    "        }\n",
    "\n",
    "        print(\"üìä Statistical Summary:\\n\")\n",
    "        print(f\"Count:      {stats['count']} queries\")\n",
    "        print(f\"Mean:       {stats['mean']:.3f}s\")\n",
    "        print(f\"Median:     {stats['median']:.3f}s\")\n",
    "        print(f\"Std Dev:    {stats['std']:.3f}s\")\n",
    "        print(f\"Min:        {stats['min']:.3f}s\")\n",
    "        print(f\"Max:        {stats['max']:.3f}s\")\n",
    "        print(f\"P95:        {stats['p95']:.3f}s\")\n",
    "\n",
    "        print(\"\\nüí° SQLite Performance Context:\")\n",
    "        print(\"   SQLite is single-threaded and OLTP-optimized\")\n",
    "        print(\"   These times are expected for analytical workloads\")\n",
    "        print(\"   For faster analytics, consider DuckDB or cloud warehouses\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No successful queries to analyze\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No query results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshooting-header",
   "metadata": {},
   "source": [
    "## 6. Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diag-desc",
   "metadata": {},
   "source": [
    "### Diagnostics Function\n",
    "\n",
    "Comprehensive diagnostic tool for troubleshooting SQLite issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diag-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_sqlite():\n",
    "    \"\"\"Diagnose SQLite setup and configuration\"\"\"\n",
    "    print(\"üîç SQLite Diagnostic\\n\")\n",
    "\n",
    "    # Check 1: SQLite version\n",
    "    print(\"1. Checking SQLite version...\")\n",
    "    print(f\"   ‚úÖ SQLite {sqlite3.sqlite_version}\")\n",
    "    print(f\"   ‚úÖ Python sqlite3 {sqlite3.version}\")\n",
    "\n",
    "    # Check 2: Test connection\n",
    "    print(\"\\n2. Testing connection...\")\n",
    "    try:\n",
    "        conn = sqlite3.connect(\":memory:\")\n",
    "        result = conn.execute(\"SELECT 42 as answer;\").fetchone()\n",
    "        conn.close()\n",
    "        print(f\"   ‚úÖ Connection successful: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Connection failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    # Check 3: Database file\n",
    "    print(\"\\n3. Checking database file...\")\n",
    "    db_path = Path(config[\"database_file\"])\n",
    "    if db_path.exists():\n",
    "        size_mb = db_path.stat().st_size / (1024**2)\n",
    "        print(f\"   ‚úÖ Database exists: {size_mb:.1f} MB\")\n",
    "\n",
    "        # Check if file is locked\n",
    "        try:\n",
    "            test_conn = sqlite3.connect(config[\"database_file\"])\n",
    "            test_conn.close()\n",
    "            print(\"   ‚úÖ Database accessible\")\n",
    "        except sqlite3.OperationalError as e:\n",
    "            print(f\"   ‚ùå Database locked: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"   ‚ÑπÔ∏è  Database will be created: {db_path}\")\n",
    "\n",
    "    # Check 4: Compile options\n",
    "    print(\"\\n4. Checking compile options...\")\n",
    "    conn = sqlite3.connect(\":memory:\")\n",
    "    cursor = conn.cursor()\n",
    "    options = cursor.execute(\"PRAGMA compile_options;\").fetchall()\n",
    "\n",
    "    key_features = [\"THREADSAFE\", \"ENABLE_FTS\", \"ENABLE_JSON\", \"ENABLE_RTREE\"]\n",
    "    for (option,) in options:\n",
    "        for feature in key_features:\n",
    "            if feature in option:\n",
    "                print(f\"   ‚úÖ {option}\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    print(\"\\n‚úÖ All diagnostics passed!\")\n",
    "    print(\"\\nüìö Resources:\")\n",
    "    print(\"   - SQLite Documentation: https://sqlite.org/docs.html\")\n",
    "    print(\"   - SQL As Understood By SQLite: https://sqlite.org/lang.html\")\n",
    "    print(\"   - Performance Tuning: https://sqlite.org/speed.html\")\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run diagnostics\n",
    "diagnose_sqlite()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-issues-desc",
   "metadata": {},
   "source": [
    "### Common Issues and Solutions\n",
    "\n",
    "**1. Database Locked Error**\n",
    "```\n",
    "sqlite3.OperationalError: database is locked\n",
    "```\n",
    "**Solution:**\n",
    "- Close all connections: `conn.close()`\n",
    "- Use WAL mode: `PRAGMA journal_mode=WAL;`\n",
    "- Increase timeout: `conn = sqlite3.connect(db, timeout=30)`\n",
    "- Check for stale lock files (delete .db-journal if safe)\n",
    "\n",
    "**2. Slow Query Performance**\n",
    "**Solution:**\n",
    "- Create indexes on filtered/joined columns\n",
    "- Run ANALYZE to update statistics\n",
    "- Increase cache_size: `PRAGMA cache_size=-64000;`\n",
    "- Use EXPLAIN QUERY PLAN to find bottlenecks\n",
    "- Consider DuckDB for complex analytical queries\n",
    "\n",
    "**3. Disk I/O Error**\n",
    "**Solution:**\n",
    "- Check disk space available\n",
    "- Verify file permissions\n",
    "- Run integrity check: `PRAGMA integrity_check;`\n",
    "- Backup and restore if corruption detected\n",
    "\n",
    "**4. Out of Memory**\n",
    "**Solution:**\n",
    "- Reduce cache_size setting\n",
    "- Process data in smaller batches\n",
    "- Use temp_store=FILE instead of MEMORY\n",
    "- Simplify complex queries\n",
    "\n",
    "**5. Syntax Error (Unsupported SQL)**\n",
    "**Solution:**\n",
    "- SQLite has limited SQL support vs PostgreSQL/MySQL\n",
    "- No RIGHT JOIN (use LEFT JOIN instead)\n",
    "- No FULL OUTER JOIN (union two LEFT JOINs)\n",
    "- Limited window functions in older versions\n",
    "- Check version: `SELECT sqlite_version();`\n",
    "\n",
    "**6. Slow Bulk Insert**\n",
    "**Solution:**\n",
    "```python\n",
    "conn.execute('BEGIN TRANSACTION')\n",
    "# Insert many rows\n",
    "conn.execute('COMMIT')\n",
    "```\n",
    "- Wrap inserts in transaction\n",
    "- Use executemany() for batch inserts\n",
    "- Temporarily disable indexes\n",
    "- Set `PRAGMA synchronous=OFF` (careful!)\n",
    "\n",
    "**7. Database Corruption**\n",
    "**Solution:**\n",
    "- Run: `PRAGMA integrity_check;`\n",
    "- Dump and restore: `.dump | sqlite3 new.db`\n",
    "- Use WAL mode to prevent corruption\n",
    "- Always close connections properly\n",
    "\n",
    "**8. Large Database File Size**\n",
    "**Solution:**\n",
    "- Run: `VACUUM;` to reclaim space\n",
    "- Enable auto_vacuum: `PRAGMA auto_vacuum=FULL;`\n",
    "- Delete unnecessary indexes\n",
    "- Archive old data to separate database\n",
    "\n",
    "**Need More Help?**\n",
    "- SQLite Documentation: https://sqlite.org/docs.html\n",
    "- SQLite Forum: https://sqlite.org/forum/\n",
    "- Stack Overflow: https://stackoverflow.com/questions/tagged/sqlite\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**Try these next:**\n",
    "1. Compare with DuckDB performance (`duckdb_benchmarking.ipynb`)\n",
    "2. Test with your own application data\n",
    "3. Experiment with index strategies\n",
    "4. Optimize PRAGMA settings for your workload\n",
    "5. Consider SQLite for OLTP, DuckDB for OLAP\n",
    "\n",
    "**Resources:**\n",
    "- BenchBox Documentation: https://github.com/joeharris76/benchbox\n",
    "- SQLite Documentation: https://sqlite.org/docs.html\n",
    "- SQLite Performance Tips: https://sqlite.org/speed.html\n",
    "- TPC Benchmarks: http://www.tpc.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
