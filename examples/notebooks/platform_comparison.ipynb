{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BenchBox Platform Comparison\n",
    "\n",
    "This notebook demonstrates how to **compare performance and costs** across multiple database platforms using BenchBox. Whether you're evaluating cloud data warehouses (Databricks, BigQuery, Snowflake, Redshift) or local analytical databases (DuckDB, SQLite), this notebook provides tools to make data-driven platform decisions.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- **Load and normalize** benchmark results from multiple platforms\n",
    "- **Compare performance** using statistical analysis and visualizations\n",
    "- **Analyze cost-effectiveness** across cloud platforms\n",
    "- **Identify platform strengths** for different query types\n",
    "- **Generate recommendations** based on your workload characteristics\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "You should have run benchmarks on at least 2 platforms using the platform-specific notebooks:\n",
    "- `databricks_benchmarking.ipynb`\n",
    "- `bigquery_benchmarking.ipynb`\n",
    "- `snowflake_benchmarking.ipynb`\n",
    "- `redshift_benchmarking.ipynb`\n",
    "- `duckdb_benchmarking.ipynb`\n",
    "- `sqlite_benchmarking.ipynb`\n",
    "\n",
    "## Expected Runtime\n",
    "\n",
    "This notebook performs analysis on existing results, so it runs quickly:\n",
    "- Loading and preparation: **30-60 seconds**\n",
    "- Visualization generation: **1-2 minutes**\n",
    "- Complete notebook: **2-3 minutes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install BenchBox if not already installed\n",
    "# !pip install benchbox\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from benchbox import __version__\n",
    "\n",
    "print(f\"BenchBox version: {__version__}\")\n",
    "print(f\"Environment: {os.environ.get('BENCHBOX_ENV', 'development')}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    # Directories where benchmark results are stored\n",
    "    \"results_dirs\": {\n",
    "        \"databricks\": \"./benchmark_results/databricks\",\n",
    "        \"bigquery\": \"./benchmark_results/bigquery\",\n",
    "        \"snowflake\": \"./benchmark_results/snowflake\",\n",
    "        \"redshift\": \"./benchmark_results/redshift\",\n",
    "        \"duckdb\": \"./benchmark_results/duckdb\",\n",
    "        \"sqlite\": \"./benchmark_results/sqlite\",\n",
    "    },\n",
    "    # Cloud platform pricing (example rates, update with your actual costs)\n",
    "    \"pricing\": {\n",
    "        \"databricks\": {\n",
    "            \"dbu_per_hour\": 8.0,  # DBU consumption rate\n",
    "            \"dbu_cost\": 0.40,  # Cost per DBU\n",
    "            \"compute_cost\": 0.15,  # EC2 cost per hour (example: i3.xlarge)\n",
    "        },\n",
    "        \"bigquery\": {\n",
    "            \"per_tb_processed\": 5.00,  # On-demand pricing\n",
    "            \"storage_per_tb\": 0.02,  # Active storage per TB/month\n",
    "        },\n",
    "        \"snowflake\": {\n",
    "            \"credit_cost\": 2.00,  # Enterprise edition\n",
    "            \"storage_per_tb\": 0.023,  # Per TB/month\n",
    "        },\n",
    "        \"redshift\": {\n",
    "            \"node_cost_per_hour\": 0.25,  # ra3.xlplus on-demand\n",
    "            \"storage_per_tb\": 0.024,  # RA3 managed storage\n",
    "        },\n",
    "        \"duckdb\": {\n",
    "            \"infrastructure_cost\": 0.0,  # Local execution\n",
    "        },\n",
    "        \"sqlite\": {\n",
    "            \"infrastructure_cost\": 0.0,  # Local execution\n",
    "        },\n",
    "    },\n",
    "    # Platform colors for consistent visualization\n",
    "    \"colors\": {\n",
    "        \"databricks\": \"#FF3621\",\n",
    "        \"bigquery\": \"#4285F4\",\n",
    "        \"snowflake\": \"#29B5E8\",\n",
    "        \"redshift\": \"#CC0000\",\n",
    "        \"duckdb\": \"#FFC220\",\n",
    "        \"sqlite\": \"#003B57\",\n",
    "    },\n",
    "    # Output directory\n",
    "    \"output_dir\": \"./comparison_results\",\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(config[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"Platforms to compare: {', '.join(config['results_dirs'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Benchmark Results\n",
    "\n",
    "Load results from all available platforms. This function will gracefully handle missing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_platform_results(platform: str, results_dir: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Load the most recent benchmark results for a platform.\n",
    "\n",
    "    Args:\n",
    "        platform: Platform name (e.g., 'databricks', 'bigquery')\n",
    "        results_dir: Directory containing benchmark results\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing benchmark results, or None if not found\n",
    "    \"\"\"\n",
    "    results_path = Path(results_dir)\n",
    "\n",
    "    if not results_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  No results found for {platform} at {results_dir}\")\n",
    "        return None\n",
    "\n",
    "    # Find most recent results file\n",
    "    json_files = list(results_path.glob(\"*.json\"))\n",
    "    if not json_files:\n",
    "        print(f\"‚ö†Ô∏è  No JSON results found for {platform}\")\n",
    "        return None\n",
    "\n",
    "    # Sort by modification time, get most recent\n",
    "    latest_file = max(json_files, key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "    try:\n",
    "        with open(latest_file) as f:\n",
    "            results = json.load(f)\n",
    "\n",
    "        print(f\"‚úÖ Loaded {platform} results from {latest_file.name}\")\n",
    "        print(f\"   Benchmark: {results.get('benchmark_name', 'unknown')}\")\n",
    "        print(f\"   Scale: {results.get('scale_factor', 'unknown')}\")\n",
    "        print(f\"   Queries: {len(results.get('query_results', []))}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {platform} results: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Load all available platform results\n",
    "platform_results = {}\n",
    "\n",
    "for platform, results_dir in config[\"results_dirs\"].items():\n",
    "    results = load_platform_results(platform, results_dir)\n",
    "    if results:\n",
    "        platform_results[platform] = results\n",
    "\n",
    "print(f\"\\nüìä Loaded results from {len(platform_results)} platforms: {', '.join(platform_results.keys())}\")\n",
    "\n",
    "if len(platform_results) < 2:\n",
    "    print(\"\\n‚ö†Ô∏è  Warning: Need results from at least 2 platforms for meaningful comparison\")\n",
    "    print(\"   Run benchmarks using the platform-specific notebooks first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalize and Align Results\n",
    "\n",
    "Different platforms may have run different queries at different scales. We need to normalize the data for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_results(platform_results: Dict[str, Dict]) -> pd.DataFrame:\n",
    "    \"\"\"Normalize benchmark results into a common DataFrame format.\n",
    "\n",
    "    Args:\n",
    "        platform_results: Dictionary mapping platform names to result dictionaries\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: platform, benchmark, scale_factor, query,\n",
    "                                execution_time_ms, success, error_message\n",
    "    \"\"\"\n",
    "    records = []\n",
    "\n",
    "    for platform, results in platform_results.items():\n",
    "        benchmark = results.get(\"benchmark_name\", \"unknown\")\n",
    "        scale_factor = results.get(\"scale_factor\", 0.0)\n",
    "\n",
    "        for query_result in results.get(\"query_results\", []):\n",
    "            records.append(\n",
    "                {\n",
    "                    \"platform\": platform,\n",
    "                    \"benchmark\": benchmark,\n",
    "                    \"scale_factor\": scale_factor,\n",
    "                    \"query\": query_result.get(\"query_name\", query_result.get(\"query_id\", \"unknown\")),\n",
    "                    \"execution_time_ms\": query_result.get(\"execution_time_ms\", None),\n",
    "                    \"success\": query_result.get(\"success\", False),\n",
    "                    \"error_message\": query_result.get(\"error_message\", None),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    # Convert execution time to seconds for easier reading\n",
    "    df[\"execution_time_s\"] = df[\"execution_time_ms\"] / 1000.0\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Create normalized DataFrame\n",
    "df = normalize_results(platform_results)\n",
    "\n",
    "print(f\"üìä Normalized results: {len(df)} query executions\")\n",
    "print(\"\\nPlatform distribution:\")\n",
    "print(df[\"platform\"].value_counts())\n",
    "print(\"\\nBenchmark distribution:\")\n",
    "print(df[\"benchmark\"].value_counts())\n",
    "print(\"\\nSuccess rate by platform:\")\n",
    "print(df.groupby(\"platform\")[\"success\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common queries across platforms for apples-to-apples comparison\n",
    "def find_common_queries(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Filter to queries that ran successfully on multiple platforms.\"\"\"\n",
    "\n",
    "    # Only consider successful queries\n",
    "    successful = df[df[\"success\"] == True].copy()\n",
    "\n",
    "    # Group by benchmark, scale, and query\n",
    "    query_platforms = successful.groupby([\"benchmark\", \"scale_factor\", \"query\"])[\"platform\"].apply(set).reset_index()\n",
    "\n",
    "    # Find queries that ran on at least 2 platforms\n",
    "    query_platforms[\"platform_count\"] = query_platforms[\"platform\"].apply(len)\n",
    "    common = query_platforms[query_platforms[\"platform_count\"] >= 2]\n",
    "\n",
    "    # Filter original DataFrame to common queries\n",
    "    common_queries_df = successful.merge(\n",
    "        common[[\"benchmark\", \"scale_factor\", \"query\"]], on=[\"benchmark\", \"scale_factor\", \"query\"], how=\"inner\"\n",
    "    )\n",
    "\n",
    "    return common_queries_df\n",
    "\n",
    "\n",
    "df_common = find_common_queries(df)\n",
    "\n",
    "print(f\"üìä Common queries across platforms: {len(df_common)} executions\")\n",
    "print(f\"   Unique queries: {df_common['query'].nunique()}\")\n",
    "print(\"\\nQueries per platform:\")\n",
    "print(df_common.groupby(\"platform\").size())\n",
    "\n",
    "if len(df_common) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  No common queries found. Platforms may have run different benchmarks or scales.\")\n",
    "    print(\"   Continuing with all results for individual platform analysis.\")\n",
    "    df_common = df[df[\"success\"] == True].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Overall Performance Comparison\n",
    "\n",
    "Compare aggregate performance metrics across platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate aggregate statistics per platform\n",
    "platform_stats = (\n",
    "    df_common.groupby(\"platform\")\n",
    "    .agg({\"execution_time_s\": [\"mean\", \"median\", \"std\", \"min\", \"max\", \"sum\"], \"query\": \"count\"})\n",
    "    .round(3)\n",
    ")\n",
    "\n",
    "platform_stats.columns = [\"_\".join(col).strip() for col in platform_stats.columns.values]\n",
    "platform_stats = platform_stats.rename(columns={\"query_count\": \"num_queries\"})\n",
    "\n",
    "# Calculate geometric mean (better for skewed performance data)\n",
    "geo_means = (\n",
    "    df_common.groupby(\"platform\")[\"execution_time_s\"]\n",
    "    .apply(lambda x: np.exp(np.log(x.replace(0, 0.001)).mean()))\n",
    "    .round(3)\n",
    ")\n",
    "platform_stats[\"execution_time_s_geomean\"] = geo_means\n",
    "\n",
    "# Sort by geometric mean\n",
    "platform_stats = platform_stats.sort_values(\"execution_time_s_geomean\")\n",
    "\n",
    "print(\"üìä Platform Performance Summary\\n\")\n",
    "print(platform_stats)\n",
    "\n",
    "print(\"\\nüèÜ Rankings (lower is better):\")\n",
    "for i, platform in enumerate(platform_stats.index, 1):\n",
    "    geo_mean = platform_stats.loc[platform, \"execution_time_s_geomean\"]\n",
    "    print(f\"{i}. {platform.capitalize()}: {geo_mean:.3f}s geometric mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Overall performance comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Geometric mean comparison (most relevant for benchmarks)\n",
    "ax1 = axes[0]\n",
    "platforms = platform_stats.index\n",
    "geo_means = platform_stats[\"execution_time_s_geomean\"].values\n",
    "colors = [config[\"colors\"].get(p, \"#888888\") for p in platforms]\n",
    "\n",
    "bars = ax1.barh(platforms, geo_means, color=colors, alpha=0.8)\n",
    "ax1.set_xlabel(\"Geometric Mean Execution Time (seconds)\", fontsize=11)\n",
    "ax1.set_title(\"Platform Performance Comparison\\n(Lower is Better)\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, geo_means):\n",
    "    ax1.text(\n",
    "        value + max(geo_means) * 0.02, bar.get_y() + bar.get_height() / 2, f\"{value:.3f}s\", va=\"center\", fontsize=10\n",
    "    )\n",
    "\n",
    "# Plot 2: Distribution comparison (box plot)\n",
    "ax2 = axes[1]\n",
    "df_plot = df_common[[\"platform\", \"execution_time_s\"]].copy()\n",
    "\n",
    "# Create box plot\n",
    "platforms_sorted = platform_stats.index.tolist()\n",
    "df_plot[\"platform\"] = pd.Categorical(df_plot[\"platform\"], categories=platforms_sorted, ordered=True)\n",
    "df_plot = df_plot.sort_values(\"platform\")\n",
    "\n",
    "bp = ax2.boxplot(\n",
    "    [df_plot[df_plot[\"platform\"] == p][\"execution_time_s\"].values for p in platforms_sorted],\n",
    "    labels=platforms_sorted,\n",
    "    patch_artist=True,\n",
    "    vert=False,\n",
    ")\n",
    "\n",
    "# Color the boxes\n",
    "for patch, platform in zip(bp[\"boxes\"], platforms_sorted):\n",
    "    patch.set_facecolor(config[\"colors\"].get(platform, \"#888888\"))\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "ax2.set_xlabel(\"Execution Time (seconds)\", fontsize=11)\n",
    "ax2.set_title(\"Query Time Distribution by Platform\", fontsize=12, fontweight=\"bold\")\n",
    "ax2.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config['output_dir']}/platform_performance_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüíæ Saved: {config['output_dir']}/platform_performance_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Query-Level Performance Analysis\n",
    "\n",
    "Compare performance for individual queries across platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find queries that ran on multiple platforms\n",
    "query_comparison = df_common.pivot_table(\n",
    "    index=\"query\", columns=\"platform\", values=\"execution_time_s\", aggfunc=\"mean\"\n",
    ").round(3)\n",
    "\n",
    "# Calculate speedup relative to slowest platform\n",
    "query_comparison[\"fastest\"] = query_comparison.min(axis=1)\n",
    "query_comparison[\"slowest\"] = query_comparison.max(axis=1)\n",
    "query_comparison[\"speedup\"] = (query_comparison[\"slowest\"] / query_comparison[\"fastest\"]).round(2)\n",
    "\n",
    "# Sort by speedup (highest variation)\n",
    "query_comparison = query_comparison.sort_values(\"speedup\", ascending=False)\n",
    "\n",
    "print(\"üìä Per-Query Performance Comparison (execution time in seconds)\\n\")\n",
    "print(query_comparison.head(10))\n",
    "\n",
    "print(\"\\nüèÜ Queries with highest performance variation:\")\n",
    "for query in query_comparison.head(5).index:\n",
    "    speedup = query_comparison.loc[query, \"speedup\"]\n",
    "    fastest = query_comparison.loc[query, \"fastest\"]\n",
    "    slowest = query_comparison.loc[query, \"slowest\"]\n",
    "    print(f\"  {query}: {speedup:.1f}x difference ({fastest:.3f}s - {slowest:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Heatmap of query performance across platforms\n",
    "plt.figure(figsize=(12, max(6, len(query_comparison) * 0.3)))\n",
    "\n",
    "# Select only platform columns for heatmap\n",
    "platform_cols = [col for col in query_comparison.columns if col not in [\"fastest\", \"slowest\", \"speedup\"]]\n",
    "heatmap_data = query_comparison[platform_cols]\n",
    "\n",
    "# Create heatmap (log scale for better visualization if times vary widely)\n",
    "sns.heatmap(\n",
    "    np.log10(heatmap_data + 0.001),  # Log scale, add small value to avoid log(0)\n",
    "    annot=heatmap_data.values,  # Show actual values\n",
    "    fmt=\".3f\",\n",
    "    cmap=\"RdYlGn_r\",  # Red (slow) to Green (fast)\n",
    "    cbar_kws={\"label\": \"log‚ÇÅ‚ÇÄ(seconds)\"},\n",
    "    linewidths=0.5,\n",
    "    linecolor=\"gray\",\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    \"Query Performance Heatmap (seconds)\\nLog Scale: Green=Fast, Red=Slow\", fontsize=14, fontweight=\"bold\", pad=15\n",
    ")\n",
    "plt.xlabel(\"Platform\", fontsize=12)\n",
    "plt.ylabel(\"Query\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config['output_dir']}/query_performance_heatmap.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"üíæ Saved: {config['output_dir']}/query_performance_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Platform Strengths Analysis\n",
    "\n",
    "Identify which platform performs best for different types of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the fastest platform for each query\n",
    "def find_fastest_platform(row):\n",
    "    \"\"\"Determine which platform was fastest for this query.\"\"\"\n",
    "    query_data = df_common[df_common[\"query\"] == row.name]\n",
    "    if len(query_data) == 0:\n",
    "        return None\n",
    "    fastest = query_data.loc[query_data[\"execution_time_s\"].idxmin()]\n",
    "    return fastest[\"platform\"]\n",
    "\n",
    "\n",
    "query_comparison[\"fastest_platform\"] = query_comparison.apply(find_fastest_platform, axis=1)\n",
    "\n",
    "# Count wins per platform\n",
    "platform_wins = query_comparison[\"fastest_platform\"].value_counts()\n",
    "\n",
    "print(\"üèÜ Platform Performance Wins (queries where platform was fastest)\\n\")\n",
    "print(platform_wins)\n",
    "\n",
    "# Calculate win percentage\n",
    "total_queries = len(query_comparison)\n",
    "print(\"\\nüìä Win Percentage:\")\n",
    "for platform, wins in platform_wins.items():\n",
    "    pct = (wins / total_queries) * 100\n",
    "    print(f\"  {platform.capitalize()}: {wins}/{total_queries} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Platform strengths pie chart\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "colors_list = [config[\"colors\"].get(p, \"#888888\") for p in platform_wins.index]\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    platform_wins.values,\n",
    "    labels=[p.capitalize() for p in platform_wins.index],\n",
    "    autopct=\"%1.1f%%\",\n",
    "    colors=colors_list,\n",
    "    startangle=90,\n",
    "    textprops={\"fontsize\": 12},\n",
    ")\n",
    "\n",
    "# Make percentage text bold\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color(\"white\")\n",
    "    autotext.set_fontweight(\"bold\")\n",
    "\n",
    "ax.set_title(\n",
    "    \"Platform Performance Leadership\\n(% of Queries Where Platform Was Fastest)\", fontsize=14, fontweight=\"bold\", pad=20\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config['output_dir']}/platform_strengths.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"üíæ Saved: {config['output_dir']}/platform_strengths.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cost Analysis\n",
    "\n",
    "Compare cost-effectiveness across cloud platforms. **Note**: Update pricing config at the top with your actual costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_platform_cost(platform: str, results: Dict, pricing: Dict) -> Optional[float]:\n",
    "    \"\"\"Calculate estimated cost for a benchmark run.\n",
    "\n",
    "    Args:\n",
    "        platform: Platform name\n",
    "        results: Benchmark results dictionary\n",
    "        pricing: Pricing configuration\n",
    "\n",
    "    Returns:\n",
    "        Estimated cost in USD, or None if not calculable\n",
    "    \"\"\"\n",
    "    if platform not in pricing:\n",
    "        return None\n",
    "\n",
    "    platform_pricing = pricing[platform]\n",
    "\n",
    "    # Calculate total execution time in hours\n",
    "    total_time_ms = sum(\n",
    "        qr.get(\"execution_time_ms\", 0) for qr in results.get(\"query_results\", []) if qr.get(\"success\", False)\n",
    "    )\n",
    "    total_time_hours = total_time_ms / (1000 * 60 * 60)\n",
    "\n",
    "    if platform == \"databricks\":\n",
    "        dbu_consumption = total_time_hours * platform_pricing[\"dbu_per_hour\"]\n",
    "        dbu_cost = dbu_consumption * platform_pricing[\"dbu_cost\"]\n",
    "        compute_cost = total_time_hours * platform_pricing[\"compute_cost\"]\n",
    "        return dbu_cost + compute_cost\n",
    "\n",
    "    elif platform == \"bigquery\":\n",
    "        # Estimate data processed (would need actual bytes from results)\n",
    "        # For now, use scale factor as proxy\n",
    "        scale = results.get(\"scale_factor\", 0.1)\n",
    "        estimated_tb_processed = scale * 0.1  # Rough estimate\n",
    "        return estimated_tb_processed * platform_pricing[\"per_tb_processed\"]\n",
    "\n",
    "    elif platform == \"snowflake\":\n",
    "        # Estimate credits (warehouse size * time)\n",
    "        # Assume X-Small warehouse (1 credit/hour)\n",
    "        credits = total_time_hours * 1.0\n",
    "        return credits * platform_pricing[\"credit_cost\"]\n",
    "\n",
    "    elif platform == \"redshift\":\n",
    "        # Node hours (assume 2-node cluster)\n",
    "        node_hours = total_time_hours * 2\n",
    "        return node_hours * platform_pricing[\"node_cost_per_hour\"]\n",
    "\n",
    "    elif platform in [\"duckdb\", \"sqlite\"]:\n",
    "        return 0.0  # Local execution, no direct cloud cost\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Calculate costs for all platforms\n",
    "platform_costs = {}\n",
    "for platform, results in platform_results.items():\n",
    "    cost = calculate_platform_cost(platform, results, config[\"pricing\"])\n",
    "    if cost is not None:\n",
    "        platform_costs[platform] = cost\n",
    "\n",
    "print(\"üí∞ Estimated Benchmark Costs (USD)\\n\")\n",
    "for platform in sorted(platform_costs, key=platform_costs.get):\n",
    "    cost = platform_costs[platform]\n",
    "    print(f\"  {platform.capitalize()}: ${cost:.4f}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Note: These are estimates based on configuration. Update pricing config with your actual costs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cost per query and cost-performance ratio\n",
    "cost_performance = []\n",
    "\n",
    "for platform in platform_stats.index:\n",
    "    if platform in platform_costs:\n",
    "        cost = platform_costs[platform]\n",
    "        geo_mean_time = platform_stats.loc[platform, \"execution_time_s_geomean\"]\n",
    "        num_queries = platform_stats.loc[platform, \"num_queries\"]\n",
    "\n",
    "        cost_per_query = cost / num_queries if num_queries > 0 else 0\n",
    "        # Cost-performance ratio: lower is better (cost per second of execution)\n",
    "        cost_per_second = cost / (geo_mean_time * num_queries) if geo_mean_time > 0 else 0\n",
    "\n",
    "        cost_performance.append(\n",
    "            {\n",
    "                \"platform\": platform,\n",
    "                \"total_cost\": cost,\n",
    "                \"cost_per_query\": cost_per_query,\n",
    "                \"geo_mean_time_s\": geo_mean_time,\n",
    "                \"cost_per_second\": cost_per_second,\n",
    "            }\n",
    "        )\n",
    "\n",
    "df_cost = pd.DataFrame(cost_performance).sort_values(\"cost_per_query\")\n",
    "\n",
    "print(\"üí∞ Cost-Performance Analysis\\n\")\n",
    "print(df_cost.to_string(index=False))\n",
    "\n",
    "print(\"\\nüèÜ Most Cost-Effective Platforms:\")\n",
    "for i, row in df_cost.head(3).iterrows():\n",
    "    print(f\"  {i + 1}. {row['platform'].capitalize()}: ${row['cost_per_query']:.6f} per query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Cost vs Performance scatter plot\n",
    "if len(df_cost) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "    for _, row in df_cost.iterrows():\n",
    "        platform = row[\"platform\"]\n",
    "        ax.scatter(\n",
    "            row[\"geo_mean_time_s\"],\n",
    "            row[\"cost_per_query\"],\n",
    "            s=300,\n",
    "            color=config[\"colors\"].get(platform, \"#888888\"),\n",
    "            alpha=0.7,\n",
    "            edgecolors=\"black\",\n",
    "            linewidths=2,\n",
    "        )\n",
    "        ax.annotate(\n",
    "            platform.capitalize(),\n",
    "            (row[\"geo_mean_time_s\"], row[\"cost_per_query\"]),\n",
    "            xytext=(10, 10),\n",
    "            textcoords=\"offset points\",\n",
    "            fontsize=11,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Performance (Geometric Mean Time, seconds)\\n‚Üê Faster\", fontsize=12)\n",
    "    ax.set_ylabel(\"Cost per Query (USD)\\n‚Üì Cheaper\", fontsize=12)\n",
    "    ax.set_title(\"Cost vs Performance\\n(Bottom-Left Quadrant = Best)\", fontsize=14, fontweight=\"bold\", pad=15)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add quadrant lines at median\n",
    "    if len(df_cost) >= 2:\n",
    "        median_time = df_cost[\"geo_mean_time_s\"].median()\n",
    "        median_cost = df_cost[\"cost_per_query\"].median()\n",
    "        ax.axvline(median_time, color=\"gray\", linestyle=\"--\", alpha=0.5, linewidth=1)\n",
    "        ax.axhline(median_cost, color=\"gray\", linestyle=\"--\", alpha=0.5, linewidth=1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config['output_dir']}/cost_vs_performance.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"üíæ Saved: {config['output_dir']}/cost_vs_performance.png\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Insufficient cost data for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Statistical Comparison\n",
    "\n",
    "Perform statistical tests to determine if performance differences are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\n",
    "def compare_platforms_statistically(df: pd.DataFrame, platform1: str, platform2: str) -> Dict:\n",
    "    \"\"\"Compare two platforms using statistical tests.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with normalized results\n",
    "        platform1: First platform name\n",
    "        platform2: Second platform name\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with statistical test results\n",
    "    \"\"\"\n",
    "    p1_times = df[df[\"platform\"] == platform1][\"execution_time_s\"].values\n",
    "    p2_times = df[df[\"platform\"] == platform2][\"execution_time_s\"].values\n",
    "\n",
    "    if len(p1_times) == 0 or len(p2_times) == 0:\n",
    "        return None\n",
    "\n",
    "    # Mann-Whitney U test (non-parametric, good for performance data)\n",
    "    statistic, p_value = stats.mannwhitneyu(p1_times, p2_times, alternative=\"two-sided\")\n",
    "\n",
    "    # Calculate effect size (Cohen's d)\n",
    "    mean1, mean2 = np.mean(p1_times), np.mean(p2_times)\n",
    "    std_pooled = np.sqrt((np.var(p1_times) + np.var(p2_times)) / 2)\n",
    "    cohens_d = (mean1 - mean2) / std_pooled if std_pooled > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"platform1\": platform1,\n",
    "        \"platform2\": platform2,\n",
    "        \"p_value\": p_value,\n",
    "        \"significant\": p_value < 0.05,\n",
    "        \"cohens_d\": cohens_d,\n",
    "        \"mean_diff\": mean1 - mean2,\n",
    "        \"mean_diff_pct\": ((mean1 - mean2) / mean2 * 100) if mean2 > 0 else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "# Compare all platform pairs\n",
    "platforms = df_common[\"platform\"].unique()\n",
    "comparisons = []\n",
    "\n",
    "for i, p1 in enumerate(platforms):\n",
    "    for p2 in platforms[i + 1 :]:\n",
    "        result = compare_platforms_statistically(df_common, p1, p2)\n",
    "        if result:\n",
    "            comparisons.append(result)\n",
    "\n",
    "df_comparisons = pd.DataFrame(comparisons).sort_values(\"p_value\")\n",
    "\n",
    "print(\"üìä Statistical Platform Comparisons (Mann-Whitney U Test)\\n\")\n",
    "print(\"Significant differences (p < 0.05):\\n\")\n",
    "\n",
    "for _, row in df_comparisons[df_comparisons[\"significant\"]].iterrows():\n",
    "    faster = row[\"platform2\"] if row[\"mean_diff\"] > 0 else row[\"platform1\"]\n",
    "    slower = row[\"platform1\"] if row[\"mean_diff\"] > 0 else row[\"platform2\"]\n",
    "    diff_pct = abs(row[\"mean_diff_pct\"])\n",
    "\n",
    "    print(f\"  ‚úì {faster.capitalize()} is significantly faster than {slower.capitalize()}\")\n",
    "    print(f\"    Difference: {diff_pct:.1f}% (p={row['p_value']:.4f}, Cohen's d={abs(row['cohens_d']):.2f})\\n\")\n",
    "\n",
    "if not any(df_comparisons[\"significant\"]):\n",
    "    print(\"  No statistically significant differences found (all p ‚â• 0.05)\")\n",
    "    print(\"  This could mean platforms have similar performance, or sample size is too small.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Platform Selection Recommendations\n",
    "\n",
    "Generate data-driven recommendations based on workload characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(platform_stats: pd.DataFrame, platform_costs: Dict, platform_wins: pd.Series) -> List[str]:\n",
    "    \"\"\"Generate platform selection recommendations.\"\"\"\n",
    "\n",
    "    recommendations = []\n",
    "\n",
    "    # 1. Overall fastest\n",
    "    fastest = platform_stats[\"execution_time_s_geomean\"].idxmin()\n",
    "    fastest_time = platform_stats.loc[fastest, \"execution_time_s_geomean\"]\n",
    "    recommendations.append(f\"**Fastest Overall**: {fastest.capitalize()} ({fastest_time:.3f}s geometric mean)\")\n",
    "\n",
    "    # 2. Most consistent\n",
    "    # Use coefficient of variation (CV) = std / mean\n",
    "    platform_stats[\"cv\"] = platform_stats[\"execution_time_s_std\"] / platform_stats[\"execution_time_s_mean\"]\n",
    "    most_consistent = platform_stats[\"cv\"].idxmin()\n",
    "    cv = platform_stats.loc[most_consistent, \"cv\"]\n",
    "    recommendations.append(f\"**Most Consistent**: {most_consistent.capitalize()} (CV={cv:.2f}, lower variance)\")\n",
    "\n",
    "    # 3. Most cost-effective (if cost data available)\n",
    "    if platform_costs:\n",
    "        # Find platform with best cost/performance ratio\n",
    "        cost_perf = {}\n",
    "        for platform in platform_stats.index:\n",
    "            if platform in platform_costs and platform_costs[platform] > 0:\n",
    "                geo_mean = platform_stats.loc[platform, \"execution_time_s_geomean\"]\n",
    "                # Performance score (inverse of time) divided by cost\n",
    "                cost_perf[platform] = (1 / geo_mean) / platform_costs[platform]\n",
    "\n",
    "        if cost_perf:\n",
    "            most_cost_effective = max(cost_perf, key=cost_perf.get)\n",
    "            recommendations.append(\n",
    "                f\"**Most Cost-Effective**: {most_cost_effective.capitalize()} (best performance per dollar)\"\n",
    "            )\n",
    "\n",
    "    # 4. Most versatile (wins most query types)\n",
    "    if len(platform_wins) > 0:\n",
    "        most_versatile = platform_wins.idxmax()\n",
    "        win_pct = (platform_wins[most_versatile] / platform_wins.sum()) * 100\n",
    "        recommendations.append(\n",
    "            f\"**Most Versatile**: {most_versatile.capitalize()} (fastest for {win_pct:.1f}% of queries)\"\n",
    "        )\n",
    "\n",
    "    # 5. Best for small datasets\n",
    "    local_platforms = [p for p in platform_stats.index if p in [\"duckdb\", \"sqlite\"]]\n",
    "    if local_platforms:\n",
    "        fastest_local = min(local_platforms, key=lambda p: platform_stats.loc[p, \"execution_time_s_geomean\"])\n",
    "        recommendations.append(\n",
    "            f\"**Best for Local/Small Data**: {fastest_local.capitalize()} (no cloud costs, good for <1GB)\"\n",
    "        )\n",
    "\n",
    "    # 6. Best for large scale\n",
    "    cloud_platforms = [p for p in platform_stats.index if p not in [\"duckdb\", \"sqlite\"]]\n",
    "    if cloud_platforms:\n",
    "        fastest_cloud = min(cloud_platforms, key=lambda p: platform_stats.loc[p, \"execution_time_s_geomean\"])\n",
    "        recommendations.append(\n",
    "            f\"**Best for Large Scale**: {fastest_cloud.capitalize()} (cloud scalability, fast for large datasets)\"\n",
    "        )\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = generate_recommendations(platform_stats, platform_costs, platform_wins)\n",
    "\n",
    "print(\"üéØ Platform Selection Recommendations\\n\")\n",
    "print(\"Based on benchmark results:\\n\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(\"\\nüí° General Guidance:\")\n",
    "print(\"  ‚Ä¢ **Development/Testing**: Use DuckDB or SQLite for fast iteration\")\n",
    "print(\"  ‚Ä¢ **Production Analytics**: Choose based on your data scale and budget\")\n",
    "print(\"  ‚Ä¢ **Cost-Sensitive**: Consider local platforms or most cost-effective cloud option\")\n",
    "print(\"  ‚Ä¢ **Performance-Critical**: Choose the fastest platform for your specific queries\")\n",
    "print(\"  ‚Ä¢ **Hybrid Approach**: Use local for dev, cloud for production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Comparison Results\n",
    "\n",
    "Save comparison data for future reference and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comprehensive comparison report\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 1. Platform statistics to CSV\n",
    "platform_stats_file = f\"{config['output_dir']}/platform_statistics_{timestamp}.csv\"\n",
    "platform_stats.to_csv(platform_stats_file)\n",
    "print(f\"‚úÖ Exported platform statistics: {platform_stats_file}\")\n",
    "\n",
    "# 2. Query comparison to CSV\n",
    "query_comp_file = f\"{config['output_dir']}/query_comparison_{timestamp}.csv\"\n",
    "query_comparison.to_csv(query_comp_file)\n",
    "print(f\"‚úÖ Exported query comparison: {query_comp_file}\")\n",
    "\n",
    "# 3. Cost analysis to CSV (if available)\n",
    "if len(df_cost) > 0:\n",
    "    cost_file = f\"{config['output_dir']}/cost_analysis_{timestamp}.csv\"\n",
    "    df_cost.to_csv(cost_file, index=False)\n",
    "    print(f\"‚úÖ Exported cost analysis: {cost_file}\")\n",
    "\n",
    "# 4. Complete report to JSON\n",
    "report = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"platforms_compared\": list(platform_results.keys()),\n",
    "    \"total_queries\": len(df_common),\n",
    "    \"common_queries\": df_common[\"query\"].nunique(),\n",
    "    \"platform_stats\": platform_stats.to_dict(),\n",
    "    \"platform_wins\": platform_wins.to_dict() if len(platform_wins) > 0 else {},\n",
    "    \"platform_costs\": platform_costs,\n",
    "    \"recommendations\": recommendations,\n",
    "    \"statistical_comparisons\": df_comparisons.to_dict(\"records\") if len(comparisons) > 0 else [],\n",
    "}\n",
    "\n",
    "report_file = f\"{config['output_dir']}/comparison_report_{timestamp}.json\"\n",
    "with open(report_file, \"w\") as f:\n",
    "    json.dump(report, f, indent=2, default=str)\n",
    "\n",
    "print(f\"‚úÖ Exported comprehensive report: {report_file}\")\n",
    "print(f\"\\nüìÅ All results saved to: {config['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"üìä PLATFORM COMPARISON SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nPlatforms Analyzed: {len(platform_results)}\")\n",
    "print(f\"Queries Compared: {df_common['query'].nunique()}\")\n",
    "print(f\"Total Executions: {len(df_common)}\")\n",
    "\n",
    "print(\"\\nüèÜ Top 3 Performers (by geometric mean):\")\n",
    "for i, platform in enumerate(platform_stats.head(3).index, 1):\n",
    "    geo_mean = platform_stats.loc[platform, \"execution_time_s_geomean\"]\n",
    "    print(f\"  {i}. {platform.capitalize()}: {geo_mean:.3f}s\")\n",
    "\n",
    "if platform_costs:\n",
    "    print(\"\\nüí∞ Most Cost-Effective:\")\n",
    "    cheapest = min(platform_costs, key=platform_costs.get)\n",
    "    print(f\"  {cheapest.capitalize()}: ${platform_costs[cheapest]:.4f} for benchmark run\")\n",
    "\n",
    "print(\"\\nüìà Key Insights:\")\n",
    "if len(platform_wins) > 0:\n",
    "    leader = platform_wins.idxmax()\n",
    "    leader_pct = (platform_wins[leader] / platform_wins.sum()) * 100\n",
    "    print(f\"  ‚Ä¢ {leader.capitalize()} was fastest for {leader_pct:.1f}% of queries\")\n",
    "\n",
    "max_speedup = query_comparison[\"speedup\"].max() if len(query_comparison) > 0 else 0\n",
    "if max_speedup > 1:\n",
    "    print(f\"  ‚Ä¢ Up to {max_speedup:.1f}x performance difference between platforms for same query\")\n",
    "\n",
    "print(f\"\\nüìÅ Results exported to: {config['output_dir']}\")\n",
    "print(\"\\nüîç Next Steps:\")\n",
    "print(\"  1. Review platform-specific notebooks for detailed optimization guidance\")\n",
    "print(\"  2. Run benchmarks at larger scale factors for production-scale testing\")\n",
    "print(\"  3. Test with your actual queries using BenchBox custom query feature\")\n",
    "print(\"  4. Consider hybrid approach: local for dev, cloud for production\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
