{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ac50c0",
   "metadata": {},
   "source": [
    "# BenchBox BigQuery Benchmarking\n",
    "\n",
    "This notebook demonstrates how to use BenchBox to benchmark **Google BigQuery** with serverless analytics.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Authenticate with BigQuery using multiple methods (ADC, service account, secrets)\n",
    "- Run TPC-H, TPC-DS, and ClickBench benchmarks\n",
    "- Monitor query costs and bytes processed\n",
    "- Use partitioned and clustered tables for optimization\n",
    "- Compare slot reservations vs on-demand pricing\n",
    "- Analyze performance with statistical visualizations\n",
    "- Troubleshoot common BigQuery issues\n",
    "\n",
    "**Prerequisites:**\n",
    "- Google Cloud Platform account\n",
    "- BigQuery API enabled\n",
    "- Project with billing enabled\n",
    "- Appropriate IAM permissions (BigQuery Admin or Data Editor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7fa73",
   "metadata": {},
   "source": [
    "## Installation & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fdc00e",
   "metadata": {},
   "source": [
    "### 1.1 Install Required Libraries\n",
    "\n",
    "Install BenchBox and Google Cloud libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bcc431",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install benchbox google-cloud-bigquery google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import_section",
   "metadata": {},
   "source": [
    "### 1.2 Import Libraries\n",
    "\n",
    "Import BenchBox components and visualization libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Google Cloud imports\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# BenchBox imports\n",
    "from benchbox.core.config import BenchmarkConfig, DatabaseConfig\n",
    "from benchbox.core.runner import LifecyclePhases, run_benchmark_lifecycle\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auth_section",
   "metadata": {},
   "source": [
    "### 1.3 Authentication\n",
    "\n",
    "BigQuery supports three authentication methods:\n",
    "\n",
    "**Method 1: Application Default Credentials (ADC)** - Recommended for development\n",
    "```bash\n",
    "gcloud auth application-default login\n",
    "```\n",
    "\n",
    "**Method 2: Service Account Key** - Recommended for production\n",
    "```bash\n",
    "export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/service-account-key.json\"\n",
    "```\n",
    "\n",
    "**Method 3: Direct Service Account** - For programmatic use\n",
    "```python\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '/path/to/service-account-key.json'\n",
    ")\n",
    "```\n",
    "\n",
    "This notebook will try ADC first, then fall back to environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure BigQuery connection\n",
    "try:\n",
    "    # Try environment variables first\n",
    "    BQ_PROJECT = os.environ.get(\"BIGQUERY_PROJECT\")\n",
    "    BQ_LOCATION = os.environ.get(\"BIGQUERY_LOCATION\", \"US\")\n",
    "    BQ_DATASET = os.environ.get(\"BIGQUERY_DATASET\", \"benchbox\")\n",
    "\n",
    "    if not BQ_PROJECT:\n",
    "        print(\"‚ö†Ô∏è  BIGQUERY_PROJECT environment variable not set\")\n",
    "        print(\"\\nüí° Set up authentication:\")\n",
    "        print(\"  Option 1 (ADC): gcloud auth application-default login\")\n",
    "        print(\"  Option 2 (Env): export BIGQUERY_PROJECT='your-project-id'\")\n",
    "        print(\"  Option 3 (SA):  export GOOGLE_APPLICATION_CREDENTIALS='/path/to/key.json'\")\n",
    "\n",
    "        # Try to detect from ADC\n",
    "        try:\n",
    "            client = bigquery.Client()\n",
    "            BQ_PROJECT = client.project\n",
    "            print(f\"\\n‚úÖ Using ADC with project: {BQ_PROJECT}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Could not detect project from ADC: {e}\")\n",
    "            raise ValueError(\"Please set BIGQUERY_PROJECT environment variable\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Using project: {BQ_PROJECT}\")\n",
    "        print(f\"‚úÖ Location: {BQ_LOCATION}\")\n",
    "        print(f\"‚úÖ Dataset: {BQ_DATASET}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Authentication error: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"  1. Run: gcloud auth application-default login\")\n",
    "    print(\"  2. Set BIGQUERY_PROJECT environment variable\")\n",
    "    print(\"  3. Verify BigQuery API is enabled in your project\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connection_test_section",
   "metadata": {},
   "source": [
    "### 1.4 Test Connection\n",
    "\n",
    "Verify connectivity and permissions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connection_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize BigQuery client\n",
    "    client = bigquery.Client(project=BQ_PROJECT, location=BQ_LOCATION)\n",
    "\n",
    "    # Test 1: List datasets\n",
    "    print(\"1Ô∏è‚É£ Testing dataset listing...\")\n",
    "    datasets = list(client.list_datasets())\n",
    "    print(f\"   ‚úÖ Found {len(datasets)} dataset(s) in project {BQ_PROJECT}\")\n",
    "\n",
    "    # Test 2: Check if benchmark dataset exists\n",
    "    print(f\"\\n2Ô∏è‚É£ Checking for dataset '{BQ_DATASET}'...\")\n",
    "    dataset_id = f\"{BQ_PROJECT}.{BQ_DATASET}\"\n",
    "    try:\n",
    "        dataset = client.get_dataset(dataset_id)\n",
    "        print(f\"   ‚úÖ Dataset exists: {dataset_id}\")\n",
    "        print(f\"   Location: {dataset.location}\")\n",
    "        print(f\"   Created: {dataset.created}\")\n",
    "    except Exception:\n",
    "        print(f\"   ‚ö†Ô∏è  Dataset '{BQ_DATASET}' does not exist\")\n",
    "        print(\"   üí° Creating dataset...\")\n",
    "\n",
    "        dataset = bigquery.Dataset(dataset_id)\n",
    "        dataset.location = BQ_LOCATION\n",
    "        dataset = client.create_dataset(dataset, exists_ok=True)\n",
    "        print(f\"   ‚úÖ Created dataset: {dataset_id}\")\n",
    "\n",
    "    # Test 3: Run simple query\n",
    "    print(\"\\n3Ô∏è‚É£ Testing query execution...\")\n",
    "    query = \"SELECT 1 as test\"\n",
    "    query_job = client.query(query)\n",
    "    results = query_job.result()\n",
    "    print(\"   ‚úÖ Query executed successfully\")\n",
    "    print(f\"   Bytes processed: {query_job.total_bytes_processed:,}\")\n",
    "    print(f\"   Bytes billed: {query_job.total_bytes_billed:,}\")\n",
    "\n",
    "    print(\"\\n‚úÖ All connection tests passed!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection test failed: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"  1. Verify BigQuery API is enabled\")\n",
    "    print(\"  2. Check IAM permissions (roles/bigquery.admin or roles/bigquery.dataEditor)\")\n",
    "    print(\"  3. Ensure billing is enabled for the project\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_overview_section",
   "metadata": {},
   "source": [
    "### 1.5 Configuration Overview\n",
    "\n",
    "Summary of your BigQuery configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä BigQuery Configuration Summary\\n\")\n",
    "print(f\"Project:  {BQ_PROJECT}\")\n",
    "print(f\"Location: {BQ_LOCATION}\")\n",
    "print(f\"Dataset:  {BQ_DATASET}\")\n",
    "print(\"\\nPricing Model: On-demand (pay per byte processed)\")\n",
    "print(\"Cost: $5 per TB processed (first 1TB/month free)\")\n",
    "print(\"\\nüí° Tip: Use partitioned/clustered tables to reduce costs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e124d7",
   "metadata": {},
   "source": [
    "## Quick Start Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick_start_intro",
   "metadata": {},
   "source": [
    "### 2.1 Run TPC-H Power Test\n",
    "\n",
    "Run a TPC-H power test at scale factor 0.01 (~10MB data).\n",
    "\n",
    "**What happens:**\n",
    "1. Generate TPC-H data (8 tables: customer, orders, lineitem, etc.)\n",
    "2. Load data into BigQuery tables\n",
    "3. Execute 22 TPC-H queries sequentially\n",
    "4. Collect execution times and cost metrics\n",
    "\n",
    "**Expected time:** 2-3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e27271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure database connection\n",
    "db_cfg = DatabaseConfig(type=\"bigquery\", name=\"bigquery_benchbox\")\n",
    "\n",
    "# BigQuery platform configuration\n",
    "platform_cfg = {\n",
    "    \"project\": BQ_PROJECT,\n",
    "    \"location\": BQ_LOCATION,\n",
    "    \"dataset\": BQ_DATASET,\n",
    "    # Optional: Use specific credentials\n",
    "    # \"credentials_path\": \"/path/to/service-account-key.json\"\n",
    "}\n",
    "\n",
    "# Configure TPC-H benchmark\n",
    "bench_cfg = BenchmarkConfig(\n",
    "    name=\"tpch\",\n",
    "    display_name=\"TPC-H\",\n",
    "    scale_factor=0.01,  # 10MB dataset\n",
    "    test_execution_type=\"power\",\n",
    ")\n",
    "\n",
    "# Run full lifecycle: generate ‚Üí load ‚Üí execute\n",
    "print(\"üöÄ Starting TPC-H power test on BigQuery...\\n\")\n",
    "\n",
    "results = run_benchmark_lifecycle(\n",
    "    benchmark_config=bench_cfg,\n",
    "    database_config=db_cfg,\n",
    "    system_profile=None,\n",
    "    platform_config=platform_cfg,\n",
    "    phases=LifecyclePhases(\n",
    "        generate=True,  # Generate TPC-H data\n",
    "        load=True,  # Load into BigQuery\n",
    "        execute=True,  # Execute 22 queries\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Power test completed on BigQuery\")\n",
    "print(f\"Total queries executed: {len(results.query_results)}\")\n",
    "print(f\"Results saved to: {results.results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz_section",
   "metadata": {},
   "source": [
    "### 2.2 Visualize Results\n",
    "\n",
    "Create a bar chart of query execution times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results.query_results:\n",
    "    # Extract query names and execution times\n",
    "    query_names = [qr.query_name for qr in results.query_results]\n",
    "    execution_times = [qr.execution_time for qr in results.query_results]\n",
    "\n",
    "    # Create bar chart\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    bars = ax.bar(query_names, execution_times, color=\"#4285F4\", alpha=0.8)  # Google Blue\n",
    "\n",
    "    # Highlight slowest queries (top 30%)\n",
    "    max_time = max(execution_times)\n",
    "    for i, (bar, time) in enumerate(zip(bars, execution_times)):\n",
    "        if time > max_time * 0.7:\n",
    "            bar.set_color(\"#EA4335\")  # Google Red for slow queries\n",
    "\n",
    "    ax.set_xlabel(\"Query\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Execution Time (seconds)\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_title(\"TPC-H Query Performance on BigQuery\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"\\nüìä Performance Summary:\")\n",
    "    print(f\"  Total time: {sum(execution_times):.2f}s\")\n",
    "    print(f\"  Average: {sum(execution_times) / len(execution_times):.2f}s\")\n",
    "    print(f\"  Fastest: {min(execution_times):.2f}s ({query_names[execution_times.index(min(execution_times))]})\")\n",
    "    print(f\"  Slowest: {max(execution_times):.2f}s ({query_names[execution_times.index(max(execution_times))]})\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No query results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost_section",
   "metadata": {},
   "source": [
    "### 2.3 Analyze Query Costs\n",
    "\n",
    "BigQuery charges based on bytes processed. Let's analyze the cost of each query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cost_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query INFORMATION_SCHEMA.JOBS to get bytes processed\n",
    "# Note: This requires recent query execution\n",
    "try:\n",
    "    # Get job IDs from recent queries\n",
    "    jobs_query = f\"\"\"\n",
    "    SELECT \n",
    "        job_id,\n",
    "        query,\n",
    "        total_bytes_processed,\n",
    "        total_bytes_billed,\n",
    "        total_slot_ms,\n",
    "        creation_time\n",
    "    FROM `{BQ_PROJECT}.{BQ_LOCATION}.INFORMATION_SCHEMA.JOBS_BY_PROJECT`\n",
    "    WHERE creation_time > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR)\n",
    "        AND job_type = 'QUERY'\n",
    "        AND state = 'DONE'\n",
    "    ORDER BY creation_time DESC\n",
    "    LIMIT 50\n",
    "    \"\"\"\n",
    "\n",
    "    query_job = client.query(jobs_query)\n",
    "    jobs_df = query_job.to_dataframe()\n",
    "\n",
    "    if not jobs_df.empty:\n",
    "        # Calculate costs ($5 per TB on-demand)\n",
    "        jobs_df[\"cost_usd\"] = jobs_df[\"total_bytes_billed\"] / (1024**4) * 5\n",
    "\n",
    "        print(\"üí∞ Recent Query Costs:\\n\")\n",
    "        print(f\"Total bytes processed: {jobs_df['total_bytes_processed'].sum() / (1024**3):.2f} GB\")\n",
    "        print(f\"Total bytes billed: {jobs_df['total_bytes_billed'].sum() / (1024**3):.2f} GB\")\n",
    "        print(f\"Estimated cost: ${jobs_df['cost_usd'].sum():.4f}\")\n",
    "        print(\"\\nüí° Note: First 1 TB per month is free\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No recent jobs found\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not query INFORMATION_SCHEMA.JOBS: {e}\")\n",
    "    print(\"This may require additional permissions (roles/bigquery.resourceViewer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_overview_section",
   "metadata": {},
   "source": [
    "### 2.4 Results Overview\n",
    "\n",
    "View comprehensive results summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results_overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã Benchmark Results Summary\\n\")\n",
    "print(f\"Benchmark: {results.benchmark_name}\")\n",
    "print(f\"Platform: {results.database_config.type}\")\n",
    "print(f\"Scale Factor: {results.benchmark_config.scale_factor}\")\n",
    "print(f\"Test Type: {results.benchmark_config.test_execution_type}\")\n",
    "print(\"\\nExecution:\")\n",
    "print(f\"  Start: {results.execution_metadata.start_time}\")\n",
    "print(f\"  End: {results.execution_metadata.end_time}\")\n",
    "print(f\"  Duration: {results.execution_metadata.total_duration:.2f}s\")\n",
    "print(\"\\nQueries:\")\n",
    "print(f\"  Total: {len(results.query_results)}\")\n",
    "print(f\"  Successful: {sum(1 for qr in results.query_results if qr.success)}\")\n",
    "print(f\"  Failed: {sum(1 for qr in results.query_results if not qr.success)}\")\n",
    "print(\"\\nResults Location:\")\n",
    "print(f\"  {results.results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851e27b",
   "metadata": {},
   "source": [
    "## Advanced Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tpcds_section",
   "metadata": {},
   "source": [
    "### 3.1 TPC-DS Benchmark\n",
    "\n",
    "Run TPC-DS (99 queries, more complex than TPC-H):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tpcds_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPC-DS configuration\n",
    "tpcds_cfg = BenchmarkConfig(\n",
    "    name=\"tpcds\",\n",
    "    display_name=\"TPC-DS\",\n",
    "    scale_factor=0.01,  # 10MB dataset\n",
    "    test_execution_type=\"power\",\n",
    ")\n",
    "\n",
    "print(\"üöÄ Running TPC-DS benchmark...\")\n",
    "print(\"‚ö†Ô∏è  Note: TPC-DS has 99 queries and will take longer\\n\")\n",
    "\n",
    "tpcds_results = run_benchmark_lifecycle(\n",
    "    benchmark_config=tpcds_cfg,\n",
    "    database_config=db_cfg,\n",
    "    system_profile=None,\n",
    "    platform_config=platform_cfg,\n",
    "    phases=LifecyclePhases(generate=True, load=True, execute=True),\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ TPC-DS completed: {len(tpcds_results.query_results)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scale_comparison_section",
   "metadata": {},
   "source": [
    "### 3.2 Scale Factor Comparison\n",
    "\n",
    "Compare performance across different data sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scale_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple scale factors\n",
    "scale_factors = [0.01, 0.1, 1.0]  # 10MB, 100MB, 1GB\n",
    "scale_results = {}\n",
    "\n",
    "for sf in scale_factors:\n",
    "    print(f\"\\nüìä Testing scale factor {sf} ({sf * 1000}MB)...\")\n",
    "\n",
    "    sf_cfg = BenchmarkConfig(name=\"tpch\", display_name=f\"TPC-H SF{sf}\", scale_factor=sf, test_execution_type=\"power\")\n",
    "\n",
    "    sf_results = run_benchmark_lifecycle(\n",
    "        benchmark_config=sf_cfg,\n",
    "        database_config=db_cfg,\n",
    "        system_profile=None,\n",
    "        platform_config=platform_cfg,\n",
    "        phases=LifecyclePhases(generate=True, load=True, execute=True),\n",
    "    )\n",
    "\n",
    "    scale_results[sf] = sf_results\n",
    "    avg_time = sum(qr.execution_time for qr in sf_results.query_results) / len(sf_results.query_results)\n",
    "    print(f\"  Average query time: {avg_time:.2f}s\")\n",
    "\n",
    "print(\"\\n‚úÖ Scale comparison complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query_subset_section",
   "metadata": {},
   "source": [
    "### 3.3 Query Subset Selection\n",
    "\n",
    "Run only specific queries for faster iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_subset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only queries 1, 6, and 14 (fast queries for CI/CD)\n",
    "subset_cfg = BenchmarkConfig(\n",
    "    name=\"tpch\",\n",
    "    display_name=\"TPC-H Subset\",\n",
    "    scale_factor=0.01,\n",
    "    test_execution_type=\"power\",\n",
    "    query_filter=[1, 6, 14],  # Only these queries\n",
    ")\n",
    "\n",
    "print(\"üöÄ Running query subset (1, 6, 14)...\\n\")\n",
    "\n",
    "subset_results = run_benchmark_lifecycle(\n",
    "    benchmark_config=subset_cfg,\n",
    "    database_config=db_cfg,\n",
    "    system_profile=None,\n",
    "    platform_config=platform_cfg,\n",
    "    phases=LifecyclePhases(generate=False, load=False, execute=True),  # Reuse data\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Subset complete: {len(subset_results.query_results)} queries\")\n",
    "print(\"\\nüí° Use case: Fast regression testing in CI/CD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partitioned_section",
   "metadata": {},
   "source": [
    "### 3.4 Partitioned Tables\n",
    "\n",
    "Use date/timestamp partitioning to reduce costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partitioned_tables",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure platform with partitioning\n",
    "partitioned_cfg = {\n",
    "    \"project\": BQ_PROJECT,\n",
    "    \"location\": BQ_LOCATION,\n",
    "    \"dataset\": BQ_DATASET,\n",
    "    \"table_options\": {\n",
    "        \"orders\": {\"partition_field\": \"o_orderdate\", \"partition_type\": \"DAY\"},\n",
    "        \"lineitem\": {\"partition_field\": \"l_shipdate\", \"partition_type\": \"MONTH\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"üöÄ Running with partitioned tables...\")\n",
    "print(\"  - orders: partitioned by o_orderdate (daily)\")\n",
    "print(\"  - lineitem: partitioned by l_shipdate (monthly)\\n\")\n",
    "\n",
    "part_cfg = BenchmarkConfig(\n",
    "    name=\"tpch\",\n",
    "    display_name=\"TPC-H Partitioned\",\n",
    "    scale_factor=1.0,  # Larger dataset to see benefits\n",
    "    test_execution_type=\"power\",\n",
    ")\n",
    "\n",
    "part_results = run_benchmark_lifecycle(\n",
    "    benchmark_config=part_cfg,\n",
    "    database_config=db_cfg,\n",
    "    system_profile=None,\n",
    "    platform_config=partitioned_cfg,\n",
    "    phases=LifecyclePhases(generate=True, load=True, execute=True),\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Partitioned benchmark complete\")\n",
    "print(\"\\nüí° Benefits:\")\n",
    "print(\"  - Reduced bytes scanned for date-filtered queries\")\n",
    "print(\"  - Lower costs (pay only for partitions accessed)\")\n",
    "print(\"  - Faster query execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clustered_section",
   "metadata": {},
   "source": [
    "### 3.5 Clustered Tables\n",
    "\n",
    "Use clustering to optimize filtering and aggregation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clustered_tables",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure platform with clustering\n",
    "clustered_cfg = {\n",
    "    \"project\": BQ_PROJECT,\n",
    "    \"location\": BQ_LOCATION,\n",
    "    \"dataset\": BQ_DATASET,\n",
    "    \"table_options\": {\n",
    "        \"lineitem\": {\"cluster_fields\": [\"l_shipdate\", \"l_returnflag\", \"l_linestatus\"]},\n",
    "        \"orders\": {\"cluster_fields\": [\"o_orderdate\", \"o_orderstatus\"]},\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"üöÄ Running with clustered tables...\")\n",
    "print(\"  - lineitem: clustered by (l_shipdate, l_returnflag, l_linestatus)\")\n",
    "print(\"  - orders: clustered by (o_orderdate, o_orderstatus)\\n\")\n",
    "\n",
    "cluster_cfg = BenchmarkConfig(\n",
    "    name=\"tpch\", display_name=\"TPC-H Clustered\", scale_factor=1.0, test_execution_type=\"power\"\n",
    ")\n",
    "\n",
    "cluster_results = run_benchmark_lifecycle(\n",
    "    benchmark_config=cluster_cfg,\n",
    "    database_config=db_cfg,\n",
    "    system_profile=None,\n",
    "    platform_config=clustered_cfg,\n",
    "    phases=LifecyclePhases(generate=True, load=True, execute=True),\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Clustered benchmark complete\")\n",
    "print(\"\\nüí° Benefits:\")\n",
    "print(\"  - Faster filtering on clustered columns\")\n",
    "print(\"  - Reduced bytes scanned\")\n",
    "print(\"  - Optimal for GROUP BY operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "slot_reservation_section",
   "metadata": {},
   "source": [
    "### 3.6 Slot Reservation vs On-Demand\n",
    "\n",
    "Compare on-demand pricing with flat-rate (slot reservation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "slot_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This requires slot reservations to be configured in your project\n",
    "print(\"üí° Pricing Comparison:\\n\")\n",
    "print(\"On-Demand:\")\n",
    "print(\"  - Pay per byte processed ($5/TB)\")\n",
    "print(\"  - No commitment\")\n",
    "print(\"  - Best for sporadic workloads\")\n",
    "print(\"  - First 1 TB/month free\")\n",
    "print(\"\\nFlat-Rate (Slot Reservations):\")\n",
    "print(\"  - Pay per slot hour (~$40/hour for 100 slots)\")\n",
    "print(\"  - Monthly/annual commitment discounts\")\n",
    "print(\"  - Best for steady workloads (>$50k/month)\")\n",
    "print(\"  - Predictable costs\")\n",
    "print(\"\\nAutoscaling (Flex Slots):\")\n",
    "print(\"  - Pay per slot second ($0.04/slot/hour)\")\n",
    "print(\"  - No commitment (60-second minimum)\")\n",
    "print(\"  - Best for burst workloads\")\n",
    "print(\"  - Scale 0-2000 slots automatically\")\n",
    "\n",
    "# To use slot reservations, configure:\n",
    "# slot_cfg = {\n",
    "#     \"project\": BQ_PROJECT,\n",
    "#     \"location\": BQ_LOCATION,\n",
    "#     \"dataset\": BQ_DATASET,\n",
    "#     \"reservation_id\": \"your-reservation-name\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "throughput_section",
   "metadata": {},
   "source": [
    "### 3.7 Throughput Testing\n",
    "\n",
    "Run concurrent queries to test throughput:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "throughput_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throughput test configuration\n",
    "throughput_cfg = BenchmarkConfig(\n",
    "    name=\"tpch\",\n",
    "    display_name=\"TPC-H Throughput\",\n",
    "    scale_factor=0.1,\n",
    "    test_execution_type=\"throughput\",\n",
    "    throughput_streams=4,  # 4 concurrent streams\n",
    ")\n",
    "\n",
    "print(\"üöÄ Running throughput test (4 concurrent streams)...\\n\")\n",
    "\n",
    "throughput_results = run_benchmark_lifecycle(\n",
    "    benchmark_config=throughput_cfg,\n",
    "    database_config=db_cfg,\n",
    "    system_profile=None,\n",
    "    platform_config=platform_cfg,\n",
    "    phases=LifecyclePhases(generate=False, load=False, execute=True),\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Throughput test complete: {len(throughput_results.query_results)} queries\")\n",
    "print(\"\\nüí° BigQuery handles concurrent queries automatically with slot allocation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "result_comparison_section",
   "metadata": {},
   "source": [
    "### 3.8 Result Comparison\n",
    "\n",
    "Compare results across different configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "result_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare results\n",
    "if \"results\" in locals() and \"tpcds_results\" in locals():\n",
    "    tpch_avg = sum(qr.execution_time for qr in results.query_results) / len(results.query_results)\n",
    "    tpcds_avg = sum(qr.execution_time for qr in tpcds_results.query_results) / len(tpcds_results.query_results)\n",
    "\n",
    "    # Create comparison visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    benchmarks = [\"TPC-H\\n(22 queries)\", \"TPC-DS\\n(99 queries)\"]\n",
    "    avg_times = [tpch_avg, tpcds_avg]\n",
    "    total_times = [\n",
    "        sum(qr.execution_time for qr in results.query_results),\n",
    "        sum(qr.execution_time for qr in tpcds_results.query_results),\n",
    "    ]\n",
    "\n",
    "    x = range(len(benchmarks))\n",
    "    width = 0.35\n",
    "\n",
    "    ax.bar([i - width / 2 for i in x], avg_times, width, label=\"Avg Time/Query\", color=\"#4285F4\")\n",
    "    ax.bar([i + width / 2 for i in x], total_times, width, label=\"Total Time\", color=\"#EA4335\")\n",
    "\n",
    "    ax.set_ylabel(\"Time (seconds)\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_title(\"Benchmark Comparison on BigQuery\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(benchmarks)\n",
    "    ax.legend()\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nüìä Comparison Summary:\")\n",
    "    print(f\"  TPC-H: {tpch_avg:.2f}s avg, {total_times[0]:.2f}s total\")\n",
    "    print(f\"  TPC-DS: {tpcds_avg:.2f}s avg, {total_times[1]:.2f}s total\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Run both TPC-H and TPC-DS benchmarks first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export_section",
   "metadata": {},
   "source": [
    "### 3.9 Export Results\n",
    "\n",
    "Export results in multiple formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchbox.core.results.exporter import ResultExporter\n",
    "\n",
    "# Export to JSON (default)\n",
    "exporter = ResultExporter(results)\n",
    "json_path = exporter.export(format=\"json\")\n",
    "print(f\"‚úÖ Exported to JSON: {json_path}\")\n",
    "\n",
    "# Export to CSV\n",
    "csv_path = exporter.export(format=\"csv\")\n",
    "print(f\"‚úÖ Exported to CSV: {csv_path}\")\n",
    "\n",
    "# Export to HTML\n",
    "html_path = exporter.export(format=\"html\")\n",
    "print(f\"‚úÖ Exported to HTML: {html_path}\")\n",
    "\n",
    "print(\"\\nüí° Use these exports for:\")\n",
    "print(\"  - JSON: API integration, programmatic analysis\")\n",
    "print(\"  - CSV: Excel, data science tools, dashboards\")\n",
    "print(\"  - HTML: Shareable reports, documentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost_optimization_section",
   "metadata": {},
   "source": [
    "### 3.10 Cost Optimization Strategies\n",
    "\n",
    "Techniques to reduce BigQuery costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cost_optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí∞ BigQuery Cost Optimization Strategies\\n\")\n",
    "print(\"1. Use Partitioned Tables:\")\n",
    "print(\"   - Partition by date/timestamp columns\")\n",
    "print(\"   - Scan only relevant partitions (prune others)\")\n",
    "print(\"   - Example: WHERE date BETWEEN '2023-01-01' AND '2023-01-31'\")\n",
    "print(\"   - Savings: 50-95% on filtered queries\\n\")\n",
    "\n",
    "print(\"2. Use Clustered Tables:\")\n",
    "print(\"   - Cluster by frequently filtered columns\")\n",
    "print(\"   - Co-locate related data\")\n",
    "print(\"   - Up to 4 clustering columns\")\n",
    "print(\"   - Savings: 20-50% on filtered/grouped queries\\n\")\n",
    "\n",
    "print(\"3. Materialized Views:\")\n",
    "print(\"   - Pre-compute expensive aggregations\")\n",
    "print(\"   - Auto-refresh on base table changes\")\n",
    "print(\"   - Savings: 90%+ on repeated aggregations\\n\")\n",
    "\n",
    "print(\"4. BI Engine:\")\n",
    "print(\"   - In-memory analysis engine\")\n",
    "print(\"   - Cache frequently accessed data\")\n",
    "print(\"   - Best for dashboards and repeated queries\")\n",
    "print(\"   - Cost: $0.06/GB/hour + $0.75/TB scan\\n\")\n",
    "\n",
    "print(\"5. Query Optimization:\")\n",
    "print(\"   - SELECT only needed columns (not SELECT *)\")\n",
    "print(\"   - Use WHERE clauses before JOINs\")\n",
    "print(\"   - Use APPROX_COUNT_DISTINCT instead of COUNT(DISTINCT)\")\n",
    "print(\"   - Avoid CROSS JOINs on large tables\")\n",
    "print(\"   - Use table preview (LIMIT) for exploration\\n\")\n",
    "\n",
    "print(\"6. Slot Management:\")\n",
    "print(\"   - On-demand: Good for <$50k/month spend\")\n",
    "print(\"   - Flat-rate: Good for >$50k/month (break-even)\")\n",
    "print(\"   - Flex slots: Good for burst workloads\")\n",
    "print(\"   - Autoscaling: Best of both worlds\\n\")\n",
    "\n",
    "print(\"7. Storage Optimization:\")\n",
    "print(\"   - Long-term storage: 50% discount after 90 days\")\n",
    "print(\"   - Delete unused tables/partitions\")\n",
    "print(\"   - Use table expiration settings\")\n",
    "print(\"   - Archive to Cloud Storage if rarely accessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a28c7e",
   "metadata": {},
   "source": [
    "## Platform-Specific Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "information_schema_section",
   "metadata": {},
   "source": [
    "### 4.1 Query Cost Analysis with INFORMATION_SCHEMA\n",
    "\n",
    "Analyze costs using BigQuery's INFORMATION_SCHEMA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "information_schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query detailed job information\n",
    "try:\n",
    "    cost_query = f\"\"\"\n",
    "    SELECT \n",
    "        user_email,\n",
    "        job_id,\n",
    "        query,\n",
    "        total_bytes_processed,\n",
    "        total_bytes_billed,\n",
    "        ROUND(total_bytes_billed / POW(1024, 4) * 5, 4) as cost_usd,\n",
    "        total_slot_ms,\n",
    "        TIMESTAMP_DIFF(end_time, start_time, MILLISECOND) as duration_ms,\n",
    "        cache_hit,\n",
    "        creation_time\n",
    "    FROM `{BQ_PROJECT}.region-{BQ_LOCATION}.INFORMATION_SCHEMA.JOBS_BY_PROJECT`\n",
    "    WHERE creation_time > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR)\n",
    "        AND job_type = 'QUERY'\n",
    "        AND state = 'DONE'\n",
    "    ORDER BY total_bytes_billed DESC\n",
    "    LIMIT 20\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üìä Analyzing recent query costs...\\n\")\n",
    "\n",
    "    query_job = client.query(cost_query)\n",
    "    cost_df = query_job.to_dataframe()\n",
    "\n",
    "    if not cost_df.empty:\n",
    "        # Display top costly queries\n",
    "        print(\"üí∞ Most Expensive Queries:\\n\")\n",
    "        for idx, row in cost_df.head(5).iterrows():\n",
    "            print(f\"{idx + 1}. Cost: ${row['cost_usd']:.4f}\")\n",
    "            print(f\"   Bytes processed: {row['total_bytes_processed'] / (1024**3):.2f} GB\")\n",
    "            print(f\"   Duration: {row['duration_ms'] / 1000:.2f}s\")\n",
    "            print(f\"   Cache hit: {row['cache_hit']}\")\n",
    "            print(f\"   Query preview: {row['query'][:80]}...\\n\")\n",
    "\n",
    "        # Summary statistics\n",
    "        total_cost = cost_df[\"cost_usd\"].sum()\n",
    "        total_gb = cost_df[\"total_bytes_processed\"].sum() / (1024**3)\n",
    "        cache_hit_rate = (cost_df[\"cache_hit\"].sum() / len(cost_df)) * 100\n",
    "\n",
    "        print(\"\\nüìà Summary Statistics:\")\n",
    "        print(f\"  Total queries: {len(cost_df)}\")\n",
    "        print(f\"  Total cost: ${total_cost:.4f}\")\n",
    "        print(f\"  Total data processed: {total_gb:.2f} GB\")\n",
    "        print(f\"  Cache hit rate: {cache_hit_rate:.1f}%\")\n",
    "        print(f\"  Average cost/query: ${total_cost / len(cost_df):.4f}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No recent queries found\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error querying INFORMATION_SCHEMA: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"  1. Verify roles/bigquery.resourceViewer permission\")\n",
    "    print(\"  2. Check that recent queries exist (past hour)\")\n",
    "    print(\"  3. Verify region matches your BigQuery location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "slot_monitoring_section",
   "metadata": {},
   "source": [
    "### 4.2 Slot Usage Monitoring\n",
    "\n",
    "Monitor slot allocation and usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "slot_monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query slot usage from INFORMATION_SCHEMA\n",
    "try:\n",
    "    slot_query = f\"\"\"\n",
    "    SELECT \n",
    "        job_id,\n",
    "        total_slot_ms,\n",
    "        TIMESTAMP_DIFF(end_time, start_time, MILLISECOND) as duration_ms,\n",
    "        ROUND(total_slot_ms / TIMESTAMP_DIFF(end_time, start_time, MILLISECOND), 2) as avg_slots\n",
    "    FROM `{BQ_PROJECT}.region-{BQ_LOCATION}.INFORMATION_SCHEMA.JOBS_BY_PROJECT`\n",
    "    WHERE creation_time > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR)\n",
    "        AND job_type = 'QUERY'\n",
    "        AND state = 'DONE'\n",
    "        AND total_slot_ms > 0\n",
    "    ORDER BY total_slot_ms DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üìä Analyzing slot usage...\\n\")\n",
    "\n",
    "    query_job = client.query(slot_query)\n",
    "    slot_df = query_job.to_dataframe()\n",
    "\n",
    "    if not slot_df.empty:\n",
    "        print(\"üîß Top Slot Consumers:\\n\")\n",
    "        for idx, row in slot_df.head(5).iterrows():\n",
    "            print(f\"{idx + 1}. Job: {row['job_id'][-20:]}\")\n",
    "            print(f\"   Total slot-ms: {row['total_slot_ms']:,.0f}\")\n",
    "            print(f\"   Duration: {row['duration_ms'] / 1000:.2f}s\")\n",
    "            print(f\"   Avg slots: {row['avg_slots']:.2f}\\n\")\n",
    "\n",
    "        # Summary\n",
    "        total_slot_ms = slot_df[\"total_slot_ms\"].sum()\n",
    "        avg_slots = slot_df[\"avg_slots\"].mean()\n",
    "\n",
    "        print(\"üìà Slot Usage Summary:\")\n",
    "        print(f\"  Total slot-ms: {total_slot_ms:,.0f}\")\n",
    "        print(f\"  Average slots/query: {avg_slots:.2f}\")\n",
    "        print(f\"  Peak slots: {slot_df['avg_slots'].max():.2f}\")\n",
    "\n",
    "        print(\"\\nüí° Notes:\")\n",
    "        print(\"  - On-demand: 2000 slots default\")\n",
    "        print(\"  - Flat-rate: Based on your reservation\")\n",
    "        print(\"  - Slots allocated dynamically based on query complexity\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No slot usage data found\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error querying slot usage: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bi_engine_section",
   "metadata": {},
   "source": [
    "### 4.3 BI Engine Acceleration\n",
    "\n",
    "Enable and monitor BI Engine for in-memory acceleration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bi_engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö° BI Engine Overview\\n\")\n",
    "print(\"What is BI Engine?\")\n",
    "print(\"  - In-memory analysis engine\")\n",
    "print(\"  - Accelerates SELECT queries\")\n",
    "print(\"  - Automatic cache management\")\n",
    "print(\"  - Best for repeated queries and dashboards\\n\")\n",
    "\n",
    "print(\"Pricing:\")\n",
    "print(\"  - $0.06 per GB per hour (reserved capacity)\")\n",
    "print(\"  - $0.75 per TB scanned (on-demand)\")\n",
    "print(\"  - Minimum: 1 GB reservation\\n\")\n",
    "\n",
    "print(\"To Enable BI Engine:\")\n",
    "print(\"  1. Go to BigQuery console\")\n",
    "print(\"  2. Click 'BI Engine' in left nav\")\n",
    "print(\"  3. Click 'Create Reservation'\")\n",
    "print(\"  4. Choose capacity (1-100 GB)\")\n",
    "print(\"  5. Select location (must match dataset)\\n\")\n",
    "\n",
    "print(\"To Use BI Engine in BenchBox:\")\n",
    "print(\"  platform_cfg = {\")\n",
    "print('      \"project\": BQ_PROJECT,')\n",
    "print('      \"location\": BQ_LOCATION,')\n",
    "print('      \"dataset\": BQ_DATASET,')\n",
    "print('      \"use_bi_engine\": True  # Enable BI Engine')\n",
    "print(\"  }\\n\")\n",
    "\n",
    "print(\"Performance Gains:\")\n",
    "print(\"  - 3-10x faster for cached queries\")\n",
    "print(\"  - Sub-second response times\")\n",
    "print(\"  - Automatic cache warming\")\n",
    "print(\"  - No query rewriting needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external_tables_section",
   "metadata": {},
   "source": [
    "### 4.4 External Tables (Cloud Storage)\n",
    "\n",
    "Query data directly from Google Cloud Storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external_tables",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚òÅÔ∏è  External Tables Overview\\n\")\n",
    "print(\"What are External Tables?\")\n",
    "print(\"  - Query data in Cloud Storage (GCS) without loading\")\n",
    "print(\"  - Supports CSV, JSON, Avro, Parquet, ORC\")\n",
    "print(\"  - Data stays in GCS (no storage cost in BigQuery)\")\n",
    "print(\"  - Best for infrequently accessed data\\n\")\n",
    "\n",
    "print(\"Benefits:\")\n",
    "print(\"  - Lower storage costs\")\n",
    "print(\"  - No ETL required\")\n",
    "print(\"  - Data lake integration\")\n",
    "print(\"  - Automatic schema detection\\n\")\n",
    "\n",
    "print(\"Limitations:\")\n",
    "print(\"  - Slower than native tables (no cache)\")\n",
    "print(\"  - No DML (INSERT/UPDATE/DELETE)\")\n",
    "print(\"  - No clustering or partitioning\")\n",
    "print(\"  - Limited query optimization\\n\")\n",
    "\n",
    "print(\"Example: Create External Table\")\n",
    "print(\"\"\"\n",
    "CREATE EXTERNAL TABLE `project.dataset.table`\n",
    "OPTIONS (\n",
    "  format = 'PARQUET',\n",
    "  uris = ['gs://bucket/path/*.parquet'],\n",
    "  hive_partition_uri_prefix = 'gs://bucket/path',\n",
    "  require_hive_partition_filter = true\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüí° Use Case: Query data lake from BigQuery without moving data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b57e6",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_results_section",
   "metadata": {},
   "source": [
    "### 5.1 Load and Prepare Results\n",
    "\n",
    "Load benchmark results for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from previous run\n",
    "if \"results\" in locals() and results.query_results:\n",
    "    # Convert to pandas DataFrame for analysis\n",
    "    df_results = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"query\": qr.query_name,\n",
    "                \"time\": qr.execution_time,\n",
    "                \"success\": qr.success,\n",
    "                \"rows_returned\": qr.row_count if hasattr(qr, \"row_count\") else None,\n",
    "            }\n",
    "            for qr in results.query_results\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Results loaded into DataFrame\")\n",
    "    print(f\"\\nShape: {df_results.shape[0]} queries, {df_results.shape[1]} columns\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df_results.head())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results available. Run a benchmark first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats_analysis_section",
   "metadata": {},
   "source": [
    "### 5.2 Statistical Analysis\n",
    "\n",
    "Compute detailed statistics and identify outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"df_results\" in locals():\n",
    "    # Compute statistics\n",
    "    stats = df_results[\"time\"].describe(percentiles=[0.25, 0.5, 0.75, 0.95, 0.99])\n",
    "\n",
    "    print(\"üìä Execution Time Statistics\\n\")\n",
    "    print(stats)\n",
    "\n",
    "    print(\"\\nüîç Key Percentiles:\")\n",
    "    print(f\"  P25 (25th percentile): {df_results['time'].quantile(0.25):.3f}s\")\n",
    "    print(f\"  P50 (median): {df_results['time'].median():.3f}s\")\n",
    "    print(f\"  P75 (75th percentile): {df_results['time'].quantile(0.75):.3f}s\")\n",
    "    print(f\"  P95 (95th percentile): {df_results['time'].quantile(0.95):.3f}s\")\n",
    "    print(f\"  P99 (99th percentile): {df_results['time'].quantile(0.99):.3f}s\")\n",
    "\n",
    "    # Identify outliers (>2 standard deviations)\n",
    "    mean_time = df_results[\"time\"].mean()\n",
    "    std_time = df_results[\"time\"].std()\n",
    "    outliers = df_results[df_results[\"time\"] > mean_time + 2 * std_time]\n",
    "\n",
    "    if not outliers.empty:\n",
    "        print(\"\\n‚ö†Ô∏è  Performance Outliers (>2œÉ):\")\n",
    "        for _, row in outliers.iterrows():\n",
    "            z_score = (row[\"time\"] - mean_time) / std_time\n",
    "            print(f\"  {row['query']}: {row['time']:.2f}s (z-score: {z_score:.2f})\")\n",
    "\n",
    "        print(\"\\nüí° Investigation steps:\")\n",
    "        print(\"  1. Review query execution plan (EXPLAIN)\")\n",
    "        print(\"  2. Check bytes processed (INFORMATION_SCHEMA.JOBS)\")\n",
    "        print(\"  3. Consider partitioning/clustering\")\n",
    "        print(\"  4. Verify slot allocation during execution\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No significant outliers detected\")\n",
    "\n",
    "    # Coefficient of variation (CV)\n",
    "    cv = (std_time / mean_time) * 100\n",
    "    print(\"\\nüìà Variability:\")\n",
    "    print(f\"  Standard deviation: {std_time:.3f}s\")\n",
    "    print(f\"  Coefficient of variation: {cv:.1f}%\")\n",
    "    if cv < 20:\n",
    "        print(\"  Assessment: Low variability (consistent performance)\")\n",
    "    elif cv < 50:\n",
    "        print(\"  Assessment: Moderate variability (typical for mixed workload)\")\n",
    "    else:\n",
    "        print(\"  Assessment: High variability (investigate slow queries)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Load results first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive_viz_section",
   "metadata": {},
   "source": [
    "### 5.3 Comprehensive Visualizations\n",
    "\n",
    "Multi-panel performance visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"df_results\" in locals():\n",
    "    # Create 2x2 subplot grid\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(\"BigQuery Performance Analysis\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    # 1. Distribution histogram\n",
    "    axes[0, 0].hist(df_results[\"time\"], bins=20, color=\"#4285F4\", alpha=0.7, edgecolor=\"black\")\n",
    "    axes[0, 0].axvline(df_results[\"time\"].mean(), color=\"red\", linestyle=\"--\", linewidth=2, label=\"Mean\")\n",
    "    axes[0, 0].axvline(df_results[\"time\"].median(), color=\"green\", linestyle=\"--\", linewidth=2, label=\"Median\")\n",
    "    axes[0, 0].set_xlabel(\"Execution Time (seconds)\", fontweight=\"bold\")\n",
    "    axes[0, 0].set_ylabel(\"Frequency\", fontweight=\"bold\")\n",
    "    axes[0, 0].set_title(\"Execution Time Distribution\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # 2. Box plot\n",
    "    bp = axes[0, 1].boxplot(df_results[\"time\"], patch_artist=True, vert=True)\n",
    "    bp[\"boxes\"][0].set_facecolor(\"#4285F4\")\n",
    "    bp[\"boxes\"][0].set_alpha(0.7)\n",
    "    axes[0, 1].set_ylabel(\"Execution Time (seconds)\", fontweight=\"bold\")\n",
    "    axes[0, 1].set_title(\"Box Plot (Outlier Detection)\")\n",
    "    axes[0, 1].set_xticklabels([\"All Queries\"])\n",
    "    axes[0, 1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # 3. Sorted horizontal bar chart (top 15)\n",
    "    df_sorted = df_results.sort_values(\"time\", ascending=True).tail(15)\n",
    "    colors = [\"#EA4335\" if t > df_results[\"time\"].quantile(0.9) else \"#4285F4\" for t in df_sorted[\"time\"]]\n",
    "    axes[1, 0].barh(df_sorted[\"query\"], df_sorted[\"time\"], color=colors, alpha=0.8)\n",
    "    axes[1, 0].set_xlabel(\"Execution Time (seconds)\", fontweight=\"bold\")\n",
    "    axes[1, 0].set_title(\"Slowest 15 Queries\")\n",
    "    axes[1, 0].grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "    # 4. Cumulative performance (Pareto analysis)\n",
    "    df_sorted_desc = df_results.sort_values(\"time\", ascending=False)\n",
    "    df_sorted_desc[\"cumulative_pct\"] = df_sorted_desc[\"time\"].cumsum() / df_sorted_desc[\"time\"].sum() * 100\n",
    "    axes[1, 1].plot(\n",
    "        range(len(df_sorted_desc)), df_sorted_desc[\"cumulative_pct\"], marker=\"o\", color=\"#4285F4\", linewidth=2\n",
    "    )\n",
    "    axes[1, 1].axhline(80, color=\"red\", linestyle=\"--\", linewidth=2, label=\"80% threshold\")\n",
    "    axes[1, 1].set_xlabel(\"Number of Queries (sorted by time)\", fontweight=\"bold\")\n",
    "    axes[1, 1].set_ylabel(\"Cumulative % of Total Time\", fontweight=\"bold\")\n",
    "    axes[1, 1].set_title(\"Pareto Analysis (80/20 Rule)\")\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Pareto insight\n",
    "    queries_for_80pct = len(df_sorted_desc[df_sorted_desc[\"cumulative_pct\"] <= 80])\n",
    "    print(\"\\nüìä Pareto Insight:\")\n",
    "    print(\n",
    "        f\"  {queries_for_80pct} queries ({queries_for_80pct / len(df_results) * 100:.1f}%) account for 80% of total time\"\n",
    "    )\n",
    "    print(f\"  üí° Focus optimization efforts on these {queries_for_80pct} queries\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Load results first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost_per_query_section",
   "metadata": {},
   "source": [
    "### 5.4 Cost Per Query Analysis\n",
    "\n",
    "Analyze cost distribution across queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cost_per_query",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This requires INFORMATION_SCHEMA.JOBS access\n",
    "try:\n",
    "    # Query job costs\n",
    "    cost_query = rf\"\"\"\n",
    "    SELECT \n",
    "        REGEXP_EXTRACT(query, r'query(\\d+)') as query_num,\n",
    "        total_bytes_processed,\n",
    "        total_bytes_billed,\n",
    "        ROUND(total_bytes_billed / POW(1024, 4) * 5, 4) as cost_usd\n",
    "    FROM `{BQ_PROJECT}.region-{BQ_LOCATION}.INFORMATION_SCHEMA.JOBS_BY_PROJECT`\n",
    "    WHERE creation_time > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR)\n",
    "        AND job_type = 'QUERY'\n",
    "        AND state = 'DONE'\n",
    "        AND query LIKE '%tpch%'\n",
    "    ORDER BY creation_time\n",
    "    \"\"\"\n",
    "\n",
    "    query_job = client.query(cost_query)\n",
    "    cost_df = query_job.to_dataframe()\n",
    "\n",
    "    if not cost_df.empty:\n",
    "        # Visualize cost per query\n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "        bars = ax.bar(cost_df[\"query_num\"], cost_df[\"cost_usd\"], color=\"#34A853\", alpha=0.8)\n",
    "\n",
    "        # Highlight expensive queries (top 20%)\n",
    "        threshold = cost_df[\"cost_usd\"].quantile(0.8)\n",
    "        for bar, cost in zip(bars, cost_df[\"cost_usd\"]):\n",
    "            if cost > threshold:\n",
    "                bar.set_color(\"#EA4335\")\n",
    "\n",
    "        ax.set_xlabel(\"Query Number\", fontsize=12, fontweight=\"bold\")\n",
    "        ax.set_ylabel(\"Cost (USD)\", fontsize=12, fontweight=\"bold\")\n",
    "        ax.set_title(\"Cost Per Query on BigQuery\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Cost summary\n",
    "        total_cost = cost_df[\"cost_usd\"].sum()\n",
    "        total_gb = cost_df[\"total_bytes_processed\"].sum() / (1024**3)\n",
    "\n",
    "        print(\"\\nüí∞ Cost Summary:\")\n",
    "        print(f\"  Total cost: ${total_cost:.4f}\")\n",
    "        print(f\"  Total data processed: {total_gb:.2f} GB\")\n",
    "        print(f\"  Average cost/query: ${total_cost / len(cost_df):.4f}\")\n",
    "        print(f\"  Most expensive query: ${cost_df['cost_usd'].max():.4f}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No cost data available\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not analyze costs: {e}\")\n",
    "    print(\"This requires roles/bigquery.resourceViewer permission\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bytes_processed_section",
   "metadata": {},
   "source": [
    "### 5.5 Bytes Processed Analysis\n",
    "\n",
    "Analyze data scanning patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bytes_processed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Bytes Processed Analysis\\n\")\n",
    "print(\"Why This Matters:\")\n",
    "print(\"  - BigQuery charges $5 per TB processed (on-demand)\")\n",
    "print(\"  - Scanning less data = lower costs\")\n",
    "print(\"  - Partitioning/clustering reduces bytes scanned\\n\")\n",
    "\n",
    "print(\"Optimization Strategies:\")\n",
    "print(\"  1. Use partitioned tables (reduce scan range)\")\n",
    "print(\"  2. Use clustered tables (prune irrelevant blocks)\")\n",
    "print(\"  3. SELECT only needed columns (avoid SELECT *)\")\n",
    "print(\"  4. Use WHERE clauses early (partition pruning)\")\n",
    "print(\"  5. Avoid CROSS JOINs (Cartesian product scanning)\\n\")\n",
    "\n",
    "print(\"Example Impact:\")\n",
    "print(\"  Scenario: 10 TB table, querying 1 month of data\")\n",
    "print(\"  Without partitioning: Scans 10 TB = $50\")\n",
    "print(\"  With daily partitioning: Scans 0.3 TB = $1.50\")\n",
    "print(\"  Savings: $48.50 (97% reduction)\\n\")\n",
    "\n",
    "print(\"üí° Use the Cost Per Query Analysis cell above to see actual bytes processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regression_section",
   "metadata": {},
   "source": [
    "### 5.6 Regression Detection\n",
    "\n",
    "Compare against baseline to detect performance regressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regression_detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"df_results\" in locals():\n",
    "    # Compare against baseline (you can load from a saved baseline file)\n",
    "    # For demonstration, we'll use a mock baseline\n",
    "    baseline_avg = 1.5  # seconds (mock baseline)\n",
    "    current_avg = df_results[\"time\"].mean()\n",
    "\n",
    "    # Calculate change\n",
    "    change_pct = ((current_avg - baseline_avg) / baseline_avg) * 100\n",
    "\n",
    "    print(\"üîç Performance Regression Analysis\\n\")\n",
    "    print(f\"Baseline average: {baseline_avg:.2f}s\")\n",
    "    print(f\"Current average: {current_avg:.2f}s\")\n",
    "    print(f\"Change: {change_pct:+.1f}%\\n\")\n",
    "\n",
    "    # Threshold: 10% change\n",
    "    if abs(change_pct) > 10:\n",
    "        if change_pct > 0:\n",
    "            status = \"‚ùå REGRESSION DETECTED\"\n",
    "            print(status)\n",
    "            print(f\"Performance degraded by {change_pct:.1f}%\\n\")\n",
    "            print(\"üí° Investigation Steps:\")\n",
    "            print(\"  1. Check for slot contention (INFORMATION_SCHEMA.JOBS)\")\n",
    "            print(\"  2. Verify data size hasn't increased unexpectedly\")\n",
    "            print(\"  3. Review recent schema changes (partitions, clustering)\")\n",
    "            print(\"  4. Check for concurrent workloads\")\n",
    "            print(\"  5. Verify query cache hit rate\")\n",
    "        else:\n",
    "            status = \"‚úÖ PERFORMANCE IMPROVEMENT\"\n",
    "            print(status)\n",
    "            print(f\"Performance improved by {abs(change_pct):.1f}%\\n\")\n",
    "            print(\"üí° Possible Reasons:\")\n",
    "            print(\"  - Partitioning/clustering optimization\")\n",
    "            print(\"  - BI Engine caching\")\n",
    "            print(\"  - Query optimization\")\n",
    "            print(\"  - Increased slot allocation\")\n",
    "    else:\n",
    "        print(\"‚úÖ Performance stable (within 10% threshold)\\n\")\n",
    "\n",
    "    # Per-query regression analysis\n",
    "    print(\"\\nüìä Per-Query Regression (mock baseline):\")\n",
    "    for _, row in df_results.head(5).iterrows():\n",
    "        # Mock per-query baseline (in practice, load from saved file)\n",
    "        query_baseline = baseline_avg * (0.8 + 0.4 * (hash(row[\"query\"]) % 100) / 100)\n",
    "        query_change = ((row[\"time\"] - query_baseline) / query_baseline) * 100\n",
    "\n",
    "        status_icon = \"‚ö†Ô∏è\" if abs(query_change) > 20 else \"‚úÖ\"\n",
    "        print(\n",
    "            f\"{status_icon} {row['query']}: {row['time']:.2f}s (baseline: {query_baseline:.2f}s, change: {query_change:+.1f}%)\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nüí° Save current run as new baseline:\")\n",
    "    print(\"   df_results.to_csv('baseline_bigquery_tpch.csv', index=False)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Load results first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf4ec99",
   "metadata": {},
   "source": [
    "## Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connection_diagnostics_section",
   "metadata": {},
   "source": [
    "### 6.1 Connection Diagnostics\n",
    "\n",
    "Comprehensive connection troubleshooting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connection_diagnostics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_bigquery_connection():\n",
    "    \"\"\"Diagnose BigQuery connection issues\"\"\"\n",
    "    print(\"üîç BigQuery Connection Diagnostic\\n\")\n",
    "\n",
    "    # Check 1: Environment variables\n",
    "    print(\"1Ô∏è‚É£ Checking environment variables...\")\n",
    "    if os.environ.get(\"BIGQUERY_PROJECT\"):\n",
    "        print(f\"   ‚úÖ BIGQUERY_PROJECT = {os.environ.get('BIGQUERY_PROJECT')}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  BIGQUERY_PROJECT not set\")\n",
    "\n",
    "    if os.environ.get(\"GOOGLE_APPLICATION_CREDENTIALS\"):\n",
    "        creds_path = os.environ.get(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "        print(f\"   ‚úÖ GOOGLE_APPLICATION_CREDENTIALS = {creds_path}\")\n",
    "        if os.path.exists(creds_path):\n",
    "            print(\"   ‚úÖ Credentials file exists\")\n",
    "        else:\n",
    "            print(\"   ‚ùå Credentials file not found\")\n",
    "    else:\n",
    "        print(\"   ‚ÑπÔ∏è  GOOGLE_APPLICATION_CREDENTIALS not set (using ADC)\")\n",
    "\n",
    "    # Check 2: ADC availability\n",
    "    print(\"\\n2Ô∏è‚É£ Checking Application Default Credentials...\")\n",
    "    try:\n",
    "        test_client = bigquery.Client()\n",
    "        print(f\"   ‚úÖ ADC available (project: {test_client.project})\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå ADC not available: {e}\")\n",
    "\n",
    "    # Check 3: API connectivity\n",
    "    print(\"\\n3Ô∏è‚É£ Testing BigQuery API connectivity...\")\n",
    "    try:\n",
    "        client = bigquery.Client(project=BQ_PROJECT)\n",
    "        query = \"SELECT 1 as test\"\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "        print(\"   ‚úÖ API connectivity successful\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå API connectivity failed: {e}\")\n",
    "\n",
    "    # Check 4: Dataset access\n",
    "    print(\"\\n4Ô∏è‚É£ Checking dataset access...\")\n",
    "    try:\n",
    "        client = bigquery.Client(project=BQ_PROJECT)\n",
    "        dataset_id = f\"{BQ_PROJECT}.{BQ_DATASET}\"\n",
    "        dataset = client.get_dataset(dataset_id)\n",
    "        print(f\"   ‚úÖ Dataset accessible: {dataset_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Dataset access failed: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìö Troubleshooting Guide:\\n\")\n",
    "    print(\"If authentication fails:\")\n",
    "    print(\"  1. Run: gcloud auth application-default login\")\n",
    "    print(\"  2. Or set GOOGLE_APPLICATION_CREDENTIALS to service account key\")\n",
    "    print(\"  3. Verify project ID is correct\\n\")\n",
    "\n",
    "    print(\"If API connectivity fails:\")\n",
    "    print(\"  1. Enable BigQuery API: https://console.cloud.google.com/apis/library/bigquery.googleapis.com\")\n",
    "    print(\"  2. Verify billing is enabled\")\n",
    "    print(\"  3. Check firewall/network settings\\n\")\n",
    "\n",
    "    print(\"If dataset access fails:\")\n",
    "    print(\"  1. Verify IAM permissions (roles/bigquery.admin or roles/bigquery.dataEditor)\")\n",
    "    print(\"  2. Create dataset if it doesn't exist\")\n",
    "    print(\"  3. Check dataset location matches client location\")\n",
    "\n",
    "\n",
    "# Run diagnostics\n",
    "diagnose_bigquery_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permission_validation_section",
   "metadata": {},
   "source": [
    "### 6.2 Permission Validation\n",
    "\n",
    "Verify required IAM permissions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permission_validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_bigquery_permissions():\n",
    "    \"\"\"Validate BigQuery permissions\"\"\"\n",
    "    print(\"üîí BigQuery Permission Validation\\n\")\n",
    "\n",
    "    client = bigquery.Client(project=BQ_PROJECT)\n",
    "\n",
    "    # Test 1: List datasets\n",
    "    print(\"1Ô∏è‚É£ Testing dataset listing (bigquery.datasets.get)...\")\n",
    "    try:\n",
    "        datasets = list(client.list_datasets())\n",
    "        print(f\"   ‚úÖ Can list datasets ({len(datasets)} found)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Cannot list datasets: {e}\")\n",
    "\n",
    "    # Test 2: Create table\n",
    "    print(\"\\n2Ô∏è‚É£ Testing table creation (bigquery.tables.create)...\")\n",
    "    try:\n",
    "        test_table_id = f\"{BQ_PROJECT}.{BQ_DATASET}.benchbox_test_table\"\n",
    "        schema = [bigquery.SchemaField(\"test_col\", \"STRING\")]\n",
    "        table = bigquery.Table(test_table_id, schema=schema)\n",
    "        table = client.create_table(table, exists_ok=True)\n",
    "        print(\"   ‚úÖ Can create tables\")\n",
    "\n",
    "        # Clean up\n",
    "        client.delete_table(test_table_id)\n",
    "        print(\"   ‚úÖ Can delete tables\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Cannot create/delete tables: {e}\")\n",
    "\n",
    "    # Test 3: Query execution\n",
    "    print(\"\\n3Ô∏è‚É£ Testing query execution (bigquery.jobs.create)...\")\n",
    "    try:\n",
    "        query = \"SELECT 1 as test\"\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "        print(\"   ‚úÖ Can execute queries\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Cannot execute queries: {e}\")\n",
    "\n",
    "    # Test 4: INFORMATION_SCHEMA access\n",
    "    print(\"\\n4Ô∏è‚É£ Testing INFORMATION_SCHEMA access (bigquery.jobs.list)...\")\n",
    "    try:\n",
    "        query = f\"\"\"\n",
    "        SELECT job_id\n",
    "        FROM `{BQ_PROJECT}.region-{BQ_LOCATION}.INFORMATION_SCHEMA.JOBS_BY_PROJECT`\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "        print(\"   ‚úÖ Can access INFORMATION_SCHEMA (cost analysis available)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Cannot access INFORMATION_SCHEMA: {e}\")\n",
    "        print(\"   Note: This requires roles/bigquery.resourceViewer\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìã Required Roles:\\n\")\n",
    "    print(\"Minimum (for benchmarking):\")\n",
    "    print(\"  - roles/bigquery.dataEditor (create/delete tables)\")\n",
    "    print(\"  - roles/bigquery.jobUser (execute queries)\\n\")\n",
    "    print(\"Recommended (full features):\")\n",
    "    print(\"  - roles/bigquery.admin (all operations)\\n\")\n",
    "    print(\"Optional (cost analysis):\")\n",
    "    print(\"  - roles/bigquery.resourceViewer (INFORMATION_SCHEMA access)\")\n",
    "\n",
    "\n",
    "# Run validation\n",
    "try:\n",
    "    validate_bigquery_permissions()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Permission validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quota_checking_section",
   "metadata": {},
   "source": [
    "### 6.3 Quota and Limits Checking\n",
    "\n",
    "Verify quotas and limits for large benchmark runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quota_checking",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä BigQuery Quotas and Limits\\n\")\n",
    "print(\"Query Quotas (per project, per day):\")\n",
    "print(\"  - Interactive queries: 2000 concurrent\")\n",
    "print(\"  - Batch queries: 100 concurrent\")\n",
    "print(\"  - Query size: 1 MB per query (text)\")\n",
    "print(\"  - Response size: 10 GB (with pagination)\\n\")\n",
    "\n",
    "print(\"Loading Quotas:\")\n",
    "print(\"  - Load jobs per table: 1,500 per day\")\n",
    "print(\"  - Load jobs per project: 100,000 per day\")\n",
    "print(\"  - Maximum file size: 5 TB (uncompressed)\")\n",
    "print(\"  - Maximum files per load: 10,000\\n\")\n",
    "\n",
    "print(\"Slot Quotas (on-demand):\")\n",
    "print(\"  - Default slots: 2000\")\n",
    "print(\"  - Burst capacity: Up to 2000 slots\")\n",
    "print(\"  - Fair scheduling: Auto slot allocation\\n\")\n",
    "\n",
    "print(\"Storage Limits:\")\n",
    "print(\"  - Maximum table size: Unlimited\")\n",
    "print(\"  - Maximum columns: 10,000 per table\")\n",
    "print(\"  - Maximum row size: 100 MB\")\n",
    "print(\"  - Maximum nested depth: 15 levels\\n\")\n",
    "\n",
    "print(\"Rate Limits:\")\n",
    "print(\"  - API requests: 100 per second per user\")\n",
    "print(\"  - Streaming inserts: 100,000 rows/second per table\")\n",
    "print(\"  - Dataset metadata operations: 100 per second\\n\")\n",
    "\n",
    "print(\"üí° For Large Benchmarks:\")\n",
    "print(\"  1. Use batch queries for non-urgent workloads (lower priority)\")\n",
    "print(\"  2. Request quota increases if needed (IAM console)\")\n",
    "print(\"  3. Monitor quota usage: https://console.cloud.google.com/iam-admin/quotas\")\n",
    "print(\"  4. Consider flat-rate pricing for sustained high usage\\n\")\n",
    "\n",
    "print(\"üîç Check Current Usage:\")\n",
    "print(\"  https://console.cloud.google.com/iam-admin/quotas?service=bigquery.googleapis.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common_issues_section",
   "metadata": {},
   "source": [
    "### 6.4 Common Issues and Solutions\n",
    "\n",
    "Quick reference for common BigQuery benchmarking issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common_issues",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Common BigQuery Benchmarking Issues\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚ùå Issue: 'Permission denied' errors\")\n",
    "print(\"‚úÖ Solution:\")\n",
    "print(\"   1. Verify IAM role: roles/bigquery.admin or roles/bigquery.dataEditor\")\n",
    "print(\"   2. Check dataset-level permissions (Dataset Info ‚Üí Permissions)\")\n",
    "print(\"   3. Ensure project billing is enabled\")\n",
    "print(\"   4. Re-authenticate: gcloud auth application-default login\\n\")\n",
    "\n",
    "print(\"‚ùå Issue: 'Quota exceeded' errors\")\n",
    "print(\"‚úÖ Solution:\")\n",
    "print(\"   1. Check quotas: https://console.cloud.google.com/iam-admin/quotas\")\n",
    "print(\"   2. Wait for quota reset (daily quotas reset at midnight PST)\")\n",
    "print(\"   3. Request quota increase in GCP console\")\n",
    "print(\"   4. Use batch queries (lower priority, less quota consumption)\\n\")\n",
    "\n",
    "print(\"‚ùå Issue: Slow query performance\")\n",
    "print(\"‚úÖ Solution:\")\n",
    "print(\"   1. Use EXPLAIN query to analyze execution plan\")\n",
    "print(\"   2. Check INFORMATION_SCHEMA.JOBS for bytes processed\")\n",
    "print(\"   3. Add partitioning/clustering to reduce scan size\")\n",
    "print(\"   4. Verify slot allocation (may be limited by concurrent workloads)\")\n",
    "print(\"   5. Consider BI Engine for repeated queries\\n\")\n",
    "\n",
    "print(\"‚ùå Issue: High costs\")\n",
    "print(\"‚úÖ Solution:\")\n",
    "print(\"   1. Use partitioned tables (scan only needed partitions)\")\n",
    "print(\"   2. Add clustering (reduce bytes scanned)\")\n",
    "print(\"   3. Avoid SELECT * (specify columns)\")\n",
    "print(\"   4. Use materialized views for repeated aggregations\")\n",
    "print(\"   5. Consider flat-rate pricing for sustained workloads (>$50k/month)\\n\")\n",
    "\n",
    "print(\"‚ùå Issue: 'Resources exceeded' during query\")\n",
    "print(\"‚úÖ Solution:\")\n",
    "print(\"   1. Break query into smaller chunks\")\n",
    "print(\"   2. Use approximate aggregations (APPROX_COUNT_DISTINCT)\")\n",
    "print(\"   3. Filter early (WHERE before JOIN)\")\n",
    "print(\"   4. Increase destination table expiration\")\n",
    "print(\"   5. Use intermediate tables for complex multi-stage queries\\n\")\n",
    "\n",
    "print(\"‚ùå Issue: Data loading failures\")\n",
    "print(\"‚úÖ Solution:\")\n",
    "print(\"   1. Check file format (CSV, JSON, Parquet)\")\n",
    "print(\"   2. Verify schema matches data\")\n",
    "print(\"   3. Use autodetect=True for schema inference\")\n",
    "print(\"   4. Check for malformed rows (enable bad record handling)\")\n",
    "print(\"   5. Split large files into smaller chunks (<5 GB each)\\n\")\n",
    "\n",
    "print(\"‚ùå Issue: 'Not found: Dataset' errors\")\n",
    "print(\"‚úÖ Solution:\")\n",
    "print(\"   1. Create dataset: bq mk --dataset PROJECT_ID:DATASET_NAME\")\n",
    "print(\"   2. Verify dataset location matches client location\")\n",
    "print(\"   3. Check dataset name spelling (case-sensitive)\")\n",
    "print(\"   4. Ensure project ID is correct\\n\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüí° More Help:\")\n",
    "print(\"  - BigQuery docs: https://cloud.google.com/bigquery/docs\")\n",
    "print(\"  - Troubleshooting guide: https://cloud.google.com/bigquery/docs/troubleshoot\")\n",
    "print(\"  - BenchBox docs: https://github.com/joeharris76/benchbox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps_section",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**Continue Learning:**\n",
    "- Explore other cloud platforms: Snowflake, Databricks, Redshift\n",
    "- Try different benchmarks: TPC-DS, ClickBench, SSB\n",
    "- Compare platforms with multi-platform notebooks\n",
    "- Set up CI/CD regression testing\n",
    "\n",
    "**Platform-Specific Features to Explore:**\n",
    "- BI Engine (in-memory acceleration)\n",
    "- Materialized views (pre-computed aggregations)\n",
    "- External tables (query data in GCS)\n",
    "- BigQuery ML (machine learning integration)\n",
    "- Data transfer service (automated ETL)\n",
    "\n",
    "**Resources:**\n",
    "- [BenchBox Documentation](https://github.com/joeharris76/benchbox)\n",
    "- [BigQuery Documentation](https://cloud.google.com/bigquery/docs)\n",
    "- [BigQuery Best Practices](https://cloud.google.com/bigquery/docs/best-practices)\n",
    "- [BigQuery Pricing](https://cloud.google.com/bigquery/pricing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
