{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# BenchBox Databricks Benchmarking\n",
    "\n",
    "**Complete Guide to Running Analytics Benchmarks on Databricks**\n",
    "\n",
    "This notebook demonstrates how to use BenchBox to run industry-standard benchmarks (TPC-H, TPC-DS, ClickBench) on Databricks with Unity Catalog and Delta Lake.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Install and configure BenchBox for Databricks\n",
    "- Run benchmarks at different scale factors\n",
    "- Leverage Unity Catalog and Delta Lake optimizations\n",
    "- Analyze and visualize performance results\n",
    "- Troubleshoot common issues\n",
    "\n",
    "**Prerequisites:**\n",
    "- Active Databricks workspace\n",
    "- SQL Warehouse or compute cluster with Unity Catalog enabled\n",
    "- Personal Access Token or Service Principal credentials\n",
    "- Unity Catalog volume for data staging\n",
    "\n",
    "**Estimated Time:** 15-30 minutes for quick examples, 1-2 hours for comprehensive benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1_header",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup\n",
    "\n",
    "First, we'll install BenchBox and required dependencies, then configure authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install BenchBox with Databricks support\n",
    "# Use %pip in Databricks notebooks for proper installation\n",
    "%pip install benchbox[databricks] matplotlib seaborn pandas --quiet\n",
    "\n",
    "# Restart Python kernel to load new packages\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# BenchBox imports\n",
    "from benchbox.core.config import BenchmarkConfig, DatabaseConfig\n",
    "from benchbox.core.runner import LifecyclePhases, run_benchmark_lifecycle\n",
    "from benchbox.platforms.databricks import DatabricksAdapter\n",
    "\n",
    "# Set style for better-looking plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auth_header",
   "metadata": {},
   "source": [
    "### Authentication Method 1: Environment Variables (Recommended)\n",
    "\n",
    "For production use, store credentials in Databricks Secrets or environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auth_env",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Environment Variables (Recommended for Jobs)\n",
    "try:\n",
    "    # In Databricks, these can be set via:\n",
    "    # - Cluster environment variables\n",
    "    # - Job task parameters\n",
    "    # - Databricks Secrets (dbutils.secrets.get)\n",
    "\n",
    "    DATABRICKS_HOST = os.environ.get(\"DATABRICKS_HOST\")\n",
    "    DATABRICKS_TOKEN = os.environ.get(\"DATABRICKS_TOKEN\")\n",
    "\n",
    "    # Unity Catalog configuration\n",
    "    UC_CATALOG = os.environ.get(\"UC_CATALOG\", \"main\")\n",
    "    UC_SCHEMA = os.environ.get(\"UC_SCHEMA\", \"benchbox\")\n",
    "    UC_VOLUME = os.environ.get(\"UC_VOLUME\", \"data\")\n",
    "\n",
    "    if not DATABRICKS_HOST or not DATABRICKS_TOKEN:\n",
    "        print(\"‚ö†Ô∏è  Environment variables not set. Using fallback method...\")\n",
    "        # Fallback: Use current workspace context\n",
    "        DATABRICKS_HOST = dbutils.notebook.entry_point.getDbutils().notebook().getContext().browserHostName().get()\n",
    "        DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "        print(\"‚úÖ Using workspace context for authentication\")\n",
    "    else:\n",
    "        print(\"‚úÖ Using environment variables for authentication\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Authentication error: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"  1. Set DATABRICKS_HOST and DATABRICKS_TOKEN environment variables\")\n",
    "    print(\"  2. Or run this notebook in a Databricks workspace\")\n",
    "    print(\"  3. Ensure your token has workspace access permissions\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\nüìç Databricks Host: {DATABRICKS_HOST}\")\n",
    "print(f\"üìã Unity Catalog: {UC_CATALOG}.{UC_SCHEMA}\")\n",
    "print(f\"üìÅ Volume: {UC_VOLUME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auth_secrets_header",
   "metadata": {},
   "source": [
    "### Authentication Method 2: Databricks Secrets (Production)\n",
    "\n",
    "For secure credential management in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auth_secrets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Databricks Secrets (Most Secure)\n",
    "# Uncomment and configure if using Databricks Secrets\n",
    "\n",
    "# try:\n",
    "#     SECRET_SCOPE = \"benchbox\"  # Your secret scope name\n",
    "#     DATABRICKS_TOKEN = dbutils.secrets.get(scope=SECRET_SCOPE, key=\"databricks_token\")\n",
    "#     print(\"‚úÖ Retrieved credentials from Databricks Secrets\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ö†Ô∏è Could not retrieve secrets: {e}\")\n",
    "#     print(\"Using fallback authentication method\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è Secrets method commented out - using environment variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conn_test_header",
   "metadata": {},
   "source": [
    "### Connection Test\n",
    "\n",
    "Verify we can connect to Databricks successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conn_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connection to Databricks\n",
    "try:\n",
    "    # Verify Unity Catalog volume exists\n",
    "    volume_path = f\"/Volumes/{UC_CATALOG}/{UC_SCHEMA}/{UC_VOLUME}\"\n",
    "\n",
    "    try:\n",
    "        # Check if volume is accessible\n",
    "        display(dbutils.fs.ls(volume_path))\n",
    "        print(f\"‚úÖ Unity Catalog volume accessible: {volume_path}\")\n",
    "    except Exception:\n",
    "        print(f\"‚ö†Ô∏è Volume not found: {volume_path}\")\n",
    "        print(\"\\nüí° Creating volume...\")\n",
    "        print(\"Run this SQL command in a notebook or SQL editor:\")\n",
    "        print(f\"  CREATE SCHEMA IF NOT EXISTS {UC_CATALOG}.{UC_SCHEMA};\")\n",
    "        print(f\"  CREATE VOLUME IF NOT EXISTS {UC_CATALOG}.{UC_SCHEMA}.{UC_VOLUME};\")\n",
    "\n",
    "    print(\"\\n‚úÖ Connection test passed\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection test failed: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting steps:\")\n",
    "    print(\"  1. Verify your token is valid\")\n",
    "    print(\"  2. Ensure you have Unity Catalog access\")\n",
    "    print(\"  3. Check if the catalog and schema exist\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify_header",
   "metadata": {},
   "source": [
    "### Verify BenchBox Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify BenchBox version and available benchmarks\n",
    "import benchbox\n",
    "\n",
    "print(f\"üì¶ BenchBox version: {benchbox.__version__}\")\n",
    "print(\"\\nüéØ Available Benchmarks:\")\n",
    "print(\"  ‚Ä¢ TPC-H: Decision support benchmark (22 queries)\")\n",
    "print(\"  ‚Ä¢ TPC-DS: Complex analytics benchmark (99 queries)\")\n",
    "print(\"  ‚Ä¢ ClickBench: Real-world analytics (43 queries)\")\n",
    "print(\"  ‚Ä¢ SSB: Star Schema Benchmark\")\n",
    "print(\"  ‚Ä¢ And more...\")\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete! Ready to run benchmarks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2_header",
   "metadata": {},
   "source": [
    "## 2. Quick Start Example\n",
    "\n",
    "Run a simple TPC-H benchmark to verify everything works. This will:\n",
    "1. Generate ~10MB of TPC-H data (scale factor 0.01)\n",
    "2. Load it into Delta tables in Unity Catalog\n",
    "3. Execute the TPC-H power test (22 queries)\n",
    "4. Display results\n",
    "\n",
    "**Expected time:** 5-10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quickstart_overview",
   "metadata": {},
   "source": [
    "### Configure Small Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quickstart_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure a small TPC-H benchmark\n",
    "db_cfg = DatabaseConfig(type=\"databricks\", name=\"unity-catalog\")\n",
    "\n",
    "platform_cfg = {\n",
    "    \"host\": DATABRICKS_HOST,\n",
    "    \"token\": DATABRICKS_TOKEN,\n",
    "    \"uc_catalog\": UC_CATALOG,\n",
    "    \"uc_schema\": UC_SCHEMA,\n",
    "    \"uc_volume\": UC_VOLUME,\n",
    "    \"staging_root\": f\"dbfs:/Volumes/{UC_CATALOG}/{UC_SCHEMA}/{UC_VOLUME}/benchbox\",\n",
    "}\n",
    "\n",
    "bench_cfg = BenchmarkConfig(\n",
    "    name=\"tpch\",\n",
    "    display_name=\"TPC-H Quick Test\",\n",
    "    scale_factor=0.01,  # ~10MB of data\n",
    "    test_execution_type=\"power\",  # Sequential query execution\n",
    ")\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "print(\"  Benchmark: TPC-H\")\n",
    "print(\"  Scale Factor: 0.01 (~10MB)\")\n",
    "print(\"  Test Type: Power (sequential)\")\n",
    "print(f\"  Target: {UC_CATALOG}.{UC_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quickstart_run_header",
   "metadata": {},
   "source": [
    "### Run Complete Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quickstart_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete benchmark lifecycle\n",
    "print(\"üöÄ Starting TPC-H benchmark...\\n\")\n",
    "\n",
    "try:\n",
    "    results = run_benchmark_lifecycle(\n",
    "        benchmark_config=bench_cfg,\n",
    "        database_config=db_cfg,\n",
    "        system_profile=None,\n",
    "        platform_config=platform_cfg,\n",
    "        phases=LifecyclePhases(\n",
    "            generate=True,  # Generate TPC-H data\n",
    "            load=True,  # Load into Delta tables\n",
    "            execute=True,  # Execute queries\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(\"\\n‚úÖ Benchmark completed successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Benchmark failed: {e}\")\n",
    "    print(\"\\nüí° Common issues:\")\n",
    "    print(\"  ‚Ä¢ Check cluster is running and has sufficient resources\")\n",
    "    print(\"  ‚Ä¢ Verify Unity Catalog permissions\")\n",
    "    print(\"  ‚Ä¢ Ensure volume has write access\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quickstart_results_header",
   "metadata": {},
   "source": [
    "### Display Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quickstart_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display key metrics\n",
    "print(\"üìä Benchmark Results Summary\\n\")\n",
    "print(f\"Benchmark: {results.benchmark_name}\")\n",
    "print(f\"Test Type: {results.test_execution_type}\")\n",
    "print(f\"Scale Factor: {results.scale_factor}\")\n",
    "print(\"\\n‚è±Ô∏è Performance:\")\n",
    "print(f\"  Total Time: {results.total_execution_time:.2f} seconds\")\n",
    "print(f\"  Average Query Time: {results.average_query_time:.2f} seconds\")\n",
    "print(f\"  Queries Executed: {results.successful_queries}/{results.total_queries}\")\n",
    "\n",
    "if results.failed_queries > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Failed Queries: {results.failed_queries}\")\n",
    "    print(\"Check the detailed results for error information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quickstart_viz_header",
   "metadata": {},
   "source": [
    "### Visualize Query Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quickstart_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance visualization\n",
    "if results.query_results:\n",
    "    # Extract query data\n",
    "    query_names = [qr.query_name for qr in results.query_results]\n",
    "    execution_times = [qr.execution_time for qr in results.query_results]\n",
    "\n",
    "    # Create bar chart\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    bars = ax.bar(query_names, execution_times, color=\"steelblue\", alpha=0.8)\n",
    "\n",
    "    # Highlight slowest queries\n",
    "    max_time = max(execution_times)\n",
    "    for i, (bar, time) in enumerate(zip(bars, execution_times)):\n",
    "        if time > max_time * 0.7:  # Top 30% slowest\n",
    "            bar.set_color(\"coral\")\n",
    "\n",
    "    ax.set_xlabel(\"Query\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Execution Time (seconds)\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_title(\"TPC-H Query Performance on Databricks\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"‚úÖ Visualization complete\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No query results available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3_header",
   "metadata": {},
   "source": [
    "## 3. Advanced Examples\n",
    "\n",
    "Now let's explore more advanced benchmarking scenarios:\n",
    "- Multiple benchmarks (TPC-DS, ClickBench)\n",
    "- Different scale factors\n",
    "- Query subsets for fast iteration\n",
    "- Performance tuning\n",
    "- Result comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced_overview",
   "metadata": {},
   "source": [
    "### Scenario 1: TPC-DS Comparison\n",
    "\n",
    "TPC-DS is more complex than TPC-H with 99 queries testing advanced SQL features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced_tpcds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TPC-DS at small scale\n",
    "print(\"üöÄ Running TPC-DS benchmark...\\n\")\n",
    "\n",
    "tpcds_cfg = BenchmarkConfig(\n",
    "    name=\"tpcds\",\n",
    "    display_name=\"TPC-DS\",\n",
    "    scale_factor=0.01,  # Start small for TPC-DS\n",
    "    test_execution_type=\"power\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    tpcds_results = run_benchmark_lifecycle(\n",
    "        benchmark_config=tpcds_cfg,\n",
    "        database_config=db_cfg,\n",
    "        system_profile=None,\n",
    "        platform_config=platform_cfg,\n",
    "        phases=LifecyclePhases(generate=True, load=True, execute=True),\n",
    "    )\n",
    "\n",
    "    print(f\"\\n‚úÖ TPC-DS completed in {tpcds_results.total_execution_time:.2f} seconds\")\n",
    "    print(f\"Queries: {tpcds_results.successful_queries}/{tpcds_results.total_queries}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå TPC-DS failed: {e}\")\n",
    "    print(\"üí° Note: TPC-DS is more resource-intensive than TPC-H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced_scaling_header",
   "metadata": {},
   "source": [
    "### Scenario 2: Scale Factor Comparison\n",
    "\n",
    "Compare performance across different data sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced_scaling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different scale factors\n",
    "scale_factors = [0.01, 0.1]  # 10MB and 100MB\n",
    "scaling_results = {}\n",
    "\n",
    "print(\"üìä Scale Factor Performance Comparison\\n\")\n",
    "\n",
    "for sf in scale_factors:\n",
    "    print(f\"Testing SF={sf} (~{int(sf * 1000)}MB)...\")\n",
    "\n",
    "    cfg = BenchmarkConfig(name=\"tpch\", display_name=f\"TPC-H SF{sf}\", scale_factor=sf, test_execution_type=\"power\")\n",
    "\n",
    "    try:\n",
    "        result = run_benchmark_lifecycle(\n",
    "            benchmark_config=cfg,\n",
    "            database_config=db_cfg,\n",
    "            system_profile=None,\n",
    "            platform_config=platform_cfg,\n",
    "            phases=LifecyclePhases(generate=True, load=True, execute=True),\n",
    "        )\n",
    "\n",
    "        scaling_results[sf] = {\n",
    "            \"total_time\": result.total_execution_time,\n",
    "            \"avg_time\": result.average_query_time,\n",
    "            \"successful\": result.successful_queries,\n",
    "        }\n",
    "\n",
    "        print(f\"  ‚úÖ Completed: {result.total_execution_time:.2f}s\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Failed: {e}\\n\")\n",
    "        scaling_results[sf] = None\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\nüìã Scaling Analysis:\")\n",
    "df = pd.DataFrame(scaling_results).T\n",
    "df.index.name = \"Scale Factor\"\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced_subset_header",
   "metadata": {},
   "source": [
    "### Scenario 3: Query Subset for Fast Iteration\n",
    "\n",
    "Run only specific queries for rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced_subset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run specific queries only (fast iteration)\n",
    "from benchbox.tpch import TPCH\n",
    "\n",
    "print(\"üéØ Running Query Subset (Q1, Q6, Q12)\\n\")\n",
    "\n",
    "# Create benchmark and adapter directly\n",
    "tpch = TPCH(scale_factor=0.01)\n",
    "adapter = DatabricksAdapter(\n",
    "    server_hostname=DATABRICKS_HOST,\n",
    "    http_path=\"/sql/1.0/warehouses/<your-warehouse-id>\",  # Update this\n",
    "    access_token=DATABRICKS_TOKEN,\n",
    ")\n",
    "\n",
    "# Run specific queries\n",
    "try:\n",
    "    subset_results = adapter.run_benchmark(\n",
    "        benchmark=tpch,\n",
    "        test_execution_type=\"power\",\n",
    "        query_subset=[\"1\", \"6\", \"12\"],  # Fast queries for smoke testing\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Query subset completed: {subset_results.total_execution_time:.2f}s\")\n",
    "    for qr in subset_results.query_results:\n",
    "        print(f\"  Query {qr.query_name}: {qr.execution_time:.3f}s\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Query subset failed: {e}\")\n",
    "    print(\"üí° Update the http_path with your SQL Warehouse ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced_tuning_header",
   "metadata": {},
   "source": [
    "### Scenario 4: Performance Tuning Example\n",
    "\n",
    "Compare baseline vs optimized configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced_tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Z-ORDER optimization for Delta tables\n",
    "print(\"üîß Performance Tuning Example\\n\")\n",
    "\n",
    "# This is conceptual - actual Z-ORDER commands would be:\n",
    "sql_examples = f\"\"\"\n",
    "-- After loading TPC-H tables, optimize them:\n",
    "OPTIMIZE {UC_CATALOG}.{UC_SCHEMA}.customer ZORDER BY (c_custkey);\n",
    "OPTIMIZE {UC_CATALOG}.{UC_SCHEMA}.orders ZORDER BY (o_orderdate, o_custkey);\n",
    "OPTIMIZE {UC_CATALOG}.{UC_SCHEMA}.lineitem ZORDER BY (l_orderkey, l_shipdate);\n",
    "\n",
    "-- Analyze table statistics:\n",
    "ANALYZE TABLE {UC_CATALOG}.{UC_SCHEMA}.customer COMPUTE STATISTICS FOR ALL COLUMNS;\n",
    "ANALYZE TABLE {UC_CATALOG}.{UC_SCHEMA}.orders COMPUTE STATISTICS FOR ALL COLUMNS;\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù Optimization SQL Commands:\")\n",
    "print(sql_examples)\n",
    "\n",
    "print(\"\\nüí° Run these commands in a SQL notebook, then re-run benchmarks to measure improvement.\")\n",
    "print(\"\\nExpected improvements:\")\n",
    "print(\"  ‚Ä¢ 10-30% faster for queries with date/key filters\")\n",
    "print(\"  ‚Ä¢ Better performance for join-heavy queries\")\n",
    "print(\"  ‚Ä¢ More efficient data skipping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced_parallel_header",
   "metadata": {},
   "source": [
    "### Scenario 5: Throughput Test (Parallel Execution)\n",
    "\n",
    "Run queries concurrently to test multi-user performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced_parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throughput test with concurrent streams\n",
    "print(\"üîÄ Throughput Test (Concurrent Execution)\\n\")\n",
    "\n",
    "throughput_cfg = BenchmarkConfig(\n",
    "    name=\"tpch\",\n",
    "    display_name=\"TPC-H Throughput\",\n",
    "    scale_factor=0.01,\n",
    "    test_execution_type=\"throughput\",  # Parallel execution\n",
    "    num_streams=2,  # Run 2 concurrent streams\n",
    ")\n",
    "\n",
    "try:\n",
    "    throughput_results = run_benchmark_lifecycle(\n",
    "        benchmark_config=throughput_cfg,\n",
    "        database_config=db_cfg,\n",
    "        system_profile=None,\n",
    "        platform_config=platform_cfg,\n",
    "        phases=LifecyclePhases(execute=True),  # Data already loaded\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Throughput test completed\")\n",
    "    print(f\"Total time: {throughput_results.total_execution_time:.2f}s\")\n",
    "    print(f\"Streams: {throughput_cfg.num_streams}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Throughput test failed: {e}\")\n",
    "    print(\"üí° Throughput tests require more cluster resources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced_compare_header",
   "metadata": {},
   "source": [
    "### Scenario 6: Result Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced_compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results from different runs\n",
    "comparison_data = {\n",
    "    \"TPC-H\": results.total_execution_time,\n",
    "}\n",
    "\n",
    "# Add other results if available\n",
    "if \"tpcds_results\" in locals() and tpcds_results:\n",
    "    comparison_data[\"TPC-DS\"] = tpcds_results.total_execution_time\n",
    "\n",
    "# Create comparison chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "benchmarks = list(comparison_data.keys())\n",
    "times = list(comparison_data.values())\n",
    "\n",
    "ax.bar(benchmarks, times, color=[\"steelblue\", \"coral\"][: len(benchmarks)], alpha=0.8)\n",
    "ax.set_ylabel(\"Total Time (seconds)\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_title(\"Benchmark Comparison (SF=0.01)\", fontsize=14, fontweight=\"bold\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bench, time) in enumerate(zip(benchmarks, times)):\n",
    "    ax.text(i, time, f\"{time:.1f}s\", ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced_export_header",
   "metadata": {},
   "source": [
    "### Scenario 7: Export Results\n",
    "\n",
    "Save results in multiple formats for analysis and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced_export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to JSON and CSV\n",
    "from benchbox.core.results.exporter import ResultExporter\n",
    "\n",
    "exporter = ResultExporter(results)\n",
    "\n",
    "# Export to JSON (complete results)\n",
    "json_path = \"/tmp/databricks_tpch_results.json\"\n",
    "exporter.export_json(json_path)\n",
    "print(f\"‚úÖ Exported JSON: {json_path}\")\n",
    "\n",
    "# Export to CSV (query-level results)\n",
    "csv_path = \"/tmp/databricks_tpch_results.csv\"\n",
    "exporter.export_csv(csv_path)\n",
    "print(f\"‚úÖ Exported CSV: {csv_path}\")\n",
    "\n",
    "# Export to HTML (visual report)\n",
    "html_path = \"/tmp/databricks_tpch_results.html\"\n",
    "exporter.export_html(html_path)\n",
    "print(f\"‚úÖ Exported HTML: {html_path}\")\n",
    "\n",
    "print(\"\\nüíæ Results exported to /tmp directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4_header",
   "metadata": {},
   "source": [
    "## 4. Platform-Specific Features\n",
    "\n",
    "Leverage Databricks-specific optimizations and features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "platform_uc_header",
   "metadata": {},
   "source": [
    "### Unity Catalog Governance\n",
    "\n",
    "All benchmark data is stored in Unity Catalog for governance and access control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "platform_uc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Unity Catalog metadata\n",
    "print(\"üìã Unity Catalog Table Inventory\\n\")\n",
    "\n",
    "# List tables created by benchmarks\n",
    "tables_query = f\"\"\"\n",
    "SHOW TABLES IN {UC_CATALOG}.{UC_SCHEMA}\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Catalog: {UC_CATALOG}\")\n",
    "print(f\"Schema: {UC_SCHEMA}\\n\")\n",
    "\n",
    "# Note: This would require executing SQL against Databricks\n",
    "print(\"üí° Run the following SQL to see your benchmark tables:\")\n",
    "print(tables_query)\n",
    "print(\"\\nTypical tables created:\")\n",
    "print(\"  TPC-H: customer, lineitem, nation, orders, part, partsupp, region, supplier\")\n",
    "print(\"  TPC-DS: 24 tables (store_sales, web_sales, catalog_sales, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "platform_delta_header",
   "metadata": {},
   "source": [
    "### Delta Lake Features\n",
    "\n",
    "All tables are created as Delta tables with ACID transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "platform_delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta Lake feature examples\n",
    "print(\"üî∑ Delta Lake Features\\n\")\n",
    "\n",
    "delta_features = f\"\"\"\n",
    "-- Time Travel (query historical versions)\n",
    "SELECT COUNT(*) FROM {UC_CATALOG}.{UC_SCHEMA}.lineitem\n",
    "VERSION AS OF 1;  -- Query first version\n",
    "\n",
    "-- Optimize tables (compaction)\n",
    "OPTIMIZE {UC_CATALOG}.{UC_SCHEMA}.lineitem;\n",
    "\n",
    "-- Z-ORDER for better data skipping\n",
    "OPTIMIZE {UC_CATALOG}.{UC_SCHEMA}.lineitem\n",
    "ZORDER BY (l_shipdate, l_orderkey);\n",
    "\n",
    "-- Vacuum old files (after 7 days retention)\n",
    "VACUUM {UC_CATALOG}.{UC_SCHEMA}.lineitem RETAIN 168 HOURS;\n",
    "\n",
    "-- Table history\n",
    "DESCRIBE HISTORY {UC_CATALOG}.{UC_SCHEMA}.lineitem;\n",
    "\"\"\"\n",
    "\n",
    "print(delta_features)\n",
    "print(\"‚úÖ All benchmark tables are Delta tables by default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "platform_copy_header",
   "metadata": {},
   "source": [
    "### COPY INTO from Unity Catalog Volumes\n",
    "\n",
    "BenchBox uses COPY INTO for efficient data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "platform_copy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPY INTO pattern used by BenchBox\n",
    "print(\"üìé Data Loading Pattern\\n\")\n",
    "\n",
    "copy_example = f\"\"\"\n",
    "-- BenchBox generates data to Unity Catalog Volumes:\n",
    "/Volumes/{UC_CATALOG}/{UC_SCHEMA}/{UC_VOLUME}/benchbox/tpch_sf01/lineitem/\n",
    "\n",
    "-- Then loads using COPY INTO:\n",
    "COPY INTO {UC_CATALOG}.{UC_SCHEMA}.lineitem\n",
    "FROM '/Volumes/{UC_CATALOG}/{UC_SCHEMA}/{UC_VOLUME}/benchbox/tpch_sf01/lineitem/'\n",
    "FILEFORMAT = PARQUET\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\"\"\"\n",
    "\n",
    "print(copy_example)\n",
    "print(\"‚úÖ COPY INTO provides:\")\n",
    "print(\"  ‚Ä¢ Idempotent loads (safe to retry)\")\n",
    "print(\"  ‚Ä¢ Automatic file tracking\")\n",
    "print(\"  ‚Ä¢ Schema evolution support\")\n",
    "print(\"  ‚Ä¢ Better performance than INSERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "platform_spark_header",
   "metadata": {},
   "source": [
    "### Spark UI Integration\n",
    "\n",
    "Monitor query execution in the Spark UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "platform_spark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark UI tips\n",
    "print(\"üìä Spark UI Monitoring\\n\")\n",
    "\n",
    "print(\"After running benchmarks, check the Spark UI for:\")\n",
    "print(\"  1. SQL tab: See all executed queries\")\n",
    "print(\"  2. Jobs tab: Understand Spark job execution\")\n",
    "print(\"  3. Stages tab: Identify slow stages\")\n",
    "print(\"  4. Storage tab: Check cached data\")\n",
    "print(\"  5. Executors tab: Monitor resource usage\")\n",
    "\n",
    "print(\"\\nüîç Key metrics to watch:\")\n",
    "print(\"  ‚Ä¢ Data scanned vs data output (selectivity)\")\n",
    "print(\"  ‚Ä¢ Shuffle read/write (join efficiency)\")\n",
    "print(\"  ‚Ä¢ Task execution time distribution\")\n",
    "print(\"  ‚Ä¢ Data skew indicators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "platform_comparison_header",
   "metadata": {},
   "source": [
    "### Performance Comparison: Standard vs Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "platform_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual comparison (would require actual runs)\n",
    "print(\"üî• Optimization Impact (Typical Results)\\n\")\n",
    "\n",
    "optimization_data = {\n",
    "    \"Configuration\": [\"Standard\", \"Z-ORDER\", \"Z-ORDER + Stats\", \"Z-ORDER + Stats + Cache\"],\n",
    "    \"Query Time (s)\": [45.2, 38.1, 32.5, 28.3],\n",
    "    \"Improvement (%)\": [0, 15.7, 28.1, 37.4],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(optimization_data)\n",
    "display(df)\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.bar(df[\"Configuration\"], df[\"Query Time (s)\"], color=\"steelblue\", alpha=0.8)\n",
    "bars[-1].set_color(\"green\")  # Highlight best\n",
    "\n",
    "ax.set_ylabel(\"Average Query Time (seconds)\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_title(\"Optimization Impact on TPC-H Performance\", fontsize=14, fontweight=\"bold\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "plt.xticks(rotation=15, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Typical improvements: 15-40% with proper optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5_header",
   "metadata": {},
   "source": [
    "## 5. Performance Analysis\n",
    "\n",
    "Deep dive into performance metrics and optimization opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis_load_header",
   "metadata": {},
   "source": [
    "### Load and Parse Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis_load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "query_data = []\n",
    "for qr in results.query_results:\n",
    "    query_data.append(\n",
    "        {\n",
    "            \"query\": qr.query_name,\n",
    "            \"time\": qr.execution_time,\n",
    "            \"rows\": qr.row_count,\n",
    "            \"status\": \"success\" if qr.success else \"failed\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_results = pd.DataFrame(query_data)\n",
    "print(\"üìä Query Performance Data\\n\")\n",
    "display(df_results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis_stats_header",
   "metadata": {},
   "source": [
    "### Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis_stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute detailed statistics\n",
    "print(\"üìä Performance Statistics\\n\")\n",
    "\n",
    "stats = df_results[\"time\"].describe(percentiles=[0.5, 0.95, 0.99])\n",
    "print(stats)\n",
    "\n",
    "print(\"\\nüîç Key Percentiles:\")\n",
    "print(f\"  Median (P50): {df_results['time'].median():.3f}s\")\n",
    "print(f\"  P95: {df_results['time'].quantile(0.95):.3f}s\")\n",
    "print(f\"  P99: {df_results['time'].quantile(0.99):.3f}s\")\n",
    "\n",
    "# Identify outliers\n",
    "mean_time = df_results[\"time\"].mean()\n",
    "std_time = df_results[\"time\"].std()\n",
    "outliers = df_results[df_results[\"time\"] > mean_time + 2 * std_time]\n",
    "\n",
    "if not outliers.empty:\n",
    "    print(\"\\n‚ö†Ô∏è Performance Outliers (>2 std dev):\")\n",
    "    for _, row in outliers.iterrows():\n",
    "        print(f\"  Query {row['query']}: {row['time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis_breakdown_header",
   "metadata": {},
   "source": [
    "### Query Performance Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis_breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize queries by performance\n",
    "df_sorted = df_results.sort_values(\"time\", ascending=False)\n",
    "\n",
    "print(\"üê¢ Top 5 Slowest Queries:\\n\")\n",
    "display(df_sorted.head())\n",
    "\n",
    "print(\"\\n‚ö° Top 5 Fastest Queries:\\n\")\n",
    "display(df_sorted.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis_viz_header",
   "metadata": {},
   "source": [
    "### Advanced Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization suite\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Distribution histogram\n",
    "axes[0, 0].hist(df_results[\"time\"], bins=20, color=\"steelblue\", alpha=0.7, edgecolor=\"black\")\n",
    "axes[0, 0].axvline(df_results[\"time\"].mean(), color=\"red\", linestyle=\"--\", linewidth=2, label=\"Mean\")\n",
    "axes[0, 0].axvline(df_results[\"time\"].median(), color=\"green\", linestyle=\"--\", linewidth=2, label=\"Median\")\n",
    "axes[0, 0].set_xlabel(\"Execution Time (seconds)\", fontweight=\"bold\")\n",
    "axes[0, 0].set_ylabel(\"Frequency\", fontweight=\"bold\")\n",
    "axes[0, 0].set_title(\"Query Time Distribution\", fontweight=\"bold\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Box plot\n",
    "axes[0, 1].boxplot(df_results[\"time\"], vert=True, patch_artist=True, boxprops=dict(facecolor=\"lightblue\", alpha=0.7))\n",
    "axes[0, 1].set_ylabel(\"Execution Time (seconds)\", fontweight=\"bold\")\n",
    "axes[0, 1].set_title(\"Query Time Box Plot\", fontweight=\"bold\")\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Sorted bar chart\n",
    "df_sorted.plot(x=\"query\", y=\"time\", kind=\"barh\", ax=axes[1, 0], color=\"coral\", alpha=0.8, legend=False)\n",
    "axes[1, 0].set_xlabel(\"Execution Time (seconds)\", fontweight=\"bold\")\n",
    "axes[1, 0].set_ylabel(\"Query\", fontweight=\"bold\")\n",
    "axes[1, 0].set_title(\"Queries Ranked by Performance\", fontweight=\"bold\")\n",
    "axes[1, 0].grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# 4. Cumulative time\n",
    "df_sorted[\"cumulative_pct\"] = df_sorted[\"time\"].cumsum() / df_sorted[\"time\"].sum() * 100\n",
    "axes[1, 1].plot(\n",
    "    range(len(df_sorted)), df_sorted[\"cumulative_pct\"], marker=\"o\", linewidth=2, markersize=6, color=\"purple\"\n",
    ")\n",
    "axes[1, 1].axhline(80, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"80% threshold\")\n",
    "axes[1, 1].fill_between(range(len(df_sorted)), df_sorted[\"cumulative_pct\"], alpha=0.3, color=\"purple\")\n",
    "axes[1, 1].set_xlabel(\"Number of Queries\", fontweight=\"bold\")\n",
    "axes[1, 1].set_ylabel(\"Cumulative Time (%)\", fontweight=\"bold\")\n",
    "axes[1, 1].set_title(\"Cumulative Performance (Pareto Analysis)\", fontweight=\"bold\")\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find 80/20 point\n",
    "pareto_80 = df_sorted[df_sorted[\"cumulative_pct\"] <= 80]\n",
    "print(\n",
    "    f\"\\nüéØ Pareto Principle: {len(pareto_80)} queries ({len(pareto_80) / len(df_sorted) * 100:.1f}%) account for 80% of total time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis_regression_header",
   "metadata": {},
   "source": [
    "### Regression Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis_regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare against baseline (if available)\n",
    "print(\"üîç Regression Analysis\\n\")\n",
    "\n",
    "# Simulated baseline for demonstration\n",
    "baseline_avg = 2.1  # seconds\n",
    "current_avg = df_results[\"time\"].mean()\n",
    "\n",
    "change_pct = ((current_avg - baseline_avg) / baseline_avg) * 100\n",
    "\n",
    "if abs(change_pct) > 10:\n",
    "    status = \"‚ùå REGRESSION\" if change_pct > 0 else \"‚úÖ IMPROVEMENT\"\n",
    "    print(f\"{status} DETECTED\")\n",
    "else:\n",
    "    print(\"‚úÖ Performance stable\")\n",
    "\n",
    "print(f\"\\nBaseline: {baseline_avg:.2f}s\")\n",
    "print(f\"Current: {current_avg:.2f}s\")\n",
    "print(f\"Change: {change_pct:+.1f}%\")\n",
    "\n",
    "if change_pct > 10:\n",
    "    print(\"\\nüí° Investigation steps:\")\n",
    "    print(\"  1. Check cluster configuration changes\")\n",
    "    print(\"  2. Review recent Unity Catalog updates\")\n",
    "    print(\"  3. Verify data volume hasn't changed\")\n",
    "    print(\"  4. Check for table VACUUM/OPTIMIZE status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis_recommendations_header",
   "metadata": {},
   "source": [
    "### Optimization Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis_recommendations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate optimization recommendations\n",
    "print(\"üí° Performance Optimization Recommendations\\n\")\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Check for slow queries\n",
    "slow_queries = df_results[df_results[\"time\"] > df_results[\"time\"].quantile(0.9)]\n",
    "if not slow_queries.empty:\n",
    "    recommendations.append(\n",
    "        f\"‚ö° {len(slow_queries)} slow queries detected (>P90). Consider Z-ORDER optimization for these tables.\"\n",
    "    )\n",
    "\n",
    "# Check variance\n",
    "cv = df_results[\"time\"].std() / df_results[\"time\"].mean()\n",
    "if cv > 0.5:\n",
    "    recommendations.append(\n",
    "        f\"üìà High performance variance detected (CV={cv:.2f}). Review query plans for inconsistent performance.\"\n",
    "    )\n",
    "\n",
    "# Check failed queries\n",
    "failed = df_results[df_results[\"status\"] == \"failed\"]\n",
    "if not failed.empty:\n",
    "    recommendations.append(f\"‚ùå {len(failed)} failed queries. Review error logs and fix issues.\")\n",
    "\n",
    "# General recommendations\n",
    "recommendations.extend(\n",
    "    [\n",
    "        \"üî∑ Run OPTIMIZE command on all Delta tables\",\n",
    "        \"üìä Run ANALYZE TABLE for better query planning\",\n",
    "        \"üíæ Consider result caching for repeated queries\",\n",
    "        \"üöÄ Enable Photon acceleration for faster performance\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(\"\\n‚úÖ Run these optimizations and re-test to measure improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6_header",
   "metadata": {},
   "source": [
    "## 6. Troubleshooting\n",
    "\n",
    "Common issues and solutions for running benchmarks on Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshoot_common_header",
   "metadata": {},
   "source": [
    "### Common Issues and Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "troubleshoot_common",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common troubleshooting scenarios\n",
    "print(\"üîß Common Issues and Solutions\\n\")\n",
    "\n",
    "issues = [\n",
    "    {\n",
    "        \"issue\": \"Authentication Failed\",\n",
    "        \"symptoms\": [\"401 Unauthorized\", \"Invalid token\", \"Access denied\"],\n",
    "        \"solutions\": [\n",
    "            \"Verify token has not expired\",\n",
    "            \"Check token has workspace access\",\n",
    "            \"Ensure DATABRICKS_HOST includes https://\",\n",
    "            \"Try regenerating the Personal Access Token\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"issue\": \"Unity Catalog Volume Not Found\",\n",
    "        \"symptoms\": [\"Volume does not exist\", \"Path not found\", \"VOLUME_NOT_FOUND\"],\n",
    "        \"solutions\": [\n",
    "            \"Create catalog: CREATE CATALOG IF NOT EXISTS main;\",\n",
    "            \"Create schema: CREATE SCHEMA IF NOT EXISTS main.benchbox;\",\n",
    "            \"Create volume: CREATE VOLUME IF NOT EXISTS main.benchbox.data;\",\n",
    "            \"Verify permissions on catalog/schema/volume\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"issue\": \"Out of Memory\",\n",
    "        \"symptoms\": [\"OOM error\", \"Executor lost\", \"GC overhead limit\"],\n",
    "        \"solutions\": [\n",
    "            \"Reduce scale factor (try 0.001 or 0.01)\",\n",
    "            \"Use larger cluster\",\n",
    "            \"Enable auto-scaling\",\n",
    "            \"Run fewer concurrent queries\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"issue\": \"Slow Performance\",\n",
    "        \"symptoms\": [\"Queries taking 10x longer\", \"Timeout errors\"],\n",
    "        \"solutions\": [\n",
    "            \"Run OPTIMIZE on all tables\",\n",
    "            \"Add Z-ORDER by commonly filtered columns\",\n",
    "            \"Run ANALYZE TABLE for statistics\",\n",
    "            \"Check cluster is not shared/overloaded\",\n",
    "            \"Enable Photon acceleration\",\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "for i, item in enumerate(issues, 1):\n",
    "    print(f\"{i}. ‚ùå {item['issue']}\")\n",
    "    print(f\"   Symptoms: {', '.join(item['symptoms'])}\")\n",
    "    print(\"   Solutions:\")\n",
    "    for sol in item[\"solutions\"]:\n",
    "        print(f\"     ‚Ä¢ {sol}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshoot_connection_header",
   "metadata": {},
   "source": [
    "### Connection Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "troubleshoot_connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection diagnostic\n",
    "def diagnose_connection():\n",
    "    \"\"\"\n",
    "    Diagnose Databricks connection issues\n",
    "    \"\"\"\n",
    "    print(\"üîç Connection Diagnostic\\n\")\n",
    "\n",
    "    # Check 1: Environment variables\n",
    "    print(\"1. Checking environment variables...\")\n",
    "    if DATABRICKS_HOST and DATABRICKS_TOKEN:\n",
    "        print(\"   ‚úÖ DATABRICKS_HOST and DATABRICKS_TOKEN are set\")\n",
    "        print(f\"   Host: {DATABRICKS_HOST}\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Missing environment variables\")\n",
    "        return False\n",
    "\n",
    "    # Check 2: Host format\n",
    "    print(\"\\n2. Validating host format...\")\n",
    "    if DATABRICKS_HOST.startswith(\"https://\"):\n",
    "        print(\"   ‚úÖ Host includes https://\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Host should start with https://\")\n",
    "\n",
    "    # Check 3: Token format\n",
    "    print(\"\\n3. Checking token format...\")\n",
    "    if len(DATABRICKS_TOKEN) > 20:\n",
    "        print(\"   ‚úÖ Token appears valid (length check)\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Token seems too short\")\n",
    "        return False\n",
    "\n",
    "    # Check 4: Unity Catalog config\n",
    "    print(\"\\n4. Unity Catalog configuration...\")\n",
    "    print(f\"   Catalog: {UC_CATALOG}\")\n",
    "    print(f\"   Schema: {UC_SCHEMA}\")\n",
    "    print(f\"   Volume: {UC_VOLUME}\")\n",
    "    print(\"   ‚úÖ Configuration looks good\")\n",
    "\n",
    "    print(\"\\n‚úÖ All checks passed\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run diagnostic\n",
    "diagnose_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshoot_permissions_header",
   "metadata": {},
   "source": [
    "### Permission Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "troubleshoot_permissions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check permissions\n",
    "print(\"üîê Permission Validation\\n\")\n",
    "\n",
    "print(\"Required permissions for benchmarking:\\n\")\n",
    "\n",
    "permissions = [\n",
    "    (\"Catalog\", UC_CATALOG, [\"USE CATALOG\", \"CREATE SCHEMA\"]),\n",
    "    (\"Schema\", f\"{UC_CATALOG}.{UC_SCHEMA}\", [\"USE SCHEMA\", \"CREATE TABLE\", \"CREATE VOLUME\"]),\n",
    "    (\"Volume\", f\"{UC_CATALOG}.{UC_SCHEMA}.{UC_VOLUME}\", [\"READ FILES\", \"WRITE FILES\"]),\n",
    "]\n",
    "\n",
    "for resource_type, resource_name, perms in permissions:\n",
    "    print(f\"{resource_type}: {resource_name}\")\n",
    "    for perm in perms:\n",
    "        print(f\"  ‚Ä¢ {perm}\")\n",
    "    print()\n",
    "\n",
    "print(\"üí° To grant permissions, run in SQL editor:\")\n",
    "print(f\"GRANT USE CATALOG, CREATE SCHEMA ON CATALOG {UC_CATALOG} TO `<principal>`;\")\n",
    "print(f\"GRANT ALL PRIVILEGES ON SCHEMA {UC_CATALOG}.{UC_SCHEMA} TO `<principal>`;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshoot_diagnostic_header",
   "metadata": {},
   "source": [
    "### Diagnostic Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "troubleshoot_diagnostic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive diagnostic\n",
    "def run_full_diagnostic():\n",
    "    \"\"\"\n",
    "    Run complete diagnostic check\n",
    "    \"\"\"\n",
    "    print(\"üõ†Ô∏è Running Full Diagnostic...\\n\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # System info\n",
    "    print(\"\\nüíª System Information:\")\n",
    "    print(f\"  Python version: {sys.version.split()[0]}\")\n",
    "    print(f\"  BenchBox version: {benchbox.__version__}\")\n",
    "\n",
    "    # Databricks config\n",
    "    print(\"\\n‚òÅÔ∏è Databricks Configuration:\")\n",
    "    print(f\"  Host: {DATABRICKS_HOST}\")\n",
    "    print(f\"  Token: {'*' * 20} (hidden)\")\n",
    "    print(f\"  Unity Catalog: {UC_CATALOG}.{UC_SCHEMA}\")\n",
    "    print(f\"  Volume: {UC_VOLUME}\")\n",
    "\n",
    "    # Test connection\n",
    "    print(\"\\nüîå Connection Test:\")\n",
    "    try:\n",
    "        # Try to list volume\n",
    "        volume_path = f\"/Volumes/{UC_CATALOG}/{UC_SCHEMA}/{UC_VOLUME}\"\n",
    "        dbutils.fs.ls(volume_path)\n",
    "        print(f\"  ‚úÖ Successfully connected to {volume_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Connection failed: {e}\")\n",
    "\n",
    "    # Memory check\n",
    "    print(\"\\nüíæ Resource Check:\")\n",
    "    print(f\"  Cluster: {sc.getConf().get('spark.databricks.clusterUsageTags.clusterName', 'Unknown')}\")\n",
    "    print(f\"  Driver Memory: {sc.getConf().get('spark.driver.memory', 'Unknown')}\")\n",
    "    print(f\"  Executor Memory: {sc.getConf().get('spark.executor.memory', 'Unknown')}\")\n",
    "\n",
    "    print(\"\\n=\" * 60)\n",
    "    print(\"‚úÖ Diagnostic complete\")\n",
    "\n",
    "\n",
    "# Run it\n",
    "run_full_diagnostic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion_header",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully completed the BenchBox Databricks benchmarking guide!\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. ‚úÖ **Installation & Setup**: Configured BenchBox with Unity Catalog\n",
    "2. ‚úÖ **Quick Start**: Ran TPC-H benchmark and visualized results\n",
    "3. ‚úÖ **Advanced Examples**: Multiple benchmarks, scale factors, and optimizations\n",
    "4. ‚úÖ **Platform Features**: Delta Lake, Unity Catalog, and Databricks optimizations\n",
    "5. ‚úÖ **Performance Analysis**: Statistical analysis and visualization\n",
    "6. ‚úÖ **Troubleshooting**: Diagnostic tools and common issue resolution\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Scale Up**: Try larger scale factors (1.0, 10.0) for production testing\n",
    "- **Optimize**: Apply Z-ORDER and OPTIMIZE to improve performance\n",
    "- **Compare**: Run benchmarks on different cluster sizes\n",
    "- **Monitor**: Set up continuous performance monitoring\n",
    "- **Production**: Integrate BenchBox into your CI/CD pipeline\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [BenchBox Documentation](https://github.com/joeharris76/benchbox)\n",
    "- [Databricks Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html)\n",
    "- [Delta Lake Optimization](https://docs.databricks.com/delta/optimize.html)\n",
    "- [TPC-H Specification](http://www.tpc.org/tpch/)\n",
    "\n",
    "**Happy Benchmarking!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
