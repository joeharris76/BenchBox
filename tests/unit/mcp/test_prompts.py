"""Tests for BenchBox MCP prompts.

Copyright 2026 Joe Harris / BenchBox Project

Licensed under the MIT License. See LICENSE file in the project root for details.
"""

import sys

import pytest

# Skip all tests if Python < 3.10
pytestmark = pytest.mark.skipif(sys.version_info < (3, 10), reason="MCP server requires Python 3.10+")


class TestAnalyzeResultsPrompt:
    """Tests for analyze_results prompt."""

    def test_prompt_returns_text(self):
        """Test that prompt returns text content."""
        # Test the prompt template logic
        benchmark = "tpch"
        platform = "duckdb"
        focus = "slowest_queries"

        prompt_text = f"Analyze the {benchmark.upper()} benchmark results on {platform}."
        focus_text = f"Pay special attention to {focus} in your analysis."

        assert "TPCH" in prompt_text
        assert "duckdb" in prompt_text
        assert "slowest_queries" in focus_text

    def test_prompt_without_focus(self):
        """Test prompt without focus parameter."""
        benchmark = "tpcds"
        platform = "polars-df"
        focus = None

        prompt_text = f"Analyze the {benchmark.upper()} benchmark results on {platform}."
        focus_text = f"\nPay special attention to {focus} in your analysis." if focus else ""

        assert "TPCDS" in prompt_text
        assert focus_text == ""


class TestComparePlatformsPrompt:
    """Tests for compare_platforms prompt."""

    def test_prompt_parses_platforms(self):
        """Test that platforms are parsed correctly."""
        platforms_str = "duckdb,polars-df,datafusion"
        platform_list = [p.strip() for p in platforms_str.split(",")]

        assert len(platform_list) == 3
        assert "duckdb" in platform_list
        assert "polars-df" in platform_list
        assert "datafusion" in platform_list

    def test_prompt_includes_scale_factor(self):
        """Test that scale factor is included."""
        scale_factor = 0.1
        prompt_text = f"Scale Factor: {scale_factor}"

        assert "0.1" in prompt_text


class TestIdentifyRegressionsPrompt:
    """Tests for identify_regressions prompt."""

    def test_prompt_with_specific_runs(self):
        """Test prompt with specific baseline and comparison runs."""
        baseline_run = "run1.json"
        comparison_run = "run2.json"
        threshold_percent = 10.0

        prompt_text = f"Baseline: {baseline_run}\nComparison: {comparison_run}\nThreshold: {threshold_percent}%"

        assert "run1.json" in prompt_text
        assert "run2.json" in prompt_text
        assert "10.0" in prompt_text

    def test_prompt_without_specific_runs(self):
        """Test prompt for automatic run detection."""
        baseline_run = None
        comparison_run = None

        assert baseline_run is None
        assert comparison_run is None


class TestBenchmarkPlanningPrompt:
    """Tests for benchmark_planning prompt."""

    def test_prompt_includes_time_budget(self):
        """Test that time budget is included."""
        time_budget_minutes = 30
        prompt_text = f"Time Budget: {time_budget_minutes} minutes"

        assert "30 minutes" in prompt_text

    def test_prompt_includes_use_case(self):
        """Test that use case is included."""
        use_case = "regression"
        prompt_text = f"Use Case: {use_case}"

        assert "regression" in prompt_text


class TestTroubleshootFailurePrompt:
    """Tests for troubleshoot_failure prompt."""

    def test_prompt_with_error_message(self):
        """Test prompt with error message."""
        error_message = "Connection refused"
        platform = "snowflake"
        benchmark = "tpch"

        context = []
        if error_message:
            context.append(f"Error Message: {error_message}")
        if platform:
            context.append(f"Platform: {platform}")
        if benchmark:
            context.append(f"Benchmark: {benchmark}")

        context_str = "\n".join(context)

        assert "Connection refused" in context_str
        assert "snowflake" in context_str
        assert "tpch" in context_str

    def test_prompt_without_context(self):
        """Test prompt without specific context."""
        error_message = None
        platform = None
        benchmark = None

        context = []
        if error_message:
            context.append(f"Error Message: {error_message}")
        if platform:
            context.append(f"Platform: {platform}")
        if benchmark:
            context.append(f"Benchmark: {benchmark}")

        context_str = "\n".join(context) if context else "No specific context provided"

        assert context_str == "No specific context provided"
