"""Unit tests for the BaseBenchmark abstract class.

Copyright 2026 Joe Harris / BenchBox Project

Licensed under the MIT License. See LICENSE file in the project root for details.
"""

from unittest.mock import MagicMock, patch

import pytest

from benchbox.core.base_benchmark import BaseBenchmark  # Adjust import path as needed
from benchbox.core.results.models import BenchmarkResults

# Mark all tests in this file as unit tests
pytestmark = [pytest.mark.unit, pytest.mark.fast]


class MockBenchmark(BaseBenchmark):
    """A minimal concrete implementation of BaseBenchmark for testing."""

    def __init__(self, scale_factor=1.0, **kwargs):
        super().__init__(scale_factor=scale_factor, **kwargs)
        self._name = "Mock Benchmark"
        self._version = "1.0.0"
        self._description = "A mock benchmark for testing"
        self._resources = []

    def generate_data(self, tables=None, output_format="memory"):
        """Generate mock data for testing."""
        result = {}
        mock_tables = tables or ["table1", "table2"]
        for table in mock_tables:
            result[table] = [{"id": i, "value": f"test_{i}"} for i in range(10)]
        return result

    def get_query(self, query_id):
        """Return a mock query for the given ID."""
        queries = self.get_all_queries()
        if query_id not in queries:
            raise ValueError(f"Query ID {query_id} not found")
        return queries[query_id]

    def get_all_queries(self):
        """Return all available mock queries."""
        return {
            1: "SELECT * FROM table1",
            2: "SELECT * FROM table2 WHERE id > 5",
            3: "SELECT t1.id, t2.value FROM table1 t1 JOIN table2 t2 ON t1.id = t2.id",
        }

    def execute_query(self, query_id, connection, params=None):
        """Execute a mock query."""
        self.get_query(query_id)
        # In a real implementation, we would execute the query on the connection
        # Here we just simulate it
        return [{"id": i, "result": f"value_{i}"} for i in range(5)]

    def cleanup(self):
        """Clean up any resources used by the base_benchmark."""
        self._resources = []


@pytest.fixture
def mock_db_connection():
    """Create a mock database connection for testing."""
    conn = MagicMock()
    cursor = MagicMock()
    conn.cursor.return_value = cursor
    cursor.execute.return_value = None
    cursor.fetchall.return_value = [{"id": 1, "value": "test"}]
    return conn


@pytest.fixture
def base_benchmark():
    """Create a MockBenchmark instance for testing."""
    return MockBenchmark(scale_factor=1.0)


# Test category 1: Initialization and Configuration Tests
class TestInitialization:
    """Tests for benchmark initialization."""

    def test_basic_initialization(self):
        """Test that MockBenchmark can be initialized with default parameters."""
        benchmark = MockBenchmark()
        assert benchmark.scale_factor == 1.0
        assert benchmark._name == "Mock Benchmark"
        assert benchmark._version == "1.0.0"

    def test_custom_scale_factor(self):
        """Test initialization with a custom scale factor."""
        benchmark = MockBenchmark(scale_factor=5.0)
        assert benchmark.scale_factor == 5.0

    def test_invalid_scale_factor(self):
        """Test that initialization with an invalid scale factor raises ValueError."""
        with pytest.raises(ValueError, match="Scale factor must be positive"):
            MockBenchmark(scale_factor=-1.0)

        with pytest.raises(ValueError, match="Scale factor must be positive"):
            MockBenchmark(scale_factor=0)

    def test_custom_config(self):
        """Test initialization with custom configuration parameters."""
        benchmark = MockBenchmark(scale_factor=2.0, custom_param="value")
        assert benchmark.scale_factor == 2.0
        assert benchmark.config.get("custom_param") == "value"


# Test category 2: Abstract Method Tests
class TestAbstractMethods:
    """Tests for abstract method requirements."""

    def test_cannot_instantiate_abstract(self):
        """Test that BaseBenchmark cannot be instantiated directly."""
        with pytest.raises(TypeError, match="abstract"):
            BaseBenchmark(scale_factor=1.0)

    def test_abstract_methods_defined(self):
        """Verify that all required abstract methods are defined in BaseBenchmark."""
        abstract_methods = [
            method
            for method in dir(BaseBenchmark)
            if not method.startswith("__")
            and hasattr(getattr(BaseBenchmark, method), "__isabstractmethod__")
            and getattr(BaseBenchmark, method).__isabstractmethod__
        ]

        # These should match the abstract methods defined in BaseBenchmark
        expected_abstract_methods = [
            "generate_data",
            "get_query",
            "get_all_queries",
            "execute_query",
        ]

        for method in expected_abstract_methods:
            assert method in abstract_methods, f"Expected abstract method {method} not found"


# Test category 3: Metadata Access Tests
class TestMetadata:
    """Tests for metadata property access."""

    def test_name_access(self, base_benchmark):
        """Test retrieval of benchmark name."""
        assert base_benchmark.name == "Mock Benchmark"

    def test_version_access(self, base_benchmark):
        """Test retrieval of benchmark version."""
        assert base_benchmark.version == "1.0.0"

    def test_description_access(self, base_benchmark):
        """Test retrieval of benchmark description."""
        assert base_benchmark.description == "A mock benchmark for testing"

    def test_metadata_immutability(self, base_benchmark):
        """Test that metadata attributes are read-only."""
        with pytest.raises(AttributeError):
            base_benchmark.name = "New Name"

        with pytest.raises(AttributeError):
            base_benchmark.version = "2.0.0"

        with pytest.raises(AttributeError):
            base_benchmark.description = "New description"


# Test category 4: Resource Management Tests
class TestResourceManagement:
    """Tests for resource management functionality."""

    def test_cleanup(self, base_benchmark):
        """Test that cleanup method properly releases resources."""
        # Set up resources
        base_benchmark._resources = ["resource1", "resource2"]

        # Cleanup
        base_benchmark.cleanup()

        # Verify resources are released
        assert base_benchmark._resources == []

    def test_context_manager(self):
        """Test using MockBenchmark as a context manager."""
        with MockBenchmark(scale_factor=1.0) as benchmark:
            # Use the benchmark
            benchmark._resources = ["resource1"]
            assert len(benchmark._resources) == 1

        # After context exit, resources should be cleaned up
        assert benchmark._resources == []

    def test_context_manager_with_exception(self):
        """Test context manager behavior during exceptions."""
        benchmark = None
        try:
            with MockBenchmark(scale_factor=1.0) as benchmark:
                benchmark._resources = ["resource1"]
                raise RuntimeError("Test exception")
        except RuntimeError:
            # Even after an exception, resources should be cleaned up
            assert benchmark._resources == []


# Test category 6: Error Handling Tests
class TestErrorHandling:
    """Tests for error handling behaviors."""

    def test_invalid_query_id(self, base_benchmark):
        """Test behavior with invalid query IDs."""
        with pytest.raises(ValueError, match="Query ID .* not found"):
            base_benchmark.get_query(999)

    def test_query_execution_error(self, base_benchmark, mock_db_connection):
        """Test response to query execution errors."""
        # Make the mock connection raise an exception during execution
        mock_db_connection.cursor.return_value.execute.side_effect = Exception("DB Error")

        with patch.object(base_benchmark, "execute_query", side_effect=Exception("DB Error")):
            with pytest.raises(Exception, match="DB Error"):
                base_benchmark.execute_query(1, mock_db_connection)


# Test category 7: Query Management Tests
class TestQueryManagement:
    """Tests for query management functionality."""

    def test_get_query(self, base_benchmark):
        """Test retrieving individual queries by ID."""
        query = base_benchmark.get_query(1)
        assert query == "SELECT * FROM table1"

    def test_get_all_queries(self, base_benchmark):
        """Test retrieving all available queries."""
        queries = base_benchmark.get_all_queries()
        assert len(queries) == 3
        assert 1 in queries
        assert 2 in queries
        assert 3 in queries

    def test_nonexistent_query(self, base_benchmark):
        """Test behavior with nonexistent query IDs."""
        with pytest.raises(ValueError):
            base_benchmark.get_query(999)


# Test category 8: Data Generation Tests
class TestDataGeneration:
    """Tests for data generation functionality."""

    def test_generate_data_default(self, base_benchmark):
        """Test data generation with default parameters."""
        data = base_benchmark.generate_data()
        assert "table1" in data
        assert "table2" in data
        assert len(data["table1"]) == 10
        assert len(data["table2"]) == 10

    def test_generate_specific_tables(self, base_benchmark):
        """Test generating data for specific tables."""
        data = base_benchmark.generate_data(tables=["table1"])
        assert "table1" in data
        assert "table2" not in data

    def test_scale_factor_effect(self):
        """Test relationship between scale factor and data volume."""

        # Create a custom mock benchmark that respects scale factor
        class ScaledMockBenchmark(MockBenchmark):
            def generate_data(self, tables=None, output_format="memory"):
                result = {}
                mock_tables = tables or ["table1", "table2"]
                row_count = int(10 * self.scale_factor)
                for table in mock_tables:
                    result[table] = [{"id": i, "value": f"test_{i}"} for i in range(row_count)]
                return result

        # Test with different scale factors
        benchmark1 = ScaledMockBenchmark(scale_factor=1.0)
        data1 = benchmark1.generate_data()
        assert len(data1["table1"]) == 10

        benchmark2 = ScaledMockBenchmark(scale_factor=2.0)
        data2 = benchmark2.generate_data()
        assert len(data2["table1"]) == 20


class TestMinimalResultHelper:
    """Tests for the minimal benchmark result convenience helper."""

    def test_create_minimal_benchmark_result_populates_core_fields(self, base_benchmark):
        """Minimal helper should return a fully-typed BenchmarkResults instance."""

        result = base_benchmark.create_minimal_benchmark_result(
            validation_status="FAILED",
            validation_details={"error": "boom"},
            duration_seconds=2.5,
            platform="duckdb",
            execution_metadata={"reason": "test"},
            system_profile={"cpu": "M2"},
            phases={"setup": {"status": "FAILED"}},
            execution_id="custom-id",
        )

        assert isinstance(result, BenchmarkResults)
        assert result.platform == "duckdb"
        assert result.validation_status == "FAILED"
        assert result.validation_details == {"error": "boom"}
        assert result.duration_seconds == 2.5
        assert result.execution_metadata["result_type"] == "minimal"
        assert result.execution_metadata["status"] == "FAILED"
        assert result.execution_metadata["reason"] == "test"
        assert result.execution_metadata["benchmark_id"] == "mock_benchmark"
        assert result.system_profile == {"cpu": "M2"}
        # execution_phases is None because phases parameter is dict but field expects ExecutionPhases dataclass
        assert result.execution_phases is None
        assert result.execution_id == "custom-id"

    def test_create_minimal_benchmark_result_defaults(self, base_benchmark):
        """Defaults should produce a minimal but complete result."""

        result = base_benchmark.create_minimal_benchmark_result(validation_status="INTERRUPTED")

        assert isinstance(result, BenchmarkResults)
        assert result.platform == "unknown"
        assert result.validation_status == "INTERRUPTED"
        assert result.validation_details == {}
        assert result.duration_seconds == 0.0
        assert result.query_results == []
        assert result.execution_metadata["result_type"] == "minimal"
        assert result.execution_metadata["status"] == "INTERRUPTED"
        assert result.execution_metadata["benchmark_id"] == "mock_benchmark"


if __name__ == "__main__":
    pytest.main(["-v"])
